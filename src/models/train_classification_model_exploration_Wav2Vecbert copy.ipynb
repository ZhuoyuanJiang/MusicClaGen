{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT detected as: /workspace/musicClaGen\n",
      "Adding /workspace/musicClaGen to sys.path\n",
      "/workspace/musicClaGen\n",
      "Imports and basic setup complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "# Use AutoProcessor for Wav2Vec2-BERT - it bundles feature_extractor and tokenizer (if needed)\n",
    "from transformers import AutoModelForAudioClassification, AutoProcessor\n",
    "\n",
    "from torch.optim import AdamW\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import ast # For parsing string representations of lists/arrays\n",
    "import logging\n",
    "import time\n",
    "from sklearn.metrics import hamming_loss, jaccard_score, f1_score # Add more as needed\n",
    "from tqdm.notebook import tqdm # Use notebook version of tqdm\n",
    "import librosa # Needed for loading raw audio now\n",
    "\n",
    "\n",
    "\n",
    "# --- Project Setup ---\n",
    "# Detect if running in notebook or script to adjust path\n",
    "\n",
    "cwd = os.getcwd()\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(cwd, '../../')) # NOTE: remember to change if change the directory structure\n",
    "\n",
    "\n",
    "\n",
    "print(f\"PROJECT_ROOT detected as: {PROJECT_ROOT}\")\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    print(f\"Adding {PROJECT_ROOT} to sys.path\")\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "# --- Config and Utils ---\n",
    "try:\n",
    "    import config # Import your configuration file\n",
    "    # Optionally import utils if needed, e.g., for get_audio_path if not defined here\n",
    "    # import src.utils as utils\n",
    "except ModuleNotFoundError:\n",
    "     print(\"ERROR: Cannot import config or utils. Make sure PROJECT_ROOT is correct and src is importable.\")\n",
    "     # Or add src to path: sys.path.insert(0, os.path.join(PROJECT_ROOT, 'src'))\n",
    "     # import config\n",
    "     # import utils\n",
    "\n",
    "\n",
    "# --- Setup Logging ---\n",
    "for handler in logging.root.handlers[:]: logging.root.removeHandler(handler) # Clear previous\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    handlers=[logging.StreamHandler(sys.stdout)])\n",
    "\n",
    "print(\"Imports and basic setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 04:27:10,712 - INFO - Loaded 22 unified genres from /workspace/musicClaGen/data/processed/unified_genres.txt\n",
      "2025-05-04 04:27:10,715 - INFO - Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Cell 2\n",
    "# --- Load Config ---\n",
    "# Ensure config.py has the correct paths in the PATHS dict\n",
    "manifest_path = config.PATHS.get('SMALL_MULTILABEL_PATH', os.path.join(config.PATHS['PROCESSED_DATA_DIR'], 'small_subset_multihot.csv')) # Use .get for safety\n",
    "genre_list_path = config.PATHS.get('GENRE_LIST_PATH', os.path.join(config.PATHS['PROCESSED_DATA_DIR'], 'unified_genres.txt'))\n",
    "model_save_dir = config.PATHS['MODELS_DIR']\n",
    "\n",
    "# Ensure config.py has MODEL_PARAMS dict with model_checkpoint\n",
    "model_checkpoint = config.MODEL_PARAMS['model_checkpoint'] # e.g., \"facebook/w2v-bert-2.0\" - VERIFY!\n",
    "learning_rate = config.MODEL_PARAMS['learning_rate']\n",
    "batch_size = config.MODEL_PARAMS['batch_size'] # Use the small BS for notebook test\n",
    "num_epochs_debug = 1 # <<<--- RUN ONLY 1 EPOCH FOR DEBUGGING ---<<<\n",
    "weight_decay = config.MODEL_PARAMS['weight_decay']\n",
    "gradient_accumulation_steps = config.MODEL_PARAMS['gradient_accumulation_steps']\n",
    "\n",
    "# --- Load unified genre list ---\n",
    "try:\n",
    "    with open(genre_list_path, 'r') as f:\n",
    "        unified_genres = [line.strip() for line in f if line.strip()]\n",
    "    num_labels = len(unified_genres) # should be the number of labels defined in the unified_genres.txt file, in this case it should be 22.\n",
    "    logging.info(f\"Loaded {num_labels} unified genres from {genre_list_path}\")\n",
    "    if num_labels == 0: raise ValueError(\"Genre list is empty!\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to load or process unified genre list: {e}\", exc_info=True)\n",
    "    raise SystemExit(\"Cannot proceed without genre list.\")\n",
    "\n",
    "# --- Setup Device ---\n",
    "device = torch.device(config.DEVICE if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(f\"Using device: {device}\")\n",
    "if not torch.cuda.is_available() and config.DEVICE==\"cuda\":\n",
    "     logging.warning(\"CUDA selected but not available, falling back to CPU.\")\n",
    "\n",
    "# --- Create Save Directory ---\n",
    "os.makedirs(model_save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 2: Load Config & Define Constants (Modified for CPU Debugging)\n",
    "\n",
    "# import torch # Ensure torch is imported\n",
    "# import os\n",
    "# import logging\n",
    "# import config # Your config file\n",
    "\n",
    "# # --- Load Config ---\n",
    "# # Ensure config.py has the correct paths in the PATHS dict\n",
    "# # Use the key pointing to your manifest with raw audio paths\n",
    "# manifest_path = config.PATHS.get('SMALL_MULTILABEL_PATH', os.path.join(config.PATHS['PROCESSED_DATA_DIR'], 'small_subset_multihot.csv'))\n",
    "# genre_list_path = config.PATHS.get('GENRE_LIST_PATH', os.path.join(config.PATHS['PROCESSED_DATA_DIR'], 'unified_genres.txt'))\n",
    "# model_save_dir = config.PATHS['MODELS_DIR']\n",
    "\n",
    "# # Ensure config.py has MODEL_PARAMS dict with model_checkpoint\n",
    "# model_checkpoint = config.MODEL_PARAMS['model_checkpoint'] # e.g., \"facebook/w2v-bert-2.0\"\n",
    "# learning_rate = config.MODEL_PARAMS['learning_rate']\n",
    "# batch_size = config.MODEL_PARAMS['batch_size']\n",
    "# num_epochs_debug = 1 # Keep as 1 for debug run\n",
    "# weight_decay = config.MODEL_PARAMS['weight_decay']\n",
    "# gradient_accumulation_steps = config.MODEL_PARAMS['gradient_accumulation_steps']\n",
    "\n",
    "# # --- Load unified genre list ---\n",
    "# try:\n",
    "#     with open(genre_list_path, 'r') as f:\n",
    "#         unified_genres = [line.strip() for line in f if line.strip()]\n",
    "#     num_labels = len(unified_genres)\n",
    "#     logging.info(f\"Loaded {num_labels} unified genres from {genre_list_path}\")\n",
    "#     if num_labels == 0: raise ValueError(\"Genre list is empty!\")\n",
    "# except Exception as e:\n",
    "#     logging.error(f\"Failed to load or process unified genre list: {e}\", exc_info=True)\n",
    "#     raise SystemExit(\"Cannot proceed without genre list.\")\n",
    "\n",
    "# # --- Setup Device (FORCED TO CPU FOR DEBUGGING) ---\n",
    "# # Original line: device = torch.device(config.DEVICE if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\") # <<<--- TEMPORARY CHANGE FOR DEBUGGING CUDA ERROR\n",
    "# logging.info(f\"Using device: {device} (Forced CPU for debugging)\")\n",
    "# # --- End Temporary Change ---\n",
    "\n",
    "# # --- Create Save Directory ---\n",
    "# os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# print(\"Cell 2 setup complete (Device forced to CPU for debugging).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/musicClaGen/data/processed/small_subset_multihot.csv\n"
     ]
    }
   ],
   "source": [
    "print(manifest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 3: Dataset Class Definition (Raw Audio Version) This cell uses the regex parser to parse the multi_hot_label string back into a list of integers.\n",
    "\n",
    "\n",
    "\n",
    "# # Define(recollect)the regex parser from preprocess.py if needed, \n",
    "# # otherwise use ast.literal_eval--- \n",
    "# # NOTE: After changing usage.ipynb 05/03/2025, should fall back to ast.literal_eval now. Clean code later\n",
    "\n",
    "# import re\n",
    "\n",
    "# def parse_numpy_array_string(array_str):\n",
    "#     \"\"\"\n",
    "#     Parse strings like '[np.float32(1.0), np.float32(0.0), ...]' into a list of integers.\n",
    "#     This is needed because ast.literal_eval cannot handle 'np.float32()' in the string.\n",
    "#     \"\"\"\n",
    "#     if not isinstance(array_str, str):\n",
    "#         return []\n",
    "    \n",
    "#     try:\n",
    "#         # Extract all the float values using regular expressions\n",
    "#         float_matches = re.findall(r'np\\.float32\\((\\d+\\.\\d+)\\)', array_str)\n",
    "        \n",
    "#         # Convert matches to integers (1.0 -> 1, 0.0 -> 0)\n",
    "#         values = []\n",
    "#         for match in float_matches:\n",
    "#             value = float(match)\n",
    "#             # Convert to integer if it's 0.0 or 1.0\n",
    "#             if value == 1.0:\n",
    "#                 values.append(1)\n",
    "#             elif value == 0.0:\n",
    "#                 values.append(0)\n",
    "#             else:\n",
    "#                 values.append(value)  # Keep as float if not 0 or 1\n",
    "                \n",
    "#         return values\n",
    "#     except Exception as e:\n",
    "#         logging.warning(f\"Error parsing array string: {e}\")\n",
    "#         return []\n",
    "\n",
    "# class FMARawAudioDataset(Dataset):\n",
    "#     \"\"\"\n",
    "#     Loads raw audio waveforms and labels from manifest, uses Hugging Face\n",
    "#     feature extractor (like ASTFeatureExtractor or Wav2Vec2Processor) on the fly.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, manifest_path, feature_extractor):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             manifest_path (str): Path to the final manifest CSV file.\n",
    "#             feature_extractor: Initialized Hugging Face AutoFeatureExtractor or AutoProcessor.\n",
    "#         \"\"\"\n",
    "#         logging.info(f\"Initializing FMARawAudioDataset from: {manifest_path}\")\n",
    "#         if feature_extractor is None:\n",
    "#              raise ValueError(\"FMARawAudioDataset requires a feature_extractor/processor instance.\")\n",
    "\n",
    "#         self.feature_extractor = feature_extractor\n",
    "#         # Get target sampling rate directly from the extractor/processor\n",
    "#         try:\n",
    "#              # Works for Wav2Vec2Processor, ASTFeatureExtractor, etc.\n",
    "#              self.target_sr = self.feature_extractor.sampling_rate\n",
    "#              logging.info(f\"Target sampling rate set from feature extractor: {self.target_sr} Hz\")\n",
    "#         except AttributeError:\n",
    "#              logging.warning(\"Could not get sampling_rate from feature_extractor, using config.\")\n",
    "#              # Fallback to config if needed, but ensuring match is crucial\n",
    "#              self.target_sr = config.PREPROCESSING_PARAMS['sample_rate']\n",
    "\n",
    "\n",
    "#         logging.info(f\"Loading manifest from: {manifest_path}\")\n",
    "#         try:\n",
    "#             self.manifest = pd.read_csv(manifest_path)\n",
    "#             # Ensure index is set if needed elsewhere, or use default range index\n",
    "#             if 'track_id' in self.manifest.columns:\n",
    "#                  self.manifest = self.manifest.set_index('track_id', drop=False)\n",
    "\n",
    "#             # --- Parse the 'multi_hot_label' string back into a list ---\n",
    "#             # Here: if we decide to use raw audio, we use regex parser; \n",
    "#             #       if we decide to use mel spectrogram, we use ast.literal_eval\n",
    "\n",
    "#             # Choose the correct parser based on how labels were saved in the CSV\n",
    "#             # If saved as '[1.0, 0.0,...]' use ast.literal_eval\n",
    "#             # label_parser = ast.literal_eval\n",
    "#             # If saved as '[np.float32(1.0)...]' uncomment and use regex parser\n",
    "#             label_parser = parse_numpy_array_string\n",
    "\n",
    "#             self.manifest['multi_hot_label'] = self.manifest['multi_hot_label'].apply(label_parser)\n",
    "#             logging.info(f\"Loaded and parsed manifest with {len(self.manifest)} entries.\")\n",
    "#             # Check the first parsed label\n",
    "#             logging.info(f\"Example parsed label (first entry): {self.manifest['multi_hot_label'].iloc[0]}\")\n",
    "\n",
    "#         except Exception as e:\n",
    "#             logging.error(f\"Error loading or parsing manifest {manifest_path}: {e}\", exc_info=True)\n",
    "#             raise\n",
    "\n",
    "#     def __len__(self):\n",
    "#         \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "#         return len(self.manifest)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         \"\"\"\n",
    "#         Loads raw audio for index idx, processes it with the feature extractor,\n",
    "#         and returns the processed inputs and labels.\n",
    "#         \"\"\"\n",
    "#         if torch.is_tensor(idx): idx = idx.tolist() # Handle tensor indices\n",
    "\n",
    "#         # Get the row data from the manifest\n",
    "#         row = self.manifest.iloc[idx]\n",
    "#         track_id = row.get('track_id', self.manifest.index[idx]) # Get track_id safely\n",
    "#         label_vector = row['multi_hot_label'] # Already parsed list/array\n",
    "\n",
    "#         # Construct absolute audio path if necessary\n",
    "#         audio_path = row['audio_path']\n",
    "\n",
    "#         #NOTE: originally, the mel-spectrogram's path is relative  but the raw audio's path is absolute, so we need to make sure the audio_path is absolute\n",
    "#         # So we are check if the audio_path is absolute or relative in case we load the wrong data, if it's relative, we need to join it with the PROJECT_ROOT\n",
    "#         if not os.path.isabs(audio_path):\n",
    "#              # Assumes path in manifest is relative to PROJECT_ROOT\n",
    "#              audio_path = os.path.join(config.PROJECT_ROOT, audio_path)\n",
    "\n",
    "#         try:\n",
    "#             # --- 1. Load RAW Audio Waveform ---\n",
    "#             # Load full 30s clip at the TARGET sample rate required by the processor\n",
    "#             waveform, loaded_sr = librosa.load(\n",
    "#                 audio_path,\n",
    "#                 sr=self.target_sr, # Use processor's sampling rate\n",
    "#                 duration=30.0     # Load the full 30 seconds\n",
    "#             )\n",
    "#             # Ensure minimum length if needed (though duration should handle it)\n",
    "#             min_samples = int(0.1 * self.target_sr) # Example: require at least 0.1s\n",
    "#             if len(waveform) < min_samples:\n",
    "#                  raise ValueError(f\"Audio signal for track {track_id} too short after loading.\")\n",
    "\n",
    "#             # --- 2. Apply Feature Extractor ---\n",
    "#             # Pass the raw waveform numpy array\n",
    "#             # The extractor handles normalization, padding/truncation, tensor conversion\n",
    "            \n",
    "#             max_length = 5000\n",
    "\n",
    "#             inputs = self.feature_extractor(\n",
    "#                 waveform,\n",
    "#                 sampling_rate=self.target_sr,\n",
    "#                 return_tensors=\"pt\",\n",
    "#                 return_attention_mask=True # Request attention mask\n",
    "#             )\n",
    "\n",
    "#             # --- 3. Prepare Outputs ---\n",
    "#             # Squeeze unnecessary batch dimension added by the extractor\n",
    "#             # Key name ('input_values', 'input_features') depends on the specific extractor\n",
    "#             feature_tensor = inputs.get('input_values', inputs.get('input_features'))\n",
    "#             if feature_tensor is None:\n",
    "#                 raise KeyError(\"Expected 'input_values' or 'input_features' key from feature_extractor output.\")\n",
    "#             feature_tensor = feature_tensor.squeeze(0) # Remove batch dim -> [Channels?, Freq?, Time] or [SeqLen, Dim]\n",
    "\n",
    "#             attention_mask = inputs.get('attention_mask', None)\n",
    "#             if attention_mask is not None:\n",
    "#                  attention_mask = attention_mask.squeeze(0)\n",
    "\n",
    "#             # Convert label list/array to float tensor for BCE loss\n",
    "#             label_tensor = torch.tensor(label_vector, dtype=torch.float32)\n",
    "\n",
    "#             # Return dictionary matching model's expected input names\n",
    "#             model_input_dict = {\"labels\": label_tensor}\n",
    "#             # Use the key the feature extractor provided\n",
    "#             if 'input_values' in inputs:\n",
    "#                  model_input_dict['input_values'] = feature_tensor\n",
    "#             elif 'input_features' in inputs:\n",
    "#                  model_input_dict['input_features'] = feature_tensor\n",
    "\n",
    "#             if attention_mask is not None:\n",
    "#                  model_input_dict['attention_mask'] = attention_mask\n",
    "\n",
    "#             return model_input_dict\n",
    "\n",
    "#         except FileNotFoundError:\n",
    "#              logging.error(f\"Audio file not found for track {track_id} at {audio_path}\")\n",
    "#              raise # Or implement skipping logic with collate_fn\n",
    "#         except Exception as e:\n",
    "#             logging.error(f\"Error loading/processing track {track_id} at {audio_path}: {e}\", exc_info=True)\n",
    "#             raise # Or implement skipping logic\n",
    "\n",
    "\n",
    "# print(\"FMARawAudioDataset class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Cell 3: Dataset Class Definition (Raw Audio Version) This cell uses the ast.literal_eval parser to parse the multi_hot_label string back into a list of integers.\n",
    "\n",
    "\n",
    "# import re\n",
    "# import ast \n",
    "\n",
    "\n",
    "\n",
    "# # # Define(recollect)the regex parser from preprocess.py if needed,\n",
    "# # # otherwise use ast.literal_eval---\n",
    "# # # NOTE: After changing usage.ipynb 05/03/2025, should fall back to ast.literal_eval now. Clean code later\n",
    "# # def parse_numpy_array_string(array_str):\n",
    "# #     \"\"\"\n",
    "# #     Parse strings like '[np.float32(1.0), np.float32(0.0), ...]' into a list of integers.\n",
    "# #     This is needed because ast.literal_eval cannot handle 'np.float32()' in the string.\n",
    "# #     \"\"\"\n",
    "# #     if not isinstance(array_str, str):\n",
    "# #         return []\n",
    "# #     try:\n",
    "# #         # Extract all the float values using regular expressions\n",
    "# #         # Refined regex to handle numbers with or without decimal points\n",
    "# #         float_matches = re.findall(r'np\\.float32\\(([\\d\\.]+)\\)', array_str)\n",
    "# #         # Convert matches to floats then maybe int (use float for BCE loss)\n",
    "# #         values = []\n",
    "# #         for match in float_matches:\n",
    "# #             value = float(match)\n",
    "# #             values.append(1.0 if value == 1.0 else 0.0) # Store as float 0.0 or 1.0\n",
    "# #         return values\n",
    "# #     except Exception as e:\n",
    "# #         logging.warning(f\"Error parsing array string: {e}\")\n",
    "# #         return []\n",
    "# # --- End commented out parser ---\n",
    "\n",
    "\n",
    "# class FMARawAudioDataset(Dataset):\n",
    "#     \"\"\"\n",
    "#     Loads raw audio waveforms and labels from manifest, uses Hugging Face\n",
    "#     feature extractor (like ASTFeatureExtractor or Wav2Vec2Processor/AutoFeatureExtractor) on the fly.\n",
    "#     Assumes padding/truncation will be handled by a collate function.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, manifest_path, feature_extractor):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             manifest_path (str): Path to the final manifest CSV file (e.g., small_subset_multihot.csv).\n",
    "#             feature_extractor: Initialized Hugging Face AutoFeatureExtractor instance.\n",
    "#         \"\"\"\n",
    "#         logging.info(f\"Initializing FMARawAudioDataset from: {manifest_path}\")\n",
    "#         if feature_extractor is None:\n",
    "#              raise ValueError(\"FMARawAudioDataset requires a feature_extractor instance.\")\n",
    "\n",
    "#         self.feature_extractor = feature_extractor\n",
    "#         try:\n",
    "#              self.target_sr = self.feature_extractor.sampling_rate\n",
    "#              logging.info(f\"Target sampling rate set from feature extractor: {self.target_sr} Hz\")\n",
    "#         except AttributeError:\n",
    "#              logging.error(\"Could not get sampling_rate from feature_extractor.\", exc_info=True)\n",
    "#              raise\n",
    "\n",
    "#         logging.info(f\"Loading manifest from: {manifest_path}\")\n",
    "#         try:\n",
    "#             self.manifest = pd.read_csv(manifest_path)\n",
    "#             if 'track_id' in self.manifest.columns:\n",
    "#                  self.manifest = self.manifest.set_index('track_id', drop=False)\n",
    "\n",
    "#             # --- Parse the 'multi_hot_label' string back into a list ---\n",
    "#             # Use ast.literal_eval assuming labels were saved as standard list strings '[1.0, 0.0,...]'\n",
    "#             logging.info(\"Attempting to parse 'multi_hot_label' column using ast.literal_eval...\")\n",
    "#             label_parser = ast.literal_eval # <<<--- Using ast.literal_eval\n",
    "#             # label_parser = parse_numpy_array_string # Keep commented out\n",
    "\n",
    "#             # Ensure the column name matches your CSV ('multi_hot_label' based on your previous code)\n",
    "#             label_col_name = 'multi_hot_label'\n",
    "#             if label_col_name not in self.manifest.columns:\n",
    "#                  raise KeyError(f\"Column '{label_col_name}' not found in manifest CSV at {manifest_path}\")\n",
    "\n",
    "#             self.manifest[label_col_name] = self.manifest[label_col_name].apply(label_parser)\n",
    "\n",
    "#             # Verification step\n",
    "#             first_label = self.manifest[label_col_name].iloc[0]\n",
    "#             if not isinstance(first_label, list):\n",
    "#                  raise TypeError(f\"Parsed label is not a list, check parser/CSV format. Got type: {type(first_label)}\")\n",
    "#             global num_labels # Make sure num_labels is defined/loaded in Cell 2\n",
    "#             if len(first_label) != num_labels:\n",
    "#                 logging.warning(f\"Parsed label length ({len(first_label)}) does not match expected num_labels ({num_labels}). Check parsing or unified_genres.txt.\")\n",
    "#             logging.info(f\"Example parsed label (type {type(first_label)}, length {len(first_label)}): {str(first_label)[:100]}...\")\n",
    "\n",
    "#             logging.info(f\"Loaded and parsed manifest with {len(self.manifest)} entries.\")\n",
    "\n",
    "#         except FileNotFoundError:\n",
    "#              logging.error(f\"Manifest file not found: {manifest_path}\", exc_info=True)\n",
    "#              raise\n",
    "#         except Exception as e:\n",
    "#             logging.error(f\"Error loading or parsing manifest {manifest_path}: {e}\", exc_info=True)\n",
    "#             raise\n",
    "\n",
    "#     def __len__(self):\n",
    "#         \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "#         return len(self.manifest)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         \"\"\"\n",
    "#         Loads raw audio for index idx, processes it with the feature extractor,\n",
    "#         and returns the processed inputs and labels.\n",
    "#         \"\"\"\n",
    "#         if torch.is_tensor(idx): idx = idx.tolist() # Handle tensor indices\n",
    "\n",
    "#         try:\n",
    "#             # Get the row data from the manifest\n",
    "#             row = self.manifest.iloc[idx]\n",
    "#             track_id = row.get('track_id', self.manifest.index[idx]) # Get track_id safely\n",
    "#             multi_hot_label = row['multi_hot_label'] # Use the correct column name\n",
    "#             audio_path = row['audio_path']\n",
    "\n",
    "#             # Construct absolute audio path if necessary\n",
    "#             # NOTE: originally, the mel-spectrogram's path is relative  but the raw audio's path is absolute, so we need to make sure the audio_path is absolute\n",
    "#             # So we are check if the audio_path is absolute or relative in case we load the wrong data, if it's relative, we need to join it with the PROJECT_ROOT\n",
    "#             if not os.path.isabs(audio_path):\n",
    "#                 # Assumes path in manifest is relative to PROJECT_ROOT defined in config\n",
    "#                 audio_path = os.path.join(config.PROJECT_ROOT, audio_path)\n",
    "\n",
    "#             # --- 1. Load RAW Audio Waveform ---\n",
    "#             waveform, loaded_sr = librosa.load(\n",
    "#                 audio_path,\n",
    "#                 sr=self.target_sr, # Use extractor's sampling rate\n",
    "#                 duration=30.0      # Load the full 30 seconds\n",
    "#             )\n",
    "#             min_samples = int(0.1 * self.target_sr)\n",
    "#             if len(waveform) < min_samples:\n",
    "#                  raise ValueError(f\"Audio signal for track {track_id} too short after loading.\")\n",
    "\n",
    "#             # --- 2. Apply Feature Extractor ---\n",
    "#             # Let the Data Collator handle padding/truncation later\n",
    "#             inputs = self.feature_extractor(\n",
    "#                 waveform,\n",
    "#                 sampling_rate=self.target_sr,\n",
    "#                 return_tensors=\"pt\",\n",
    "#                 # REMOVED padding/truncation args\n",
    "#                 return_attention_mask=True # Keep requesting mask\n",
    "#             )\n",
    "\n",
    "#             # --- 3. Prepare Outputs ---\n",
    "#             feature_tensor = inputs.get('input_values', inputs.get('input_features'))\n",
    "#             if feature_tensor is None:\n",
    "#                 raise KeyError(f\"Expected 'input_values' or 'input_features' key from feature_extractor output for track {track_id}. Got keys: {inputs.keys()}\")\n",
    "#             feature_tensor = feature_tensor.squeeze(0)\n",
    "\n",
    "#             attention_mask = inputs.get('attention_mask', None)\n",
    "#             if attention_mask is not None:\n",
    "#                 attention_mask = attention_mask.squeeze(0)\n",
    "\n",
    "#             # Convert label list/array to float tensor\n",
    "#             label_tensor = torch.tensor(multi_hot_label, dtype=torch.float32)\n",
    "\n",
    "#             # Return dictionary\n",
    "#             model_input_dict = {\"labels\": label_tensor}\n",
    "#             input_key = 'input_values' if 'input_values' in inputs else 'input_features'\n",
    "#             model_input_dict[input_key] = feature_tensor\n",
    "#             if attention_mask is not None:\n",
    "#                 model_input_dict['attention_mask'] = attention_mask\n",
    "\n",
    "#             return model_input_dict\n",
    "\n",
    "#         except FileNotFoundError:\n",
    "#             logging.error(f\"Audio file not found for track {track_id} at {audio_path}\")\n",
    "#             # Return None ONLY IF collate_fn handles it, otherwise raise\n",
    "#             raise\n",
    "#         except Exception as e:\n",
    "#             logging.error(f\"Error in __getitem__ for track {track_id}: {e}\", exc_info=True)\n",
    "#             # Return None ONLY IF collate_fn handles it, otherwise raise\n",
    "#             raise\n",
    "\n",
    "# print(\"FMARawAudioDataset class defined (using raw audio, feature extractor, ast.literal_eval for labels).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Cell 3: Dataset Class Definition (Raw Audio Version) This cell uses the ast.literal_eval parser to parse the multi_hot_label string back into a list of integers.\n",
    "\n",
    "\n",
    "# import re\n",
    "# import ast \n",
    "\n",
    "\n",
    "\n",
    "# # # Define(recollect)the regex parser from preprocess.py if needed,\n",
    "# # # otherwise use ast.literal_eval---\n",
    "# # # NOTE: After changing usage.ipynb 05/03/2025, should fall back to ast.literal_eval now. Clean code later\n",
    "# # def parse_numpy_array_string(array_str):\n",
    "# #     \"\"\"\n",
    "# #     Parse strings like '[np.float32(1.0), np.float32(0.0), ...]' into a list of integers.\n",
    "# #     This is needed because ast.literal_eval cannot handle 'np.float32()' in the string.\n",
    "# #     \"\"\"\n",
    "# #     if not isinstance(array_str, str):\n",
    "# #         return []\n",
    "# #     try:\n",
    "# #         # Extract all the float values using regular expressions\n",
    "# #         # Refined regex to handle numbers with or without decimal points\n",
    "# #         float_matches = re.findall(r'np\\.float32\\(([\\d\\.]+)\\)', array_str)\n",
    "# #         # Convert matches to floats then maybe int (use float for BCE loss)\n",
    "# #         values = []\n",
    "# #         for match in float_matches:\n",
    "# #             value = float(match)\n",
    "# #             values.append(1.0 if value == 1.0 else 0.0) # Store as float 0.0 or 1.0\n",
    "# #         return values\n",
    "# #     except Exception as e:\n",
    "# #         logging.warning(f\"Error parsing array string: {e}\")\n",
    "# #         return []\n",
    "# # --- End commented out parser ---\n",
    "\n",
    "\n",
    "# class FMARawAudioDataset(Dataset):\n",
    "#     \"\"\"\n",
    "#     Loads raw audio waveforms and labels from manifest, uses Hugging Face\n",
    "#     feature extractor (like ASTFeatureExtractor or Wav2Vec2Processor/AutoFeatureExtractor) on the fly.\n",
    "#     Assumes padding/truncation will be handled by a collate function.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, manifest_path, feature_extractor):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             manifest_path (str): Path to the final manifest CSV file (e.g., small_subset_multihot.csv).\n",
    "#             feature_extractor: Initialized Hugging Face AutoFeatureExtractor instance.\n",
    "#         \"\"\"\n",
    "#         logging.info(f\"Initializing FMARawAudioDataset from: {manifest_path}\")\n",
    "#         if feature_extractor is None:\n",
    "#              raise ValueError(\"FMARawAudioDataset requires a feature_extractor instance.\")\n",
    "\n",
    "#         self.feature_extractor = feature_extractor\n",
    "#         try:\n",
    "#              self.target_sr = self.feature_extractor.sampling_rate\n",
    "#              logging.info(f\"Target sampling rate set from feature extractor: {self.target_sr} Hz\")\n",
    "#         except AttributeError:\n",
    "#              logging.error(\"Could not get sampling_rate from feature_extractor.\", exc_info=True)\n",
    "#              raise\n",
    "\n",
    "#         logging.info(f\"Loading manifest from: {manifest_path}\")\n",
    "#         try:\n",
    "#             self.manifest = pd.read_csv(manifest_path)\n",
    "#             if 'track_id' in self.manifest.columns:\n",
    "#                  self.manifest = self.manifest.set_index('track_id', drop=False)\n",
    "\n",
    "#             # --- Parse the 'multi_hot_label' string back into a list ---\n",
    "#             # Use ast.literal_eval assuming labels were saved as standard list strings '[1.0, 0.0,...]'\n",
    "#             logging.info(\"Attempting to parse 'multi_hot_label' column using ast.literal_eval...\")\n",
    "#             label_parser = ast.literal_eval # <<<--- Using ast.literal_eval\n",
    "#             # label_parser = parse_numpy_array_string # Keep commented out\n",
    "\n",
    "#             # Ensure the column name matches your CSV ('multi_hot_label' based on your previous code)\n",
    "#             label_col_name = 'multi_hot_label'\n",
    "#             if label_col_name not in self.manifest.columns:\n",
    "#                  raise KeyError(f\"Column '{label_col_name}' not found in manifest CSV at {manifest_path}\")\n",
    "\n",
    "#             self.manifest[label_col_name] = self.manifest[label_col_name].apply(label_parser)\n",
    "\n",
    "#             # Verification step\n",
    "#             first_label = self.manifest[label_col_name].iloc[0]\n",
    "#             if not isinstance(first_label, list):\n",
    "#                  raise TypeError(f\"Parsed label is not a list, check parser/CSV format. Got type: {type(first_label)}\")\n",
    "#             global num_labels # Make sure num_labels is defined/loaded in Cell 2\n",
    "#             if len(first_label) != num_labels:\n",
    "#                 logging.warning(f\"Parsed label length ({len(first_label)}) does not match expected num_labels ({num_labels}). Check parsing or unified_genres.txt.\")\n",
    "#             logging.info(f\"Example parsed label (type {type(first_label)}, length {len(first_label)}): {str(first_label)[:100]}...\")\n",
    "\n",
    "#             logging.info(f\"Loaded and parsed manifest with {len(self.manifest)} entries.\")\n",
    "\n",
    "#         except FileNotFoundError:\n",
    "#              logging.error(f\"Manifest file not found: {manifest_path}\", exc_info=True)\n",
    "#              raise\n",
    "#         except Exception as e:\n",
    "#             logging.error(f\"Error loading or parsing manifest {manifest_path}: {e}\", exc_info=True)\n",
    "#             raise\n",
    "\n",
    "#     def __len__(self):\n",
    "#         \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "#         return len(self.manifest)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         \"\"\"\n",
    "#         Loads raw audio for index idx, processes it with the feature extractor,\n",
    "#         and returns the processed inputs and labels.\n",
    "#         \"\"\"\n",
    "#         if torch.is_tensor(idx): idx = idx.tolist() # Handle tensor indices\n",
    "\n",
    "#         try:\n",
    "#             # Get the row data from the manifest\n",
    "#             row = self.manifest.loc[idx]\n",
    "#             track_id = row.get('track_id', self.manifest.index[idx]) # Get track_id safely\n",
    "#             multi_hot_label = row['multi_hot_label'] # Use the correct column name\n",
    "#             audio_path = row['audio_path']\n",
    "\n",
    "#             # Construct absolute audio path if necessary\n",
    "#             # NOTE: originally, the mel-spectrogram's path is relative  but the raw audio's path is absolute, so we need to make sure the audio_path is absolute\n",
    "#             # So we are check if the audio_path is absolute or relative in case we load the wrong data, if it's relative, we need to join it with the PROJECT_ROOT\n",
    "#             if not os.path.isabs(audio_path):\n",
    "#                 # Assumes path in manifest is relative to PROJECT_ROOT defined in config\n",
    "#                 audio_path = os.path.join(config.PROJECT_ROOT, audio_path)\n",
    "\n",
    "#             # --- 1. Load RAW Audio Waveform ---\n",
    "#             waveform, loaded_sr = librosa.load(\n",
    "#                 audio_path,\n",
    "#                 sr=self.target_sr, # Use extractor's sampling rate\n",
    "#                 duration=30.0      # Load the full 30 seconds\n",
    "#             )\n",
    "#             min_samples = int(0.1 * self.target_sr)\n",
    "#             if len(waveform) < min_samples:\n",
    "#                  raise ValueError(f\"Audio signal for track {track_id} too short after loading.\")\n",
    "\n",
    "#             # --- 2. Apply Feature Extractor ---\n",
    "#             # Let the Data Collator handle padding/truncation later\n",
    "#             inputs = self.feature_extractor(\n",
    "#                 waveform,\n",
    "#                 sampling_rate=self.target_sr,\n",
    "#                 return_tensors=\"pt\",\n",
    "#                 # REMOVED padding/truncation args\n",
    "#                 return_attention_mask=True # Keep requesting mask\n",
    "#             )\n",
    "\n",
    "#             # --- 3. Prepare Outputs ---\n",
    "#             feature_tensor = inputs.get('input_values', inputs.get('input_features'))\n",
    "#             if feature_tensor is None:\n",
    "#                 raise KeyError(f\"Expected 'input_values' or 'input_features' key from feature_extractor output for track {track_id}. Got keys: {inputs.keys()}\")\n",
    "#             feature_tensor = feature_tensor.squeeze(0)\n",
    "\n",
    "#             attention_mask = inputs.get('attention_mask', None)\n",
    "#             if attention_mask is not None:\n",
    "#                 attention_mask = attention_mask.squeeze(0)\n",
    "\n",
    "#             # Convert label list/array to float tensor\n",
    "#             label_tensor = torch.tensor(multi_hot_label, dtype=torch.float32)\n",
    "\n",
    "#             # Return dictionary\n",
    "#             model_input_dict = {\"labels\": label_tensor}\n",
    "#             input_key = 'input_values' if 'input_values' in inputs else 'input_features'\n",
    "#             model_input_dict[input_key] = feature_tensor\n",
    "#             if attention_mask is not None:\n",
    "#                 model_input_dict['attention_mask'] = attention_mask\n",
    "\n",
    "#             return model_input_dict\n",
    "\n",
    "#         except FileNotFoundError:\n",
    "#             logging.error(f\"Audio file not found for track {track_id} at {audio_path}\")\n",
    "#             # Return None ONLY IF collate_fn handles it, otherwise raise\n",
    "#             raise\n",
    "#         except Exception as e:\n",
    "#             logging.error(f\"Error in __getitem__ for track {track_id}: {e}\", exc_info=True)\n",
    "#             # Return None ONLY IF collate_fn handles it, otherwise raise\n",
    "#             raise\n",
    "\n",
    "# print(\"FMARawAudioDataset class defined (using raw audio, feature extractor, ast.literal_eval for labels).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FMARawAudioDataset class defined (using raw audio, feature extractor, ast.literal_eval for labels, .loc access).\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Dataset Class Definition (Raw Audio Version - Corrected .loc access)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import ast # For parsing label string '[1.0, 0.0,...]'\n",
    "import re  # Keep import for the commented out function below\n",
    "import logging\n",
    "import librosa\n",
    "# Ensure config is imported from a previous cell or uncomment:\n",
    "# import config\n",
    "\n",
    "# --- Optional: Keep custom parser commented out for reference ---\n",
    "# # Define(recollect)the regex parser from preprocess.py if needed,\n",
    "# # otherwise use ast.literal_eval---\n",
    "# # NOTE: After changing usage.ipynb 05/03/2025, should fall back to ast.literal_eval now. Clean code later\n",
    "# def parse_numpy_array_string(array_str):\n",
    "#     \"\"\"\n",
    "#     Parse strings like '[np.float32(1.0), np.float32(0.0), ...]' into a list of integers.\n",
    "#     This is needed because ast.literal_eval cannot handle 'np.float32()' in the string.\n",
    "#     \"\"\"\n",
    "#     if not isinstance(array_str, str): return []\n",
    "#     try:\n",
    "#         # Match digits, optionally followed by a decimal and more digits\n",
    "#         float_matches = re.findall(r'np\\.float32\\(([\\d\\.]+)\\)', array_str)\n",
    "#         values = []\n",
    "#         for match_str in float_matches:\n",
    "#             value = float(match_str) # Convert string match to float\n",
    "#             values.append(1.0 if value == 1.0 else 0.0) # Store as float 0.0 or 1.0\n",
    "#         return values\n",
    "#     except Exception as e:\n",
    "#         logging.warning(f\"Error parsing array string: {e}\")\n",
    "#         return []\n",
    "# --- End commented out parser ---\n",
    "\n",
    "\n",
    "class FMARawAudioDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads raw audio waveforms and labels from manifest, uses Hugging Face\n",
    "    feature extractor (like ASTFeatureExtractor or Wav2Vec2Processor/AutoFeatureExtractor) on the fly.\n",
    "    Assumes padding/truncation will be handled by a collate function.\n",
    "    \"\"\"\n",
    "    def __init__(self, manifest_path, feature_extractor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            manifest_path (str): Path to the final manifest CSV file (e.g., small_subset_multihot.csv).\n",
    "            feature_extractor: Initialized Hugging Face AutoFeatureExtractor instance.\n",
    "        \"\"\"\n",
    "        # Ensure num_labels is available globally or passed if needed for verification\n",
    "        global num_labels\n",
    "        if 'num_labels' not in globals():\n",
    "             logging.error(\"Global variable 'num_labels' not found. Load it first (e.g., from Cell 2).\")\n",
    "             # Alternative: pass num_labels as an argument to __init__\n",
    "\n",
    "        logging.info(f\"Initializing FMARawAudioDataset from: {manifest_path}\")\n",
    "        if feature_extractor is None:\n",
    "             raise ValueError(\"FMARawAudioDataset requires a feature_extractor instance.\")\n",
    "\n",
    "        self.feature_extractor = feature_extractor\n",
    "        try:\n",
    "             self.target_sr = self.feature_extractor.sampling_rate\n",
    "             logging.info(f\"Target sampling rate set from feature extractor: {self.target_sr} Hz\")\n",
    "        except AttributeError:\n",
    "             logging.error(\"Could not get sampling_rate from feature_extractor.\", exc_info=True)\n",
    "             raise\n",
    "\n",
    "        logging.info(f\"Loading manifest from: {manifest_path}\")\n",
    "        try:\n",
    "            self.manifest = pd.read_csv(manifest_path)\n",
    "            # Set index to track_id AFTER loading, keep column too if needed elsewhere\n",
    "            if 'track_id' in self.manifest.columns:\n",
    "                 self.manifest = self.manifest.set_index('track_id', drop=False) # Keep column if row.get('track_id'...) is used\n",
    "            else:\n",
    "                 logging.warning(\"Manifest CSV does not contain 'track_id' column. Using DataFrame index.\")\n",
    "                 # Make sure index IS the track_id\n",
    "                 if not pd.api.types.is_integer_dtype(self.manifest.index):\n",
    "                      logging.warning(\"Manifest index is not integer type. Ensure it matches track IDs.\")\n",
    "\n",
    "\n",
    "            # --- Parse the 'multi_hot_label' string back into a list ---\n",
    "            # NOTE: After changing usage.ipynb 05/03/2025, should fall back to ast.literal_eval now. Clean code later\n",
    "            # Use ast.literal_eval assuming labels were saved as standard list strings '[1.0, 0.0,...]'\n",
    "            logging.info(\"Attempting to parse 'multi_hot_label' column using ast.literal_eval...\")\n",
    "            label_parser = ast.literal_eval # <<<--- Using ast.literal_eval\n",
    "            # label_parser = parse_numpy_array_string # Keep commented out as requested\n",
    "\n",
    "            label_col_name = 'multi_hot_label'\n",
    "            if label_col_name not in self.manifest.columns:\n",
    "                 raise KeyError(f\"Column '{label_col_name}' not found in manifest CSV at {manifest_path}\")\n",
    "\n",
    "            self.manifest[label_col_name] = self.manifest[label_col_name].apply(label_parser)\n",
    "\n",
    "            # --- Verification step ---\n",
    "            first_label = self.manifest[label_col_name].iloc[0] # Use iloc[0] here to get FIRST row for checking\n",
    "            if not isinstance(first_label, list):\n",
    "                 raise TypeError(f\"Parsed label is not a list, check parser/CSV format. Got type: {type(first_label)}\")\n",
    "            # Check length against num_labels loaded in Cell 2\n",
    "            if len(first_label) != num_labels:\n",
    "                 logging.error(f\"FATAL: Parsed label length ({len(first_label)}) does not match expected num_labels ({num_labels}). Check parsing or unified_genres.txt.\")\n",
    "                 raise ValueError(\"Parsed label length mismatch.\")\n",
    "            logging.info(f\"Example parsed label verified (type {type(first_label)}, length {len(first_label)}): {str(first_label)[:100]}...\")\n",
    "            # --- End Verification ---\n",
    "\n",
    "            logging.info(f\"Loaded and parsed manifest with {len(self.manifest)} entries.\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "             logging.error(f\"Manifest file not found: {manifest_path}\", exc_info=True)\n",
    "             raise\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading or parsing manifest {manifest_path}: {e}\", exc_info=True)\n",
    "            raise\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.manifest)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Loads raw audio for index 'idx' (which is the track_id/index label),\n",
    "        processes it with the feature extractor,\n",
    "        and returns the processed inputs and labels.\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx): idx = idx.tolist() # Handle tensor indices\n",
    "\n",
    "        # --- Use idx directly as track_id BEFORE main try block ---\n",
    "        track_id = idx\n",
    "        # ---------------------------------------------------------\n",
    "\n",
    "        try:\n",
    "            # --- Get the row data using .loc with the track_id ---\n",
    "            row = self.manifest.loc[track_id] # Use .loc with the index label (track_id)\n",
    "            # ------------------------------------------------------\n",
    "\n",
    "            # --- Get required data from the row ---\n",
    "            multi_hot_label = row['multi_hot_label']\n",
    "            audio_path = row['audio_path']\n",
    "            # ---------------------------------------\n",
    "\n",
    "            # Construct absolute audio path if necessary (keep your NOTE)\n",
    "            # NOTE: originally, the mel-spectrogram's path is relative  but the raw audio's path is absolute, so we need to make sure the audio_path is absolute\n",
    "            # So we are check if the audio_path is absolute or relative in case we load the wrong data, if it's relative, we need to join it with the PROJECT_ROOT\n",
    "            if not os.path.isabs(audio_path):\n",
    "                audio_path = os.path.join(config.PROJECT_ROOT, audio_path)\n",
    "\n",
    "            # --- 1. Load RAW Audio Waveform ---\n",
    "            waveform, loaded_sr = librosa.load(\n",
    "                audio_path,\n",
    "                sr=self.target_sr, # Use extractor's sampling rate\n",
    "                duration=30.0      # Load the full 30 seconds\n",
    "            )\n",
    "            min_samples = int(0.1 * self.target_sr)\n",
    "            if len(waveform) < min_samples:\n",
    "                 logging.warning(f\"Audio signal for track {track_id} too short, returning None.\")\n",
    "                 return None # Requires collate_fn to handle None\n",
    "\n",
    "            # --- 2. Apply Feature Extractor ---\n",
    "            # Let the Data Collator handle padding/truncation later\n",
    "            inputs = self.feature_extractor(\n",
    "                waveform,\n",
    "                sampling_rate=self.target_sr,\n",
    "                return_tensors=\"pt\",\n",
    "                # REMOVED padding/truncation/max_length args\n",
    "                return_attention_mask=True # Keep requesting mask\n",
    "            )\n",
    "\n",
    "            # --- 3. Prepare Outputs ---\n",
    "            feature_tensor = inputs.get('input_values', inputs.get('input_features'))\n",
    "            if feature_tensor is None:\n",
    "                raise KeyError(f\"Expected 'input_values' or 'input_features' key from feature_extractor output. Got keys: {inputs.keys()}\")\n",
    "            feature_tensor = feature_tensor.squeeze(0)\n",
    "\n",
    "            attention_mask = inputs.get('attention_mask', None)\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = attention_mask.squeeze(0)\n",
    "\n",
    "            # Convert label list to float tensor\n",
    "            label_tensor = torch.tensor(multi_hot_label, dtype=torch.float32)\n",
    "\n",
    "            # Return dictionary\n",
    "            model_input_dict = {\"labels\": label_tensor}\n",
    "            input_key = 'input_values' if 'input_values' in inputs else 'input_features'\n",
    "            model_input_dict[input_key] = feature_tensor\n",
    "            if attention_mask is not None:\n",
    "                model_input_dict['attention_mask'] = attention_mask\n",
    "\n",
    "            return model_input_dict\n",
    "\n",
    "        except KeyError:\n",
    "             # This might catch if track_id wasn't found by .loc (handled above),\n",
    "             # or if column names like 'multi_hot_label', 'audio_path' are wrong in CSV\n",
    "             logging.error(f\"KeyError accessing data for track {track_id}. Check manifest columns.\", exc_info=True)\n",
    "             return None\n",
    "        except FileNotFoundError:\n",
    "             logging.error(f\"Audio file not found for track {track_id} at {audio_path}\")\n",
    "             return None\n",
    "        except Exception as e:\n",
    "            # Use the track_id obtained safely before the try block\n",
    "            logging.error(f\"Error loading/processing track {track_id}: {e}\", exc_info=True)\n",
    "            return None # Return None on generic error\n",
    "\n",
    "print(\"FMARawAudioDataset class defined (using raw audio, feature extractor, ast.literal_eval for labels, .loc access).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "facebook/w2v-bert-2.0\n"
     ]
    }
   ],
   "source": [
    "print(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 3.5: Define Data Collator for Padding (Corrected Padding Logic)\n",
    "\n",
    "# import torch\n",
    "# from dataclasses import dataclass\n",
    "# from typing import Dict, List, Optional, Union\n",
    "# # from transformers.feature_extraction_utils import BatchFeature # Not strictly needed here\n",
    "\n",
    "# @dataclass\n",
    "# class DataCollatorAudio:\n",
    "#     \"\"\"\n",
    "#     Data collator that dynamically pads the inputs received Feature Extractor.\n",
    "#     Correctly handles padding for [SequenceLength, FeatureDim] tensors.\n",
    "#     \"\"\"\n",
    "#     padding_value: float = 0.0 # Standard padding for features/audio\n",
    "\n",
    "#     def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "#         # features is a list of dicts like [{'input_values': tensor1, 'labels': label1, 'attention_mask': mask1}, ...]\n",
    "\n",
    "#         # --- Pad 'input_values' (or 'input_features') ---\n",
    "#         input_key = 'input_values' if 'input_values' in features[0] else 'input_features'\n",
    "#         input_features = [d[input_key] for d in features]\n",
    "\n",
    "#         # Determine max sequence length *in this batch* (assuming shape [SeqLen, FeatureDim])\n",
    "#         # Add check for empty list\n",
    "#         if not input_features:\n",
    "#              return {}\n",
    "#         max_len = max(feat.shape[0] for feat in input_features) # <<<--- Get length of FIRST dimension\n",
    "\n",
    "#         # Pad each feature tensor to max_len along the sequence dimension (first dim)\n",
    "#         padded_features = []\n",
    "#         for feat in input_features:\n",
    "#             # feat shape is [SeqLen, FeatureDim]\n",
    "#             num_frames = feat.shape[0]\n",
    "#             num_features = feat.shape[1] # Should be consistent (e.g., 160)\n",
    "#             pad_width = max_len - num_frames\n",
    "\n",
    "#             # Pad argument format for 2D tensor: (pad_left_dim1, pad_right_dim1, pad_left_dim0, pad_right_dim0)\n",
    "#             # We only want to pad the end of the sequence dimension (dim 0)\n",
    "#             # (0, 0) means no padding on left/right of feature dim (dim 1)\n",
    "#             # (0, pad_width) means 0 padding before seq dim (dim 0), pad_width padding after\n",
    "#             padded_feat = torch.nn.functional.pad(feat, (0, 0, 0, pad_width), mode='constant', value=self.padding_value)\n",
    "#             # Verify shape after padding\n",
    "#             # print(f\"Original shape: {feat.shape}, Padded shape: {padded_feat.shape}, Target max_len: {max_len}\")\n",
    "#             padded_features.append(padded_feat)\n",
    "\n",
    "#         # Stack the padded features into a batch tensor\n",
    "#         # Now all tensors in padded_features should have shape [max_len, FeatureDim]\n",
    "#         try:\n",
    "#              batch_input_features = torch.stack(padded_features) # Shape: [BatchSize, max_len, FeatureDim]\n",
    "#         except RuntimeError as e:\n",
    "#              logging.error(f\"RuntimeError during torch.stack. Shapes in batch might still differ or be incompatible.\")\n",
    "#              # Print shapes for debugging\n",
    "#              for i, p_feat in enumerate(padded_features): logging.error(f\" Padded shape {i}: {p_feat.shape}\")\n",
    "#              raise e\n",
    "\n",
    "\n",
    "#         # --- Prepare Batch Dictionary ---\n",
    "#         batch = {\"input_values\": batch_input_features}\n",
    "\n",
    "#         # --- Pad 'attention_mask' if present ---\n",
    "#         # Attention mask usually has shape [SeqLen]\n",
    "#         if \"attention_mask\" in features[0] and features[0][\"attention_mask\"] is not None:\n",
    "#             attention_masks = [d[\"attention_mask\"] for d in features]\n",
    "#             padded_masks = []\n",
    "#             for mask in attention_masks:\n",
    "#                  pad_width = max_len - mask.shape[-1] # Pad last dimension (the sequence length)\n",
    "#                  # Pad argument format for 1D tensor: (pad_left, pad_right)\n",
    "#                  padded_mask = torch.nn.functional.pad(mask, (0, pad_width), mode='constant', value=0) # Pad attention mask with 0\n",
    "#                  padded_masks.append(padded_mask)\n",
    "#             batch[\"attention_mask\"] = torch.stack(padded_masks) # Shape: [BatchSize, max_len]\n",
    "\n",
    "#         # --- Stack Labels ---\n",
    "#         labels = [d[\"labels\"] for d in features]\n",
    "#         batch[\"labels\"] = torch.stack(labels) # Shape: [BatchSize, num_labels]\n",
    "\n",
    "#         return batch\n",
    "\n",
    "# # Create an instance of the collator (do this in Cell 4)\n",
    "# # data_collator = DataCollatorAudio()\n",
    "# # print(\"DataCollatorAudio defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3.5: Define Data Collator for Padding (Handles None values)\n",
    "\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Union\n",
    "import logging # Add logging\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorAudio:\n",
    "    \"\"\"\n",
    "    Data collator that dynamically pads the inputs received Feature Extractor.\n",
    "    Handles None values returned by the Dataset on error.\n",
    "    \"\"\"\n",
    "    padding_value: float = 0.0 # Standard padding for features/audio\n",
    "\n",
    "    def __call__(self, features: List[Optional[Dict[str, Union[List[int], torch.Tensor]]]]) -> Dict[str, torch.Tensor]:\n",
    "        # features is a list of dicts OR None values from __getitem__\n",
    "\n",
    "        # --- Filter out None entries ---\n",
    "        valid_features = [f for f in features if f is not None]\n",
    "        if not valid_features:\n",
    "             # If all samples in the batch failed, return an empty dictionary\n",
    "             # The training loop should ideally handle this (e.g., skip batch)\n",
    "             logging.warning(\"Collate function received empty batch after filtering Nones.\")\n",
    "             return {}\n",
    "        # -----------------------------\n",
    "\n",
    "        # --- Determine keys and pad based on valid features ---\n",
    "        input_key = 'input_values' if 'input_values' in valid_features[0] else 'input_features'\n",
    "        input_features = [d[input_key] for d in valid_features]\n",
    "\n",
    "        # Determine sequence length dimension based on the FIRST valid tensor\n",
    "        seq_len_dim = -1\n",
    "        if len(input_features[0].shape) == 2:\n",
    "            seq_len_dim = 0 if input_features[0].shape[0] > input_features[0].shape[1] else -1\n",
    "        elif len(input_features[0].shape) == 1:\n",
    "             seq_len_dim = 0\n",
    "        else:\n",
    "             logging.warning(f\"Unexpected tensor shape {input_features[0].shape}, assuming seq len is last dim.\")\n",
    "\n",
    "        max_len = max(feat.shape[seq_len_dim] for feat in input_features)\n",
    "\n",
    "        # Pad each feature tensor to max_len\n",
    "        padded_features = []\n",
    "        for feat in input_features:\n",
    "            pad_width = max_len - feat.shape[seq_len_dim]\n",
    "            if seq_len_dim == 0 and len(feat.shape)==2: padding = (0, 0, 0, pad_width) # Pad SeqLen dim (dim 0)\n",
    "            else: padding = (0, pad_width) # Pad last dim (SeqLen)\n",
    "\n",
    "            padded_feat = torch.nn.functional.pad(feat, padding, mode='constant', value=self.padding_value)\n",
    "            padded_features.append(padded_feat)\n",
    "\n",
    "        # Stack the padded features\n",
    "        batch_input_features = torch.stack(padded_features)\n",
    "        batch = {input_key: batch_input_features} # Use the correct key\n",
    "\n",
    "        # Pad 'attention_mask' if present\n",
    "        if \"attention_mask\" in valid_features[0] and valid_features[0][\"attention_mask\"] is not None:\n",
    "            attention_masks = [d[\"attention_mask\"] for d in valid_features]\n",
    "            # Assuming mask is 1D [SeqLen] or 2D [1, SeqLen] etc. - pad last dim\n",
    "            max_mask_len = max(m.shape[-1] for m in attention_masks)\n",
    "            padded_masks = []\n",
    "            for mask in attention_masks:\n",
    "                 pad_width = max_mask_len - mask.shape[-1]\n",
    "                 padded_mask = torch.nn.functional.pad(mask, (0, pad_width), mode='constant', value=0)\n",
    "                 padded_masks.append(padded_mask)\n",
    "            batch[\"attention_mask\"] = torch.stack(padded_masks)\n",
    "\n",
    "        # Stack Labels\n",
    "        labels = [d[\"labels\"] for d in valid_features]\n",
    "        batch[\"labels\"] = torch.stack(labels)\n",
    "\n",
    "        return batch\n",
    "\n",
    "# Create an instance of the collator (do this in Cell 4)\n",
    "# data_collator = DataCollatorAudio()\n",
    "# print(\"DataCollatorAudio defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 04:27:11,087 - INFO - Loading feature extractor for: facebook/w2v-bert-2.0\n",
      "2025-05-04 04:27:11,212 - INFO - Feature extractor loaded successfully.\n",
      "Feature extractor expects sample rate: 16000\n",
      "2025-05-04 04:27:11,214 - INFO - Initializing FMARawAudioDataset from: /workspace/musicClaGen/data/processed/small_subset_multihot.csv\n",
      "2025-05-04 04:27:11,215 - INFO - Target sampling rate set from feature extractor: 16000 Hz\n",
      "2025-05-04 04:27:11,217 - INFO - Loading manifest from: /workspace/musicClaGen/data/processed/small_subset_multihot.csv\n",
      "2025-05-04 04:27:11,255 - INFO - Attempting to parse 'multi_hot_label' column using ast.literal_eval...\n",
      "2025-05-04 04:27:11,576 - INFO - Example parsed label verified (type <class 'list'>, length 22): [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "2025-05-04 04:27:11,578 - INFO - Loaded and parsed manifest with 8000 entries.\n",
      "2025-05-04 04:27:11,579 - INFO - Creating DEBUG DataLoaders with small subsets and custom collator...\n",
      "DataCollatorAudio instance created.\n",
      "2025-05-04 04:27:11,585 - INFO - DEBUG Dataset sizes: Train=16, Val=8\n",
      "2025-05-04 04:27:11,586 - INFO - DEBUG DataLoaders with custom collator created.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load Feature Extractor, Create DataLoaders with Custom Collator\n",
    "\n",
    "from transformers import AutoFeatureExtractor # Use the correct class\n",
    "\n",
    "# Ensure FMARawAudioDataset and DataCollatorAudio are defined in previous cells\n",
    "\n",
    "# --- Load Feature Extractor ---\n",
    "# (Using model_checkpoint defined in Cell 2)\n",
    "logging.info(f\"Loading feature extractor for: {model_checkpoint}\")\n",
    "try:\n",
    "    # Load the feature extractor associated with Wav2Vec2-BERT\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint)\n",
    "    logging.info(\"Feature extractor loaded successfully.\")\n",
    "    # Log the expected sample rate\n",
    "    processor_sr = feature_extractor.sampling_rate\n",
    "    print(f\"Feature extractor expects sample rate: {processor_sr}\")\n",
    "    # Ensure config matches extractor's expected rate\n",
    "    if config.PREPROCESSING_PARAMS['sample_rate'] != processor_sr:\n",
    "         logging.warning(f\"Config sample rate ({config.PREPROCESSING_PARAMS['sample_rate']}) differs from feature extractor ({processor_sr}). Ensure audio loading uses {processor_sr} Hz.\")\n",
    "         # Update config value if necessary, or ensure Dataset uses processor_sr\n",
    "         # config.PREPROCESSING_PARAMS['sample_rate'] = processor_sr # Be careful modifying config dynamically\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Could not load feature extractor for {model_checkpoint}. Cannot proceed. Error: {e}\", exc_info=True)\n",
    "    raise SystemExit # Stop execution if extractor fails\n",
    "\n",
    "# --- Create Full Dataset ---\n",
    "# Ensure FMARawAudioDataset __init__ accepts feature_extractor\n",
    "try:\n",
    "    # Pass the loaded feature_extractor instance\n",
    "    full_dataset = FMARawAudioDataset(manifest_path, feature_extractor=feature_extractor)\n",
    "    manifest_df = full_dataset.manifest\n",
    "except Exception as e:\n",
    "     logging.error(\"Failed to instantiate FMARawAudioDataset.\", exc_info=True)\n",
    "     raise SystemExit\n",
    "\n",
    "# --- Create SMALLER DEBUG Datasets ---\n",
    "logging.info(\"Creating DEBUG DataLoaders with small subsets and custom collator...\")\n",
    "try:\n",
    "    # Get indices for the splits from the manifest\n",
    "    train_indices = manifest_df[manifest_df['split'] == 'training'].index[:16].tolist() # Small subset for debug\n",
    "    val_indices = manifest_df[manifest_df['split'] == 'validation'].index[:8].tolist()  # Small subset for debug\n",
    "\n",
    "    # Create Subset instances\n",
    "    debug_train_dataset = Subset(full_dataset, train_indices)\n",
    "    debug_val_dataset = Subset(full_dataset, val_indices)\n",
    "\n",
    "    # --- Create Data Collator Instance ---\n",
    "    # (Assumes DataCollatorAudio class is defined in Cell 3.5)\n",
    "    data_collator = DataCollatorAudio()\n",
    "    print(\"DataCollatorAudio instance created.\")\n",
    "\n",
    "    # --- Create DataLoaders using the custom collate_fn ---\n",
    "    debug_train_dataloader = DataLoader(\n",
    "        debug_train_dataset,\n",
    "        batch_size=batch_size, # Use small batch_size from config\n",
    "        shuffle=True,\n",
    "        collate_fn=data_collator # Apply custom padding at batch level\n",
    "        # num_workers=4, # Optional: Add workers later for performance\n",
    "        # pin_memory=True # Optional: Add if using GPU\n",
    "    )\n",
    "    debug_val_dataloader = DataLoader(\n",
    "        debug_val_dataset,\n",
    "        batch_size=batch_size, # Use small batch_size from config\n",
    "        shuffle=False, # No need to shuffle validation data\n",
    "        collate_fn=data_collator # Apply custom padding at batch level\n",
    "        # num_workers=4,\n",
    "        # pin_memory=True\n",
    "    )\n",
    "    logging.info(f\"DEBUG Dataset sizes: Train={len(debug_train_dataset)}, Val={len(debug_val_dataset)}\")\n",
    "    logging.info(\"DEBUG DataLoaders with custom collator created.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to create DEBUG datasets/dataloaders: {e}\", exc_info=True)\n",
    "    raise SystemExit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 04:27:11,604 - INFO - Loading pre-trained Wav2Vec2-BERT model: facebook/w2v-bert-2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2BertForSequenceClassification were not initialized from the model checkpoint at facebook/w2v-bert-2.0 and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 04:27:13,932 - INFO - Model loaded initially.\n",
      "2025-05-04 04:27:13,934 - INFO - Found classifier attribute 'classifier' of type <class 'torch.nn.modules.linear.Linear'>\n",
      "2025-05-04 04:27:13,935 - INFO - Replacing classifier head 'classifier'. Original out: 22, New out: 22\n",
      "Successfully replaced classifier head 'classifier'.\n",
      "2025-05-04 04:27:14,875 - INFO - Wav2Vec2-BERT Model loaded and moved to device.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Load Wav2Vec2-BERT Model and Modify Head\n",
    "\n",
    "import torch.nn as nn # Ensure nn is imported\n",
    "from transformers import AutoModelForAudioClassification\n",
    "\n",
    "logging.info(f\"Loading pre-trained Wav2Vec2-BERT model: {model_checkpoint}\")\n",
    "try:\n",
    "    # Load the model configured for audio classification\n",
    "    model = AutoModelForAudioClassification.from_pretrained(\n",
    "        model_checkpoint,\n",
    "        num_labels=num_labels,\n",
    "        ignore_mismatched_sizes=True # Essential for replacing the head\n",
    "    )\n",
    "    logging.info(\"Model loaded initially.\")\n",
    "\n",
    "    # --- Explicit Head Replacement (Recommended) ---\n",
    "    # Though I have defined num_labels = num_labels on previous step, I want to explicitly replace it again to ensure the head is correct.\n",
    "    # If the above code is correct, the explicitly approach below might seem redundant but.\n",
    "    \n",
    "    # I MUST verify the correct attribute name for the classifier head for Wav2Vec2-BERT. \n",
    "    # Common names include 'classifier', 'projector','classification_head'. Use print(model) after loading to check.\n",
    "    classifier_attr = 'classifier' # <<<--- VERIFY THIS ATTRIBUTE NAME ---<<<\n",
    "\n",
    "    if hasattr(model, classifier_attr):\n",
    "        original_classifier = getattr(model, classifier_attr)\n",
    "        logging.info(f\"Found classifier attribute '{classifier_attr}' of type {type(original_classifier)}\")\n",
    "\n",
    "        # Check if it's a simple Linear layer or potentially a sequence/projection\n",
    "        if isinstance(original_classifier, nn.Linear):\n",
    "            in_features = original_classifier.in_features\n",
    "            logging.info(f\"Replacing classifier head '{classifier_attr}'. Original out: {original_classifier.out_features}, New out: {num_labels}\")\n",
    "            setattr(model, classifier_attr, nn.Linear(in_features, num_labels))\n",
    "            print(f\"Successfully replaced classifier head '{classifier_attr}'.\")\n",
    "        # Add checks here if Wav2Vec2-BERT uses a different common head structure\n",
    "        # elif isinstance(original_classifier, nn.Sequential): ... etc.\n",
    "        else:\n",
    "             logging.warning(f\"Classifier head '{classifier_attr}' is not nn.Linear ({type(original_classifier)}). Attempting replacement might fail or need adjustment.\")\n",
    "             # If you know the structure (e.g., model.projector + model.classifier), adjust accordingly.\n",
    "             # For now, we assume a direct replacement might work or the implicit loading handled it.\n",
    "\n",
    "    else:\n",
    "         logging.warning(f\"Could not automatically find classifier attribute '{classifier_attr}'. Ensure head size ({num_labels}) was correctly set via 'num_labels' argument during loading or modify manually.\")\n",
    "\n",
    "    model.to(device)\n",
    "    logging.info(\"Wav2Vec2-BERT Model loaded and moved to device.\")\n",
    "    # print(model) # Uncomment this line and run to inspect the model structure and find the classifier name\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to load model '{model_checkpoint}': {e}\", exc_info=True)\n",
    "    raise SystemExit # Stop if model loading fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the correct attribute name for the classifier head for Wav2Vec2-BERT.\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer, Loss, Metrics Functoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 04:27:14,934 - INFO - Optimizer AdamW defined with LR=5e-05, Weight Decay=0.01\n",
      "2025-05-04 04:27:14,936 - INFO - Loss function BCEWithLogitsLoss defined.\n",
      "Optimizer, Loss, and compute_metrics function defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Define Optimizer, Loss Function, and Metrics Calculation\n",
    "\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import hamming_loss, jaccard_score, f1_score # Make sure these are imported\n",
    "\n",
    "# --- Optimizer ---\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "logging.info(f\"Optimizer AdamW defined with LR={learning_rate}, Weight Decay={weight_decay}\")\n",
    "\n",
    "# --- Loss Function ---\n",
    "# Use BCEWithLogitsLoss for multi-label classification (includes Sigmoid)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "logging.info(\"Loss function BCEWithLogitsLoss defined.\")\n",
    "\n",
    "# --- Metrics Function ---\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"Calculates multi-label metrics from logits and labels.\"\"\"\n",
    "    logits, labels = eval_preds\n",
    "    # Ensure inputs are numpy arrays on CPU\n",
    "    logits_np = logits.detach().cpu().numpy() if isinstance(logits, torch.Tensor) else logits\n",
    "    labels_np = labels.detach().cpu().numpy() if isinstance(labels, torch.Tensor) else labels\n",
    "\n",
    "    # Apply sigmoid and threshold\n",
    "    probs = 1 / (1 + np.exp(-logits_np)) # Manual sigmoid\n",
    "    preds = (probs > 0.5).astype(int)\n",
    "    labels_np = labels_np.astype(int) # Ensure labels are integers\n",
    "\n",
    "    if labels_np.shape != preds.shape:\n",
    "         logging.error(f\"Shape mismatch in compute_metrics! Labels: {labels_np.shape}, Preds: {preds.shape}\")\n",
    "         # Return default metrics indicating failure\n",
    "         return {'hamming_loss': 1.0, 'jaccard_samples': 0.0, 'f1_micro': 0.0, 'f1_macro': 0.0}\n",
    "\n",
    "    metrics = {}\n",
    "    try:\n",
    "        metrics['hamming_loss'] = hamming_loss(labels_np, preds)\n",
    "        # Use average='samples' for Jaccard in multi-label scenario\n",
    "        metrics['jaccard_samples'] = jaccard_score(labels_np, preds, average='samples', zero_division=0)\n",
    "        metrics['f1_micro'] = f1_score(labels_np, preds, average='micro', zero_division=0)\n",
    "        metrics['f1_macro'] = f1_score(labels_np, preds, average='macro', zero_division=0)\n",
    "        # Optional: Add Accuracy (subset accuracy)\n",
    "        # metrics['accuracy'] = accuracy_score(labels_np, preds) # This is exact match accuracy\n",
    "    except Exception as e:\n",
    "         logging.error(f\"Error calculating metrics: {e}\")\n",
    "         metrics = {'hamming_loss': 1.0, 'jaccard_samples': 0.0, 'f1_micro': 0.0, 'f1_macro': 0.0}\n",
    "\n",
    "    # Log inside the main evaluate function now for better context\n",
    "    # logging.info(f\"Metrics: Hamming={metrics['hamming_loss']:.4f}, Jaccard(samples)={metrics['jaccard_samples']:.4f}, F1 Micro={metrics['f1_micro']:.4f}, F1 Macro={metrics['f1_macro']:.4f}\")\n",
    "    return metrics\n",
    "\n",
    "print(\"Optimizer, Loss, and compute_metrics function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 7: Define Training Function for One Epoch \n",
    "\n",
    "# def train_epoch(model, dataloader, criterion, optimizer, device, gradient_accumulation_steps):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     num_samples = 0\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "#     for step, batch in enumerate(progress_bar):\n",
    "#         try:\n",
    "#             # --- CORRECTED INPUT PREPARATION ---\n",
    "#             expected_model_input_key = \"input_features\"  \n",
    "\n",
    "#             if 'input_values' not in batch: # Check if extractor output key is different\n",
    "#                  raise KeyError(\"Batch dictionary missing 'input_values' from Dataset/Extractor.\")\n",
    "\n",
    "#             # Build the dictionary for the model's forward pass\n",
    "#             model_inputs = {\n",
    "#                 expected_model_input_key: batch['input_values'].to(device) # Map dataset output key to model input key\n",
    "#             }\n",
    "#             if 'attention_mask' in batch and batch['attention_mask'] is not None:\n",
    "#                  model_inputs['attention_mask'] = batch['attention_mask'].to(device)\n",
    "#             # --- END CORRECTION ---\n",
    "\n",
    "#             labels = batch['labels'].to(device)\n",
    "\n",
    "#             # Forward pass\n",
    "#             outputs = model(**model_inputs) # Pass the correctly named arguments\n",
    "#             logits = outputs.logits\n",
    "\n",
    "#             # Calculate loss\n",
    "#             loss = criterion(logits, labels)\n",
    "\n",
    "#             # ... (rest of loss scaling, backward, optimizer step remains the same) ...\n",
    "#             if torch.isnan(loss):\n",
    "#                 logging.warning(f\"NaN loss detected at step {step}. Skipping batch.\")\n",
    "#                 if (step + 1) % gradient_accumulation_steps != 0: model.zero_grad()\n",
    "#                 continue\n",
    "#             scaled_loss = loss / gradient_accumulation_steps\n",
    "#             scaled_loss.backward()\n",
    "#             batch_size_actual = labels.size(0)\n",
    "#             total_loss += loss.item() * batch_size_actual\n",
    "#             num_samples += batch_size_actual\n",
    "#             if (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == len(dataloader):\n",
    "#                 optimizer.step()\n",
    "#                 optimizer.zero_grad()\n",
    "#             progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "#         except Exception as e:\n",
    "#              logging.error(f\"Error during training step {step}, batch keys: {batch.keys()}. Error: {e}\", exc_info=True)\n",
    "#              continue\n",
    "\n",
    "#     if (step + 1) % gradient_accumulation_steps != 0 and num_samples > 0: # Ensure step was defined\n",
    "#          optimizer.step(); optimizer.zero_grad()\n",
    "\n",
    "#     avg_loss = total_loss / num_samples if num_samples > 0 else 0\n",
    "#     print(f\"\\nAverage Training Loss for Epoch: {avg_loss:.4f}\")\n",
    "#     return avg_loss\n",
    "\n",
    "# print(\"train_epoch function updated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 7: Define Training Function for One Epoch (with AMP)\n",
    "\n",
    "# from torch.cuda.amp import autocast, GradScaler # Import AMP utilities\n",
    "\n",
    "\n",
    "# # def train_epoch(model, dataloader, criterion, optimizer, device, gradient_accumulation_steps, scaler): # Add scaler argument\n",
    "# #     model.train() # Set model to training mode\n",
    "# #     total_loss = 0\n",
    "# #     num_samples = 0\n",
    "# #     optimizer.zero_grad() # Zero gradients once before the epoch loop\n",
    "\n",
    "# #     progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "# #     for step, batch in enumerate(progress_bar):\n",
    "# #         if batch is None or not batch: continue # Skip potentially None batches if Dataset has errors\n",
    "\n",
    "# #         try:\n",
    "# #             # Uses 'input_features' as the model's expected key based on previous debugging\n",
    "# #             expected_model_input_key = \"input_features\"\n",
    "# #             input_data_key = 'input_values' if 'input_values' in batch else 'input_features' # Key from feature extractor output\n",
    "\n",
    "# #             model_inputs = {}\n",
    "# #             if input_data_key in batch:\n",
    "# #                  model_inputs[expected_model_input_key] = batch[input_data_key].to(device)\n",
    "# #             else:\n",
    "# #                  raise KeyError(f\"Neither 'input_values' nor 'input_features' found in batch.\")\n",
    "\n",
    "# #             if 'attention_mask' in batch and batch['attention_mask'] is not None:\n",
    "# #                  model_inputs['attention_mask'] = batch['attention_mask'].to(device)\n",
    "\n",
    "# #             labels = batch['labels'].to(device) # Keep labels separate\n",
    "\n",
    "# #             # --- Automatic Mixed Precision ---\n",
    "# #             with autocast(device_type=device.type): # Runs forward pass and loss in mixed precision\n",
    "# #                 outputs = model(**model_inputs)\n",
    "# #                 logits = outputs.logits\n",
    "# #                 loss = criterion(logits, labels) # BCEWithLogitsLoss expects float labels\n",
    "\n",
    "# #                 # Check for NaN loss immediately after calculation\n",
    "# #                 if torch.isnan(loss):\n",
    "# #                     logging.warning(f\"NaN loss detected at step {step} *inside autocast*. Skipping batch.\")\n",
    "# #                     # Need to zero grad if skipping before optimizer step in accumulation cycle\n",
    "# #                     if (step + 1) % gradient_accumulation_steps != 0:\n",
    "# #                         optimizer.zero_grad() # Zero grad to prevent NaN propagation\n",
    "# #                     continue # Skip backward and optimizer step\n",
    "\n",
    "# #                 # Scale loss for gradient accumulation BEFORE scaler.scale()\n",
    "# #                 scaled_loss = loss / gradient_accumulation_steps\n",
    "# #             # --- End Autocast ---\n",
    "\n",
    "# #             # --- Scaler Scales the loss and Calls backward() ---\n",
    "# #             scaler.scale(scaled_loss).backward()\n",
    "# #             # ---------------------------------------------\n",
    "\n",
    "# #             # Accumulate total loss (use the original non-scaled loss for tracking)\n",
    "# #             batch_size_actual = labels.size(0)\n",
    "# #             total_loss += loss.item() * batch_size_actual\n",
    "# #             num_samples += batch_size_actual\n",
    "\n",
    "# #             # --- Optimizer Step (with Scaler) ---\n",
    "# #             if (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == len(dataloader):\n",
    "# #                 # Optional: Unscale gradients before clipping (if clipping)\n",
    "# #                 # scaler.unscale_(optimizer)\n",
    "# #                 # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "# #                 scaler.step(optimizer) # Unscales gradients, steps optimizer, checks for inf/NaN\n",
    "# #                 scaler.update() # Update scaler for next iteration\n",
    "# #                 optimizer.zero_grad() # Zero gradients *after* stepping or skipping step\n",
    "# #             # -----------------------------------\n",
    "\n",
    "# #             # Update progress bar description with non-scaled loss\n",
    "# #             progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "# #         except Exception as e:\n",
    "# #              logging.error(f\"Error during training step {step}, batch keys: {batch.keys()}. Error: {e}\", exc_info=True)\n",
    "# #              # Ensure gradients are zeroed if an error occurs mid-accumulation cycle\n",
    "# #              optimizer.zero_grad()\n",
    "# #              continue # Skip this batch on error\n",
    "\n",
    "# #     # Final calculation should use accumulated totals\n",
    "# #     avg_loss = total_loss / num_samples if num_samples > 0 else 0\n",
    "# #     print(f\"\\nAverage Training Loss for Epoch: {avg_loss:.4f}\")\n",
    "# #     return avg_loss\n",
    "\n",
    "\n",
    "# def train_epoch(model, dataloader, criterion, optimizer, device, gradient_accumulation_steps, scaler): # Add scaler argument\n",
    "#     model.train() # Set model to training mode\n",
    "#     total_loss = 0\n",
    "#     num_samples = 0\n",
    "#     optimizer.zero_grad() # Zero gradients once before the epoch loop\n",
    "    \n",
    "#     # Add counters for debugging\n",
    "#     successful_batches = 0\n",
    "#     print(f\"Starting training with {len(dataloader)} batches\")\n",
    "\n",
    "#     progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "#     for step, batch in enumerate(progress_bar):\n",
    "#         if batch is None or not batch: \n",
    "#             print(f\"Skipping empty batch at step {step}\")\n",
    "#             continue # Skip potentially None batches if Dataset has errors\n",
    "\n",
    "#         try:\n",
    "#             # Print batch shape information for debugging\n",
    "#             print(f\"Batch {step}: input shape = {batch['input_values'].shape}, label shape = {batch['labels'].shape}\")\n",
    "            \n",
    "#             # Uses 'input_features' as the model's expected key based on previous debugging\n",
    "#             expected_model_input_key = \"input_features\"\n",
    "#             input_data_key = 'input_values' if 'input_values' in batch else 'input_features' # Key from feature extractor output\n",
    "\n",
    "#             model_inputs = {}\n",
    "#             if input_data_key in batch:\n",
    "#                  model_inputs[expected_model_input_key] = batch[input_data_key].to(device)\n",
    "#             else:\n",
    "#                  raise KeyError(f\"Neither 'input_values' nor 'input_features' found in batch.\")\n",
    "\n",
    "#             if 'attention_mask' in batch and batch['attention_mask'] is not None:\n",
    "#                  model_inputs['attention_mask'] = batch['attention_mask'].to(device)\n",
    "\n",
    "#             labels = batch['labels'].to(device) # Keep labels separate\n",
    "\n",
    "#             # --- Automatic Mixed Precision ---\n",
    "#             with torch.autocast(device_type=device.type): # Runs forward pass and loss in mixed precision\n",
    "#                 outputs = model(**model_inputs)\n",
    "#                 logits = outputs.logits\n",
    "#                 loss = criterion(logits, labels) # BCEWithLogitsLoss expects float labels\n",
    "\n",
    "#                 # Check for NaN loss immediately after calculation\n",
    "#                 if torch.isnan(loss):\n",
    "#                     logging.warning(f\"NaN loss detected at step {step} *inside autocast*. Skipping batch.\")\n",
    "#                     print(f\"NaN loss detected at step {step}. Skipping batch.\")\n",
    "#                     # Need to zero grad if skipping before optimizer step in accumulation cycle\n",
    "#                     if (step + 1) % gradient_accumulation_steps != 0:\n",
    "#                         optimizer.zero_grad() # Zero grad to prevent NaN propagation\n",
    "#                     continue # Skip backward and optimizer step\n",
    "\n",
    "#                 # Scale loss for gradient accumulation BEFORE scaler.scale()\n",
    "#                 scaled_loss = loss / gradient_accumulation_steps\n",
    "#             # --- End Autocast ---\n",
    "\n",
    "#             # --- Scaler Scales the loss and Calls backward() ---\n",
    "#             scaler.scale(scaled_loss).backward()\n",
    "#             # ---------------------------------------------\n",
    "\n",
    "#             # Accumulate total loss (use the original non-scaled loss for tracking)\n",
    "#             batch_size_actual = labels.size(0)\n",
    "#             total_loss += loss.item() * batch_size_actual\n",
    "#             num_samples += batch_size_actual\n",
    "\n",
    "#             # --- Optimizer Step (with Scaler) ---\n",
    "#             if (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == len(dataloader):\n",
    "#                 # Optional: Unscale gradients before clipping (if clipping)\n",
    "#                 # scaler.unscale_(optimizer)\n",
    "#                 # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "#                 scaler.step(optimizer) # Unscales gradients, steps optimizer, checks for inf/NaN\n",
    "#                 scaler.update() # Update scaler for next iteration\n",
    "#                 optimizer.zero_grad() # Zero gradients *after* stepping or skipping step\n",
    "#             # -----------------------------------\n",
    "\n",
    "#             # Update progress bar description with non-scaled loss\n",
    "#             progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "#             progress_bar.update(1)  # Explicitly update the progress bar\n",
    "            \n",
    "#             # Count successful batches\n",
    "#             successful_batches += 1\n",
    "\n",
    "#         except Exception as e:\n",
    "#              logging.error(f\"Error during training step {step}, batch keys: {batch.keys()}. Error: {e}\", exc_info=True)\n",
    "#              print(f\"Error during training step {step}: {e}\")\n",
    "#              # Ensure gradients are zeroed if an error occurs mid-accumulation cycle\n",
    "#              optimizer.zero_grad()\n",
    "#              continue # Skip this batch on error\n",
    "\n",
    "#     print(f\"Completed training with {successful_batches}/{len(dataloader)} successful batches\")\n",
    "    \n",
    "#     # Final calculation should use accumulated totals\n",
    "#     avg_loss = total_loss / num_samples if num_samples > 0 else 0\n",
    "#     print(f\"\\nAverage Training Loss for Epoch: {avg_loss:.4f}\")\n",
    "#     return avg_loss\n",
    "\n",
    "\n",
    "# print(\"train_epoch function defined with AMP.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 7: Define Training Function for One Epoch (with AMP)\n",
    "\n",
    "# from torch.cuda.amp import autocast, GradScaler # Import AMP utilities\n",
    "\n",
    "# def train_epoch(model, dataloader, criterion, optimizer, device, gradient_accumulation_steps, scaler): # Add scaler argument\n",
    "#     model.train() # Set model to training mode\n",
    "#     total_loss = 0\n",
    "#     num_samples = 0\n",
    "#     optimizer.zero_grad() # Zero gradients once before the epoch loop\n",
    "    \n",
    "#     # Add counters for debugging\n",
    "#     successful_batches = 0\n",
    "#     print(f\"Starting training with {len(dataloader)} batches\")\n",
    "\n",
    "#     progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "#     for step, batch in enumerate(progress_bar):\n",
    "#         if batch is None or not batch: \n",
    "#             print(f\"Skipping empty batch at step {step}\")\n",
    "#             continue # Skip potentially None batches if Dataset has errors\n",
    "\n",
    "#         try:\n",
    "#             # Print batch shape information for debugging\n",
    "#             print(f\"Batch {step}: input shape = {batch['input_values'].shape}, label shape = {batch['labels'].shape}\")\n",
    "            \n",
    "#             # Uses 'input_features' as the model's expected key based on previous debugging\n",
    "#             expected_model_input_key = \"input_features\"\n",
    "#             input_data_key = 'input_values' if 'input_values' in batch else 'input_features' # Key from feature extractor output\n",
    "\n",
    "#             model_inputs = {}\n",
    "#             if input_data_key in batch:\n",
    "#                  model_inputs[expected_model_input_key] = batch[input_data_key].to(device)\n",
    "#             else:\n",
    "#                  raise KeyError(f\"Neither 'input_values' nor 'input_features' found in batch.\")\n",
    "\n",
    "#             if 'attention_mask' in batch and batch['attention_mask'] is not None:\n",
    "#                  model_inputs['attention_mask'] = batch['attention_mask'].to(device)\n",
    "\n",
    "#             labels = batch['labels'].to(device) # Keep labels separate\n",
    "\n",
    "#             # --- Automatic Mixed Precision ---\n",
    "#             with torch.autocast(device_type=device.type): # Runs forward pass and loss in mixed precision\n",
    "#                 outputs = model(**model_inputs)\n",
    "#                 logits = outputs.logits\n",
    "#                 loss = criterion(logits, labels) # BCEWithLogitsLoss expects float labels\n",
    "\n",
    "#                 # Check for NaN loss immediately after calculation\n",
    "#                 if torch.isnan(loss):\n",
    "#                     logging.warning(f\"NaN loss detected at step {step} *inside autocast*. Skipping batch.\")\n",
    "#                     print(f\"NaN loss detected at step {step}. Skipping batch.\")\n",
    "#                     # Need to zero grad if skipping before optimizer step in accumulation cycle\n",
    "#                     if (step + 1) % gradient_accumulation_steps != 0:\n",
    "#                         optimizer.zero_grad() # Zero grad to prevent NaN propagation\n",
    "#                     continue # Skip backward and optimizer step\n",
    "\n",
    "#                 # Scale loss for gradient accumulation BEFORE scaler.scale()\n",
    "#                 scaled_loss = loss / gradient_accumulation_steps\n",
    "#             # --- End Autocast ---\n",
    "\n",
    "#             # --- Scaler Scales the loss and Calls backward() ---\n",
    "#             scaler.scale(scaled_loss).backward()\n",
    "#             # ---------------------------------------------\n",
    "\n",
    "#             # Accumulate total loss (use the original non-scaled loss for tracking)\n",
    "#             batch_size_actual = labels.size(0)\n",
    "#             total_loss += loss.item() * batch_size_actual\n",
    "#             num_samples += batch_size_actual\n",
    "\n",
    "#             # --- Optimizer Step (with Scaler) ---\n",
    "#             if (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == len(dataloader):\n",
    "#                 # Optional: Unscale gradients before clipping (if clipping)\n",
    "#                 # scaler.unscale_(optimizer)\n",
    "#                 # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "#                 scaler.step(optimizer) # Unscales gradients, steps optimizer, checks for inf/NaN\n",
    "#                 scaler.update() # Update scaler for next iteration\n",
    "#                 optimizer.zero_grad() # Zero gradients *after* stepping or skipping step\n",
    "#             # -----------------------------------\n",
    "\n",
    "#             # Update progress bar description with non-scaled loss\n",
    "#             progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "#             progress_bar.update(1)  # Explicitly update the progress bar\n",
    "            \n",
    "#             # Count successful batches\n",
    "#             successful_batches += 1\n",
    "\n",
    "#         except Exception as e:\n",
    "#              logging.error(f\"Error during training step {step}, batch keys: {batch.keys()}. Error: {e}\", exc_info=True)\n",
    "#              print(f\"Error during training step {step}: {e}\")\n",
    "#              # Ensure gradients are zeroed if an error occurs mid-accumulation cycle\n",
    "#              optimizer.zero_grad()\n",
    "#              continue # Skip this batch on error\n",
    "\n",
    "#     print(f\"Completed training with {successful_batches}/{len(dataloader)} successful batches\")\n",
    "    \n",
    "#     # Final calculation should use accumulated totals\n",
    "#     avg_loss = total_loss / num_samples if num_samples > 0 else 0\n",
    "#     print(f\"\\nAverage Training Loss for Epoch: {avg_loss:.4f}\")\n",
    "#     return avg_loss\n",
    "\n",
    "\n",
    "# print(\"train_epoch function defined with AMP.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_epoch function updated to accept scheduler.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Define Training Function for One Epoch (with AMP and Scheduler)\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler # Or from torch.amp import ...\n",
    "\n",
    "# Ensure compute_metrics, torch, logging, tqdm etc. are imported\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, gradient_accumulation_steps, scaler, scheduler=None): # <<< Added scheduler=None\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_samples = 0\n",
    "    successful_steps = 0 # Counter for successful steps\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    num_batches = len(dataloader) # Get total batches for scheduler check\n",
    "\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        if batch is None or not batch: continue\n",
    "\n",
    "        try:\n",
    "            expected_model_input_key = \"input_features\" # VERIFY THIS KEY NAME\n",
    "            input_data_key = 'input_values' if 'input_values' in batch else 'input_features'\n",
    "            model_inputs = {expected_model_input_key: batch[input_data_key].to(device)}\n",
    "            if 'attention_mask' in batch and batch['attention_mask'] is not None: model_inputs['attention_mask'] = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            with autocast(device_type=device.type, enabled=(device.type=='cuda')): # Correct autocast usage\n",
    "                outputs = model(**model_inputs)\n",
    "                logits = outputs.logits\n",
    "                loss = criterion(logits, labels)\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                logging.warning(f\"NaN loss detected at step {step}. Skipping batch.\")\n",
    "                if (step + 1) % gradient_accumulation_steps != 0: optimizer.zero_grad()\n",
    "                continue\n",
    "\n",
    "            scaled_loss = loss / gradient_accumulation_steps\n",
    "            scaler.scale(scaled_loss).backward()\n",
    "\n",
    "            batch_size_actual = labels.size(0)\n",
    "            total_loss += loss.item() * batch_size_actual\n",
    "            num_samples += batch_size_actual\n",
    "\n",
    "            if (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == num_batches:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                # --- Step the scheduler AFTER the optimizer step ---\n",
    "                if scheduler:\n",
    "                    scheduler.step() # <<<--- ADDED SCHEDULER STEP HERE\n",
    "                # -------------------------------------------------\n",
    "                optimizer.zero_grad()\n",
    "                successful_steps +=1 # Count successful optimizer steps\n",
    "\n",
    "\n",
    "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}', 'lr': f'{optimizer.param_groups[0][\"lr\"]:.2e}'}) # Optionally show LR\n",
    "\n",
    "        except Exception as e:\n",
    "             logging.error(f\"Error during training step {step}: {e}\", exc_info=True)\n",
    "             optimizer.zero_grad() # Zero grad on error too\n",
    "             continue\n",
    "\n",
    "    # Final optimizer step might not be needed if scheduler steps correctly, depends on exact logic.\n",
    "    # Let's remove the extra step outside the loop for now.\n",
    "\n",
    "    avg_loss = total_loss / num_samples if num_samples > 0 else 0\n",
    "    print(f\"\\nCompleted training epoch. Successful optimizer steps: {successful_steps}\")\n",
    "    print(f\"Average Training Loss for Epoch: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "print(\"train_epoch function updated to accept scheduler.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 8: Define Evaluation Function (Corrected Model Input)\n",
    "\n",
    "# def evaluate(model, dataloader, criterion, device):\n",
    "#     model.eval()\n",
    "#     total_loss = 0\n",
    "#     all_logits = []\n",
    "#     all_labels = []\n",
    "#     num_samples = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for step, batch in enumerate(tqdm(dataloader, desc=\"Evaluating\", leave=False)):\n",
    "#             try:\n",
    "#                 # --- CORRECTED INPUT PREPARATION ---\n",
    "#                 expected_model_input_key = \"input_features\" # <<<--- VERIFY THIS KEY NAME\n",
    "\n",
    "#                 if 'input_values' not in batch:\n",
    "#                      raise KeyError(\"Batch dictionary missing 'input_values' from Dataset/Extractor.\")\n",
    "\n",
    "#                 model_inputs = {\n",
    "#                     expected_model_input_key: batch['input_values'].to(device)\n",
    "#                 }\n",
    "#                 if 'attention_mask' in batch and batch['attention_mask'] is not None:\n",
    "#                      model_inputs['attention_mask'] = batch['attention_mask'].to(device)\n",
    "#                 # --- END CORRECTION ---\n",
    "\n",
    "#                 labels = batch['labels'].to(device)\n",
    "\n",
    "#                 # Forward pass\n",
    "#                 outputs = model(**model_inputs) # Pass the correctly named arguments\n",
    "#                 logits = outputs.logits\n",
    "\n",
    "#                 # Calculate loss\n",
    "#                 loss = criterion(logits, labels)\n",
    "#                 total_loss += loss.item() * labels.size(0)\n",
    "#                 num_samples += labels.size(0)\n",
    "\n",
    "#                 all_logits.append(logits.cpu())\n",
    "#                 all_labels.append(labels.cpu())\n",
    "#             except Exception as e:\n",
    "#                  logging.error(f\"Error during evaluation step {step}, batch keys: {batch.keys()}. Error: {e}\", exc_info=True)\n",
    "#                  continue # Skip batch\n",
    "\n",
    "#     if not all_logits or not all_labels or num_samples == 0:\n",
    "#         logging.warning(\"Evaluation yielded no results (all batches failed or empty dataloader?).\")\n",
    "#         return {}\n",
    "\n",
    "#     avg_loss = total_loss / num_samples\n",
    "\n",
    "#     all_logits_cat = torch.cat(all_logits, dim=0)\n",
    "#     all_labels_cat = torch.cat(all_labels, dim=0)\n",
    "\n",
    "#     eval_preds = (all_logits_cat, all_labels_cat)\n",
    "#     metrics = compute_metrics(eval_preds)\n",
    "#     metrics['eval_loss'] = avg_loss\n",
    "\n",
    "#     print(f\"\\nValidation Loss: {avg_loss:.4f}\")\n",
    "#     for name, value in metrics.items():\n",
    "#          if name != 'eval_loss': print(f\"  Validation {name.replace('_', ' ').title()}: {value:.4f}\")\n",
    "\n",
    "#     return metrics\n",
    "\n",
    "# print(\"evaluate function updated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate function defined with AMP (autocast only).\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Define Evaluation Function (with AMP)\n",
    "\n",
    "# Ensure compute_metrics function is defined in a previous cell\n",
    "# Ensure torch, logging, tqdm, np are imported\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    total_loss = 0\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient calculations\n",
    "        for step, batch in enumerate(tqdm(dataloader, desc=\"Evaluating\", leave=False)):\n",
    "            if batch is None or not batch: continue\n",
    "            try:\n",
    "                # Prepare inputs\n",
    "                expected_model_input_key = \"input_features\" # VERIFY THIS KEY NAME\n",
    "                input_data_key = 'input_values' if 'input_values' in batch else 'input_features'\n",
    "\n",
    "                model_inputs = {}\n",
    "                if input_data_key in batch:\n",
    "                    model_inputs[expected_model_input_key] = batch[input_data_key].to(device)\n",
    "                else:\n",
    "                    raise KeyError(f\"Required input key not found in batch during evaluation.\")\n",
    "\n",
    "                if 'attention_mask' in batch and batch['attention_mask'] is not None:\n",
    "                     model_inputs['attention_mask'] = batch['attention_mask'].to(device)\n",
    "\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                # --- Use autocast for forward pass during evaluation ---\n",
    "                # Although not strictly needed for memory unless inputs are huge,\n",
    "                # it ensures consistency with training pass calculations.\n",
    "                with autocast(device_type=device.type):\n",
    "                    outputs = model(**model_inputs)\n",
    "                    logits = outputs.logits\n",
    "                    loss = criterion(logits, labels)\n",
    "                # ----------------------------------------------------\n",
    "\n",
    "                total_loss += loss.item() * labels.size(0)\n",
    "                num_samples += labels.size(0)\n",
    "\n",
    "                all_logits.append(logits.cpu()) # Store logits on CPU\n",
    "                all_labels.append(labels.cpu()) # Store labels on CPU\n",
    "            except Exception as e:\n",
    "                 logging.error(f\"Error during evaluation step {step}: {e}\", exc_info=True)\n",
    "                 continue # Skip batch on error\n",
    "\n",
    "    if not all_logits or not all_labels or num_samples == 0:\n",
    "        logging.warning(\"Evaluation yielded no results.\")\n",
    "        return {}\n",
    "\n",
    "    # Calculate average loss over processed samples\n",
    "    avg_loss = total_loss / num_samples\n",
    "\n",
    "    # Concatenate results from all batches\n",
    "    all_logits_cat = torch.cat(all_logits, dim=0)\n",
    "    all_labels_cat = torch.cat(all_labels, dim=0)\n",
    "\n",
    "    # Calculate metrics using the helper function\n",
    "    eval_preds = (all_logits_cat, all_labels_cat) # Pass tensors directly\n",
    "    metrics = compute_metrics(eval_preds)\n",
    "    metrics['eval_loss'] = avg_loss\n",
    "\n",
    "    # Log metrics\n",
    "    print(f\"\\nValidation Loss: {avg_loss:.4f}\")\n",
    "    for name, value in metrics.items():\n",
    "         if name != 'eval_loss': print(f\"  Validation {name.replace('_', ' ').title()}: {value:.4f}\")\n",
    "\n",
    "    return metrics # Return dictionary of all metrics\n",
    "\n",
    "print(\"evaluate function defined with AMP (autocast only).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 9: Run ONE Epoch for Debugging\n",
    "\n",
    "# from tqdm import tqdm # Ensure tqdm is imported\n",
    "# from torch.cuda.amp import GradScaler\n",
    "\n",
    "\n",
    "\n",
    "# # Ensure model, criterion, optimizer, dataloaders etc. are defined from previous cells\n",
    "# print(f\"\\n--- Starting Debug Training Run for {num_epochs_debug} epoch ---\")\n",
    "# start_time = time.time()\n",
    "\n",
    "\n",
    "# # --- Initialize GradScaler ---\n",
    "# scaler = GradScaler() # <<<--- ADD THIS INITIALIZATION\n",
    "# # ---------------------------\n",
    "\n",
    "\n",
    "\n",
    "# # Make sure model and criterion are on the correct device\n",
    "# model.to(device)\n",
    "# criterion.to(device)\n",
    "\n",
    "# for epoch in range(num_epochs_debug): # num_epochs_debug was set to 1 in Cell 2\n",
    "#     print(f\"\\n--- Debug Epoch {epoch+1}/{num_epochs_debug} ---\")\n",
    "\n",
    "#     # Run training step for one epoch on the debug training data\n",
    "#     train_loss = train_epoch(\n",
    "#         model,\n",
    "#         debug_train_dataloader, # Use the SMALL debug dataloader\n",
    "#         criterion,\n",
    "#         optimizer,\n",
    "#         device,\n",
    "#         gradient_accumulation_steps # Pass grad accum steps\n",
    "#     )\n",
    "\n",
    "#     # Run evaluation step on the debug validation data\n",
    "#     eval_metrics = evaluate(\n",
    "#         model,\n",
    "#         debug_val_dataloader, # Use the SMALL debug dataloader\n",
    "#         criterion,\n",
    "#         device\n",
    "#     )\n",
    "\n",
    "#     print(f\"\\nDebug Epoch {epoch+1} finished.\")\n",
    "#     print(f\"  Avg Train Loss: {train_loss:.4f}\")\n",
    "#     if eval_metrics:\n",
    "#         # Print all collected metrics\n",
    "#         for name, value in eval_metrics.items():\n",
    "#             print(f\"  Validation {name.replace('_', ' ').title()}: {value:.4f}\")\n",
    "#     else:\n",
    "#         print(\"  Validation failed to produce metrics.\")\n",
    "\n",
    "#     # Optional: Save model after this 1 epoch for inspection\n",
    "#     save_path = os.path.join(model_save_dir, f\"wav2vec2bert_debug_epoch_{epoch+1}.pth\") # <<<--- Corrected filename\n",
    "#     try:\n",
    "#          torch.save(model.state_dict(), save_path)\n",
    "#          logging.info(f\"Saved debug model checkpoint to {save_path}\")\n",
    "#     except Exception as e:\n",
    "#          logging.error(f\"Failed to save debug model checkpoint: {e}\", exc_info=True)\n",
    "\n",
    "# end_time = time.time()\n",
    "# print(f\"\\n--- Debug Run Finished in {end_time - start_time:.2f} seconds ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory before dataloader setup: 2.33 GB\n",
      "\n",
      "--- Starting Debug Training Run for 1 epoch (with AMP) ---\n",
      "\n",
      "--- Debug Epoch 1/1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed training epoch. Successful optimizer steps: 2\n",
      "Average Training Loss for Epoch: 0.6900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Loss: 0.6657\n",
      "  Validation Hamming Loss: 0.0966\n",
      "  Validation Jaccard Samples: 0.2500\n",
      "  Validation F1 Micro: 0.1905\n",
      "  Validation F1 Macro: 0.0182\n",
      "\n",
      "Debug Epoch 1 finished.\n",
      "  Avg Train Loss: 0.6900\n",
      "  Validation Hamming Loss: 0.0966\n",
      "  Validation Jaccard Samples: 0.2500\n",
      "  Validation F1 Micro: 0.1905\n",
      "  Validation F1 Macro: 0.0182\n",
      "  Validation Eval Loss: 0.6657\n",
      "2025-05-04 04:27:37,387 - INFO - Saved debug model checkpoint to /workspace/musicClaGen/models/wav2vec2bert_debug_AMP_epoch_1.pth\n",
      "\n",
      "--- Debug Run Finished in 22.28 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Run ONE Epoch for Debugging (with Updated AMP API)\n",
    "\n",
    "# --- Ensure necessary imports are present ---\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Import AMP components from the new location ---\n",
    "from torch.amp import autocast, GradScaler # <<<--- UPDATED IMPORT\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# # First, clear all existing models and tensors\n",
    "# import gc\n",
    "# import torch\n",
    "# # Force CUDA cache clearing\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()\n",
    "# Print memory status\n",
    "print(f\"GPU memory before dataloader setup: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "\n",
    "# Ensure model, criterion, optimizer, dataloaders etc. are defined from previous cells\n",
    "print(f\"\\n--- Starting Debug Training Run for {num_epochs_debug} epoch (with AMP) ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "# --- Initialize GradScaler using the NEW API ---\n",
    "# Pass device type, and enable only if device is actually cuda\n",
    "scaler = GradScaler(enabled=(device.type == 'cuda')) # <<<--- UPDATED INITIALIZATION\n",
    "# -------------------------------------------\n",
    "\n",
    "# Make sure model and criterion are on the correct device\n",
    "model.to(device)\n",
    "criterion.to(device) # Ensure criterion is also on device\n",
    "\n",
    "for epoch in range(num_epochs_debug): # num_epochs_debug was set to 1 in Cell 2\n",
    "    print(f\"\\n--- Debug Epoch {epoch+1}/{num_epochs_debug} ---\")\n",
    "\n",
    "    # Run training step (train_epoch function itself doesn't need change here, only how scaler is passed)\n",
    "    train_loss = train_epoch(\n",
    "        model,\n",
    "        debug_train_dataloader,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        device,\n",
    "        gradient_accumulation_steps,\n",
    "        scaler # Pass the scaler object (created with new API)\n",
    "    )\n",
    "\n",
    "    # Run evaluation step (evaluate function itself doesn't need change here for scaler)\n",
    "    eval_metrics = evaluate(\n",
    "        model,\n",
    "        debug_val_dataloader,\n",
    "        criterion,\n",
    "        device\n",
    "    )\n",
    "\n",
    "    print(f\"\\nDebug Epoch {epoch+1} finished.\")\n",
    "    print(f\"  Avg Train Loss: {train_loss:.4f}\")\n",
    "    if eval_metrics:\n",
    "        for name, value in eval_metrics.items():\n",
    "            print(f\"  Validation {name.replace('_', ' ').title()}: {value:.4f}\")\n",
    "    else:\n",
    "        print(\"  Validation failed to produce metrics.\")\n",
    "\n",
    "    # Optional: Save model after this 1 epoch for inspection\n",
    "    save_path = os.path.join(model_save_dir, f\"wav2vec2bert_debug_AMP_epoch_{epoch+1}.pth\")\n",
    "    try:\n",
    "         torch.save(model.state_dict(), save_path)\n",
    "         logging.info(f\"Saved debug model checkpoint to {save_path}\")\n",
    "    except Exception as e:\n",
    "         logging.error(f\"Failed to save debug model checkpoint: {e}\", exc_info=True)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\n--- Debug Run Finished in {end_time - start_time:.2f} seconds ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Trial debug training run worked! Now let's try the full training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 04:27:37,443 - INFO - Loading feature extractor for: facebook/w2v-bert-2.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 04:27:37,534 - INFO - Feature extractor loaded successfully.\n",
      "Feature extractor expects sample rate: 16000\n",
      "2025-05-04 04:27:37,536 - INFO - Initializing FMARawAudioDataset from: /workspace/musicClaGen/data/processed/small_subset_multihot.csv\n",
      "2025-05-04 04:27:37,538 - INFO - Target sampling rate set from feature extractor: 16000 Hz\n",
      "2025-05-04 04:27:37,539 - INFO - Loading manifest from: /workspace/musicClaGen/data/processed/small_subset_multihot.csv\n",
      "2025-05-04 04:27:37,572 - INFO - Attempting to parse 'multi_hot_label' column using ast.literal_eval...\n",
      "2025-05-04 04:27:37,893 - INFO - Example parsed label verified (type <class 'list'>, length 22): [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "2025-05-04 04:27:37,895 - INFO - Loaded and parsed manifest with 8000 entries.\n",
      "2025-05-04 04:27:37,897 - INFO - Creating DataLoaders with FULL splits and custom collator...\n",
      "DataCollatorAudio instance created.\n",
      "2025-05-04 04:27:37,904 - INFO - Batch size: 2, Grad Accum Steps: 4, Effective BS: 8\n",
      "2025-05-04 04:27:37,905 - INFO - FULL Dataset sizes: Train=6400, Val=800, Test=800\n",
      "2025-05-04 04:27:37,907 - INFO - FULL DataLoaders with custom collator created.\n",
      "2025-05-04 04:27:37,908 - INFO - LR Scheduler created. Total optimization steps: 6400\n",
      "\n",
      "Setup for full training run complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Setup DataLoaders for FULL Splits & LR Scheduler\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup # Import scheduler\n",
    "\n",
    "# --- Ensure Feature Extractor is Loaded ---\n",
    "# (Code from previous Cell 4 - necessary if kernel restarted)\n",
    "logging.info(f\"Loading feature extractor for: {model_checkpoint}\")\n",
    "try:\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint)\n",
    "    logging.info(\"Feature extractor loaded successfully.\")\n",
    "    target_sr = feature_extractor.sampling_rate\n",
    "    print(f\"Feature extractor expects sample rate: {target_sr}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Could not load feature extractor. Error: {e}\", exc_info=True)\n",
    "    raise SystemExit\n",
    "\n",
    "# --- Create Full Dataset instance ---\n",
    "try:\n",
    "    full_dataset = FMARawAudioDataset(manifest_path, feature_extractor=feature_extractor)\n",
    "    manifest_df = full_dataset.manifest\n",
    "except Exception as e:\n",
    "     logging.error(\"Failed to instantiate FMARawAudioDataset.\", exc_info=True)\n",
    "     raise SystemExit\n",
    "\n",
    "# --- Create FULL Datasets for Train/Val/Test ---\n",
    "logging.info(\"Creating DataLoaders with FULL splits and custom collator...\")\n",
    "try:\n",
    "    # Get indices for the splits from the manifest\n",
    "    train_indices = manifest_df[manifest_df['split'] == 'training'].index.tolist()\n",
    "    val_indices = manifest_df[manifest_df['split'] == 'validation'].index.tolist()\n",
    "    test_indices = manifest_df[manifest_df['split'] == 'test'].index.tolist() # Get test indices too\n",
    "\n",
    "    # Create Subset instances using the FULL index lists\n",
    "    train_dataset = Subset(full_dataset, train_indices)\n",
    "    val_dataset = Subset(full_dataset, val_indices)\n",
    "    test_dataset = Subset(full_dataset, test_indices) # Create test dataset\n",
    "\n",
    "    # --- Create Data Collator Instance ---\n",
    "    data_collator = DataCollatorAudio()\n",
    "    print(\"DataCollatorAudio instance created.\")\n",
    "\n",
    "    # --- Create DataLoaders ---\n",
    "    # Use actual batch_size from config\n",
    "    effective_batch_size = config.MODEL_PARAMS[\"batch_size\"] * config.MODEL_PARAMS[\"gradient_accumulation_steps\"]\n",
    "    logging.info(f\"Batch size: {config.MODEL_PARAMS['batch_size']}, Grad Accum Steps: {config.MODEL_PARAMS['gradient_accumulation_steps']}, Effective BS: {effective_batch_size}\")\n",
    "\n",
    "    # Use num_workers for faster loading (adjust based on instance cores)\n",
    "    num_workers = 4 if os.name == 'posix' else 0\n",
    "    pin_memory = True if device.type == 'cuda' else False\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, batch_size=config.MODEL_PARAMS[\"batch_size\"], shuffle=True,\n",
    "        collate_fn=data_collator, num_workers=num_workers, pin_memory=pin_memory, persistent_workers=(num_workers>0)\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset, batch_size=config.MODEL_PARAMS[\"batch_size\"], shuffle=False,\n",
    "        collate_fn=data_collator, num_workers=num_workers, pin_memory=pin_memory, persistent_workers=(num_workers>0)\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset, batch_size=config.MODEL_PARAMS[\"batch_size\"], shuffle=False,\n",
    "        collate_fn=data_collator, num_workers=num_workers, pin_memory=pin_memory, persistent_workers=(num_workers>0)\n",
    "    )\n",
    "    logging.info(f\"FULL Dataset sizes: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n",
    "    logging.info(\"FULL DataLoaders with custom collator created.\")\n",
    "\n",
    "    # --- Setup LR Scheduler ---\n",
    "    num_epochs = config.MODEL_PARAMS[\"epochs\"]\n",
    "    num_training_steps = (len(train_dataloader) // config.MODEL_PARAMS[\"gradient_accumulation_steps\"]) * num_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "         optimizer, # Optimizer defined in Cell 6\n",
    "         num_warmup_steps=0, # You can add warmup steps if desired (e.g., 10% of total steps)\n",
    "         num_training_steps=num_training_steps\n",
    "    )\n",
    "    logging.info(f\"LR Scheduler created. Total optimization steps: {num_training_steps}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to create datasets/dataloaders: {e}\", exc_info=True)\n",
    "    raise SystemExit\n",
    "\n",
    "print(\"\\nSetup for full training run complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory allocated before training: 7.00 GB\n",
      "GPU memory reserved before training: 15.53 GB\n",
      "2025-05-04 04:27:38,273 - INFO - --- Starting FULL Training for 8 epochs ---\n",
      "2025-05-04 04:27:38,283 - INFO - \n",
      "--- Epoch 1/8 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/3200 [00:00<?, ?it/s][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (3360) too large for available bit count (3240)\n",
      "Training:   2%|▏         | 51/3200 [00:44<42:34,  1.23it/s, loss=0.5499, lr=4.99e-05] /tmp/ipykernel_4431/4040614510.py:148: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  waveform, loaded_sr = librosa.load(\n",
      "/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 04:28:22,951 - ERROR - Error loading/processing track 133297: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 209, in __soundfile_load\n",
      "    context = sf.SoundFile(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 690, in __init__\n",
      "    self._file = self._open(file, mode_int, closefd)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1265, in _open\n",
      "    raise LibsndfileError(err, prefix=\"Error opening {0!r}: \".format(self.name))\n",
      "soundfile.LibsndfileError: Error opening '/workspace/musicClaGen/data/raw/fma_audio/fma_small/133/133297.mp3': File does not exist or is not a regular file (possibly a pipe?).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/parse.c:do_readahead():1083] warning: Cannot read next header, a one-frame stream? Duh...\n",
      "Training:  16%|█▋        | 521/3200 [07:11<37:45,  1.18it/s, loss=0.2326, lr=4.90e-05][src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
      "Note: Illegal Audio-MPEG-Header 0x00000000 at offset 63168.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      "/tmp/ipykernel_4431/4040614510.py:148: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  waveform, loaded_sr = librosa.load(\n",
      "/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 04:34:50,558 - ERROR - Error loading/processing track 98569: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 222, in __soundfile_load\n",
      "    y = sf_desc.read(frames=frame_duration, dtype=dtype, always_2d=False).T\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 942, in read\n",
      "    frames = self._array_io('read', out, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1394, in _array_io\n",
      "    return self._cdata_io(action, cdata, ctype, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1404, in _cdata_io\n",
      "    _error_check(self._errorcode)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1480, in _error_check\n",
      "    raise LibsndfileError(err, prefix=prefix)\n",
      "soundfile.LibsndfileError: Unspecified internal error.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█▊        | 584/3200 [08:03<37:57,  1.15it/s, loss=0.2285, lr=4.89e-05]/tmp/ipykernel_4431/4040614510.py:148: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  waveform, loaded_sr = librosa.load(\n",
      "/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 04:35:41,972 - ERROR - Error loading/processing track 99134: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 209, in __soundfile_load\n",
      "    context = sf.SoundFile(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 690, in __init__\n",
      "    self._file = self._open(file, mode_int, closefd)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1265, in _open\n",
      "    raise LibsndfileError(err, prefix=\"Error opening {0!r}: \".format(self.name))\n",
      "soundfile.LibsndfileError: Error opening '/workspace/musicClaGen/data/raw/fma_audio/fma_small/099/099134.mp3': File does not exist or is not a regular file (possibly a pipe?).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/parse.c:do_readahead():1083] warning: Cannot read next header, a one-frame stream? Duh...\n",
      "Training:  36%|███▌      | 1158/3200 [15:54<27:51,  1.22it/s, loss=0.2289, lr=4.77e-05][src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
      "Training:  37%|███▋      | 1192/3200 [16:22<27:17,  1.23it/s, loss=0.2381, lr=4.77e-05][src/libmpg123/layer3.c:INT123_do_layer3():1844] error: dequantization failed!\n",
      "Training:  44%|████▍     | 1418/3200 [19:28<24:06,  1.23it/s, loss=0.1624, lr=4.72e-05][src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
      "Training:  53%|█████▎    | 1690/3200 [23:10<20:28,  1.23it/s, loss=0.2924, lr=4.67e-05]/tmp/ipykernel_4431/4040614510.py:148: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  waveform, loaded_sr = librosa.load(\n",
      "/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 04:50:48,731 - ERROR - Error loading/processing track 108925: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 209, in __soundfile_load\n",
      "    context = sf.SoundFile(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 690, in __init__\n",
      "    self._file = self._open(file, mode_int, closefd)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1265, in _open\n",
      "    raise LibsndfileError(err, prefix=\"Error opening {0!r}: \".format(self.name))\n",
      "soundfile.LibsndfileError: Error opening '/workspace/musicClaGen/data/raw/fma_audio/fma_small/108/108925.mp3': File does not exist or is not a regular file (possibly a pipe?).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/parse.c:do_readahead():1083] warning: Cannot read next header, a one-frame stream? Duh...\n",
      "Training:  67%|██████▋   | 2129/3200 [29:11<13:30,  1.32it/s, loss=0.2103, lr=4.58e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 04:56:49,404 - ERROR - Error loading/processing track 98565: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 222, in __soundfile_load\n",
      "    y = sf_desc.read(frames=frame_duration, dtype=dtype, always_2d=False).T\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 942, in read\n",
      "    frames = self._array_io('read', out, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1394, in _array_io\n",
      "    return self._cdata_io(action, cdata, ctype, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1404, in _cdata_io\n",
      "    _error_check(self._errorcode)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1480, in _error_check\n",
      "    raise LibsndfileError(err, prefix=prefix)\n",
      "soundfile.LibsndfileError: Unspecified internal error.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Illegal Audio-MPEG-Header 0x00000000 at offset 33361.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      "Training:  81%|████████  | 2598/3200 [35:36<08:20,  1.20it/s, loss=0.1553, lr=4.49e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 05:03:14,598 - ERROR - Error loading/processing track 98567: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 222, in __soundfile_load\n",
      "    y = sf_desc.read(frames=frame_duration, dtype=dtype, always_2d=False).T\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 942, in read\n",
      "    frames = self._array_io('read', out, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1394, in _array_io\n",
      "    return self._cdata_io(action, cdata, ctype, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1404, in _cdata_io\n",
      "    _error_check(self._errorcode)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1480, in _error_check\n",
      "    raise LibsndfileError(err, prefix=prefix)\n",
      "soundfile.LibsndfileError: Unspecified internal error.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Illegal Audio-MPEG-Header 0x00000000 at offset 22401.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      "Training:  86%|████████▋ | 2762/3200 [37:50<05:56,  1.23it/s, loss=0.3741, lr=4.46e-05][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (3328) too large for available bit count (3240)\n",
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed training epoch. Successful optimizer steps: 800\n",
      "Average Training Loss for Epoch: 0.2407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Loss: 0.2316\n",
      "  Validation Hamming Loss: 0.0691\n",
      "  Validation Jaccard Samples: 0.0000\n",
      "  Validation F1 Micro: 0.0000\n",
      "  Validation F1 Macro: 0.0000\n",
      "\n",
      "Epoch 1 finished.\n",
      "  Avg Train Loss: 0.2407\n",
      "  Validation Hamming Loss: 0.0691\n",
      "  Validation Jaccard Samples: 0.0000\n",
      "  Validation F1 Micro: 0.0000\n",
      "  Validation F1 Macro: 0.0000\n",
      "  Validation Eval Loss: 0.2316\n",
      "2025-05-04 05:15:07,159 - INFO - Validation metric improved (hamming_loss=0.0691). Saved best model to /workspace/musicClaGen/models/facebook_w2v-bert-2.0_finetuned_best.pth\n",
      "2025-05-04 05:15:07,162 - INFO - Epoch 1 finished in 47.48 minutes.\n",
      "2025-05-04 05:15:07,163 - INFO - \n",
      "--- Epoch 2/8 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▌        | 481/3200 [06:37<37:07,  1.22it/s, loss=0.2863, lr=4.28e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 05:21:44,646 - ERROR - Error loading/processing track 98565: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 222, in __soundfile_load\n",
      "    y = sf_desc.read(frames=frame_duration, dtype=dtype, always_2d=False).T\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 942, in read\n",
      "    frames = self._array_io('read', out, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1394, in _array_io\n",
      "    return self._cdata_io(action, cdata, ctype, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1404, in _cdata_io\n",
      "    _error_check(self._errorcode)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1480, in _error_check\n",
      "    raise LibsndfileError(err, prefix=prefix)\n",
      "soundfile.LibsndfileError: Unspecified internal error.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Illegal Audio-MPEG-Header 0x00000000 at offset 33361.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      "Training:  22%|██▏       | 709/3200 [09:43<33:44,  1.23it/s, loss=0.1500, lr=4.24e-05][src/libmpg123/layer3.c:INT123_do_layer3():1844] error: dequantization failed!\n",
      "Training:  23%|██▎       | 743/3200 [10:11<33:42,  1.21it/s, loss=0.3134, lr=4.23e-05][src/libmpg123/parse.c:do_readahead():1083] warning: Cannot read next header, a one-frame stream? Duh...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 05:25:19,220 - ERROR - Error loading/processing track 133297: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 209, in __soundfile_load\n",
      "    context = sf.SoundFile(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 690, in __init__\n",
      "    self._file = self._open(file, mode_int, closefd)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1265, in _open\n",
      "    raise LibsndfileError(err, prefix=\"Error opening {0!r}: \".format(self.name))\n",
      "soundfile.LibsndfileError: Error opening '/workspace/musicClaGen/data/raw/fma_audio/fma_small/133/133297.mp3': File does not exist or is not a regular file (possibly a pipe?).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███       | 987/3200 [13:30<31:08,  1.18it/s, loss=0.3848, lr=4.18e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 05:28:38,364 - ERROR - Error loading/processing track 99134: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 209, in __soundfile_load\n",
      "    context = sf.SoundFile(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 690, in __init__\n",
      "    self._file = self._open(file, mode_int, closefd)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1265, in _open\n",
      "    raise LibsndfileError(err, prefix=\"Error opening {0!r}: \".format(self.name))\n",
      "soundfile.LibsndfileError: Error opening '/workspace/musicClaGen/data/raw/fma_audio/fma_small/099/099134.mp3': File does not exist or is not a regular file (possibly a pipe?).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/parse.c:do_readahead():1083] warning: Cannot read next header, a one-frame stream? Duh...\n",
      "Training:  32%|███▏      | 1018/3200 [13:56<30:44,  1.18it/s, loss=0.2118, lr=4.18e-05][src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
      "Training:  39%|███▉      | 1244/3200 [17:03<27:38,  1.18it/s, loss=0.3634, lr=4.13e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 05:32:10,991 - ERROR - Error loading/processing track 98567: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 222, in __soundfile_load\n",
      "    y = sf_desc.read(frames=frame_duration, dtype=dtype, always_2d=False).T\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 942, in read\n",
      "    frames = self._array_io('read', out, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1394, in _array_io\n",
      "    return self._cdata_io(action, cdata, ctype, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1404, in _cdata_io\n",
      "    _error_check(self._errorcode)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1480, in _error_check\n",
      "    raise LibsndfileError(err, prefix=prefix)\n",
      "soundfile.LibsndfileError: Unspecified internal error.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Illegal Audio-MPEG-Header 0x00000000 at offset 22401.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      "Training:  39%|███▉      | 1259/3200 [17:15<24:35,  1.32it/s, loss=0.2439, lr=4.13e-05][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (3328) too large for available bit count (3240)\n",
      "Training:  46%|████▌     | 1477/3200 [20:14<23:00,  1.25it/s, loss=0.1586, lr=4.09e-05][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (3360) too large for available bit count (3240)\n",
      "Training:  62%|██████▏   | 1983/3200 [27:10<16:03,  1.26it/s, loss=0.1480, lr=3.99e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 05:42:17,552 - ERROR - Error loading/processing track 98569: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 222, in __soundfile_load\n",
      "    y = sf_desc.read(frames=frame_duration, dtype=dtype, always_2d=False).T\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 942, in read\n",
      "    frames = self._array_io('read', out, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1394, in _array_io\n",
      "    return self._cdata_io(action, cdata, ctype, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1404, in _cdata_io\n",
      "    _error_check(self._errorcode)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1480, in _error_check\n",
      "    raise LibsndfileError(err, prefix=prefix)\n",
      "soundfile.LibsndfileError: Unspecified internal error.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
      "Note: Illegal Audio-MPEG-Header 0x00000000 at offset 63168.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      "Training:  70%|██████▉   | 2234/3200 [30:36<13:13,  1.22it/s, loss=0.1583, lr=3.94e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 05:45:44,355 - ERROR - Error loading/processing track 108925: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 209, in __soundfile_load\n",
      "    context = sf.SoundFile(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 690, in __init__\n",
      "    self._file = self._open(file, mode_int, closefd)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1265, in _open\n",
      "    raise LibsndfileError(err, prefix=\"Error opening {0!r}: \".format(self.name))\n",
      "soundfile.LibsndfileError: Error opening '/workspace/musicClaGen/data/raw/fma_audio/fma_small/108/108925.mp3': File does not exist or is not a regular file (possibly a pipe?).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/parse.c:do_readahead():1083] warning: Cannot read next header, a one-frame stream? Duh...\n",
      "Training:  71%|███████   | 2269/3200 [31:05<13:08,  1.18it/s, loss=0.2320, lr=3.93e-05][src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed training epoch. Successful optimizer steps: 800\n",
      "Average Training Loss for Epoch: 0.2206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Loss: 0.2310\n",
      "  Validation Hamming Loss: 0.0691\n",
      "  Validation Jaccard Samples: 0.0000\n",
      "  Validation F1 Micro: 0.0000\n",
      "  Validation F1 Macro: 0.0000\n",
      "\n",
      "Epoch 2 finished.\n",
      "  Avg Train Loss: 0.2206\n",
      "  Validation Hamming Loss: 0.0691\n",
      "  Validation Jaccard Samples: 0.0000\n",
      "  Validation F1 Micro: 0.0000\n",
      "  Validation F1 Macro: 0.0000\n",
      "  Validation Eval Loss: 0.2310\n",
      "2025-05-04 06:02:28,475 - INFO - Validation metric did not improve (hamming_loss=0.0691). Best: 0.0691\n",
      "2025-05-04 06:02:28,477 - INFO - Epoch 2 finished in 47.36 minutes.\n",
      "2025-05-04 06:02:28,478 - INFO - \n",
      "--- Epoch 3/8 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|█▏        | 375/3200 [05:11<37:18,  1.26it/s, loss=0.2326, lr=3.68e-05][src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
      "Training:  15%|█▍        | 475/3200 [06:33<37:10,  1.22it/s, loss=0.2397, lr=3.66e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 06:09:01,880 - ERROR - Error loading/processing track 99134: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 209, in __soundfile_load\n",
      "    context = sf.SoundFile(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 690, in __init__\n",
      "    self._file = self._open(file, mode_int, closefd)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1265, in _open\n",
      "    raise LibsndfileError(err, prefix=\"Error opening {0!r}: \".format(self.name))\n",
      "soundfile.LibsndfileError: Error opening '/workspace/musicClaGen/data/raw/fma_audio/fma_small/099/099134.mp3': File does not exist or is not a regular file (possibly a pipe?).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/parse.c:do_readahead():1083] warning: Cannot read next header, a one-frame stream? Duh...\n",
      "Training:  19%|█▊        | 599/3200 [08:15<35:14,  1.23it/s, loss=0.1557, lr=3.63e-05][src/libmpg123/layer3.c:INT123_do_layer3():1844] error: dequantization failed!\n",
      "Training:  26%|██▌       | 834/3200 [11:29<32:50,  1.20it/s, loss=0.1603, lr=3.59e-05][src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
      "Training:  27%|██▋       | 860/3200 [11:50<31:47,  1.23it/s, loss=0.2215, lr=3.58e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 06:14:19,855 - ERROR - Error loading/processing track 98565: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 222, in __soundfile_load\n",
      "    y = sf_desc.read(frames=frame_duration, dtype=dtype, always_2d=False).T\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 942, in read\n",
      "    frames = self._array_io('read', out, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1394, in _array_io\n",
      "    return self._cdata_io(action, cdata, ctype, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1404, in _cdata_io\n",
      "    _error_check(self._errorcode)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1480, in _error_check\n",
      "    raise LibsndfileError(err, prefix=prefix)\n",
      "soundfile.LibsndfileError: Unspecified internal error.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Illegal Audio-MPEG-Header 0x00000000 at offset 33361.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      "Training:  40%|███▉      | 1274/3200 [17:30<26:21,  1.22it/s, loss=0.2033, lr=3.50e-05]Note: Illegal Audio-MPEG-Header 0x00000000 at offset 22401.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 06:19:59,162 - ERROR - Error loading/processing track 98567: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 222, in __soundfile_load\n",
      "    y = sf_desc.read(frames=frame_duration, dtype=dtype, always_2d=False).T\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 942, in read\n",
      "    frames = self._array_io('read', out, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1394, in _array_io\n",
      "    return self._cdata_io(action, cdata, ctype, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1404, in _cdata_io\n",
      "    _error_check(self._errorcode)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1480, in _error_check\n",
      "    raise LibsndfileError(err, prefix=prefix)\n",
      "soundfile.LibsndfileError: Unspecified internal error.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 1609/3200 [22:05<20:26,  1.30it/s, loss=0.3160, lr=3.44e-05][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (3360) too large for available bit count (3240)\n",
      "Training:  64%|██████▎   | 2032/3200 [27:53<16:30,  1.18it/s, loss=0.1567, lr=3.35e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 06:30:21,633 - ERROR - Error loading/processing track 133297: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 209, in __soundfile_load\n",
      "    context = sf.SoundFile(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 690, in __init__\n",
      "    self._file = self._open(file, mode_int, closefd)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1265, in _open\n",
      "    raise LibsndfileError(err, prefix=\"Error opening {0!r}: \".format(self.name))\n",
      "soundfile.LibsndfileError: Error opening '/workspace/musicClaGen/data/raw/fma_audio/fma_small/133/133297.mp3': File does not exist or is not a regular file (possibly a pipe?).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/parse.c:do_readahead():1083] warning: Cannot read next header, a one-frame stream? Duh...\n",
      "Training:  72%|███████▏  | 2307/3200 [31:35<12:04,  1.23it/s, loss=0.2294, lr=3.30e-05][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (3328) too large for available bit count (3240)\n",
      "Training:  78%|███████▊  | 2490/3200 [34:06<09:31,  1.24it/s, loss=0.2639, lr=3.26e-05][src/libmpg123/parse.c:do_readahead():1083] warning: Cannot read next header, a one-frame stream? Duh...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 06:36:34,721 - ERROR - Error loading/processing track 108925: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 209, in __soundfile_load\n",
      "    context = sf.SoundFile(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 690, in __init__\n",
      "    self._file = self._open(file, mode_int, closefd)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1265, in _open\n",
      "    raise LibsndfileError(err, prefix=\"Error opening {0!r}: \".format(self.name))\n",
      "soundfile.LibsndfileError: Error opening '/workspace/musicClaGen/data/raw/fma_audio/fma_small/108/108925.mp3': File does not exist or is not a regular file (possibly a pipe?).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|█████████▊| 3128/3200 [42:48<00:59,  1.21it/s, loss=0.2387, lr=3.14e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 06:45:16,761 - ERROR - Error loading/processing track 98569: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 222, in __soundfile_load\n",
      "    y = sf_desc.read(frames=frame_duration, dtype=dtype, always_2d=False).T\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 942, in read\n",
      "    frames = self._array_io('read', out, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1394, in _array_io\n",
      "    return self._cdata_io(action, cdata, ctype, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1404, in _cdata_io\n",
      "    _error_check(self._errorcode)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1480, in _error_check\n",
      "    raise LibsndfileError(err, prefix=prefix)\n",
      "soundfile.LibsndfileError: Unspecified internal error.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
      "Note: Illegal Audio-MPEG-Header 0x00000000 at offset 63168.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed training epoch. Successful optimizer steps: 800\n",
      "Average Training Loss for Epoch: 0.2209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Loss: 0.2308\n",
      "  Validation Hamming Loss: 0.0691\n",
      "  Validation Jaccard Samples: 0.0000\n",
      "  Validation F1 Micro: 0.0000\n",
      "  Validation F1 Macro: 0.0000\n",
      "\n",
      "Epoch 3 finished.\n",
      "  Avg Train Loss: 0.2209\n",
      "  Validation Hamming Loss: 0.0691\n",
      "  Validation Jaccard Samples: 0.0000\n",
      "  Validation F1 Micro: 0.0000\n",
      "  Validation F1 Macro: 0.0000\n",
      "  Validation Eval Loss: 0.2308\n",
      "2025-05-04 06:49:48,748 - INFO - Validation metric did not improve (hamming_loss=0.0691). Best: 0.0691\n",
      "2025-05-04 06:49:48,749 - INFO - Epoch 3 finished in 47.34 minutes.\n",
      "2025-05-04 06:49:48,750 - INFO - \n",
      "--- Epoch 4/8 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/3200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 06:49:49,667 - ERROR - Error loading/processing track 98565: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 222, in __soundfile_load\n",
      "    y = sf_desc.read(frames=frame_duration, dtype=dtype, always_2d=False).T\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 942, in read\n",
      "    frames = self._array_io('read', out, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1394, in _array_io\n",
      "    return self._cdata_io(action, cdata, ctype, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1404, in _cdata_io\n",
      "    _error_check(self._errorcode)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1480, in _error_check\n",
      "    raise LibsndfileError(err, prefix=prefix)\n",
      "soundfile.LibsndfileError: Unspecified internal error.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Illegal Audio-MPEG-Header 0x00000000 at offset 33361.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      "Training:   6%|▌         | 196/3200 [02:43<42:15,  1.18it/s, loss=0.2225, lr=3.09e-05][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (3360) too large for available bit count (3240)\n",
      "Training:   7%|▋         | 239/3200 [03:18<41:50,  1.18it/s, loss=0.3324, lr=3.08e-05][src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
      "Training:   9%|▉         | 297/3200 [04:06<40:37,  1.19it/s, loss=0.1605, lr=3.07e-05][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (3328) too large for available bit count (3240)\n",
      "Training:  32%|███▏      | 1039/3200 [14:17<28:37,  1.26it/s, loss=0.1627, lr=2.92e-05][src/libmpg123/layer3.c:INT123_do_layer3():1844] error: dequantization failed!\n",
      "Training:  45%|████▌     | 1440/3200 [19:44<23:59,  1.22it/s, loss=0.2088, lr=2.84e-05][src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
      "Training:  48%|████▊     | 1539/3200 [21:05<22:31,  1.23it/s, loss=0.2766, lr=2.82e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 07:10:54,959 - ERROR - Error loading/processing track 98569: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 222, in __soundfile_load\n",
      "    y = sf_desc.read(frames=frame_duration, dtype=dtype, always_2d=False).T\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 942, in read\n",
      "    frames = self._array_io('read', out, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1394, in _array_io\n",
      "    return self._cdata_io(action, cdata, ctype, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1404, in _cdata_io\n",
      "    _error_check(self._errorcode)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1480, in _error_check\n",
      "    raise LibsndfileError(err, prefix=prefix)\n",
      "soundfile.LibsndfileError: Unspecified internal error.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
      "Note: Illegal Audio-MPEG-Header 0x00000000 at offset 63168.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      "Training:  52%|█████▏    | 1663/3200 [22:47<21:14,  1.21it/s, loss=0.2967, lr=2.80e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 07:12:36,857 - ERROR - Error loading/processing track 99134: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 209, in __soundfile_load\n",
      "    context = sf.SoundFile(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 690, in __init__\n",
      "    self._file = self._open(file, mode_int, closefd)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1265, in _open\n",
      "    raise LibsndfileError(err, prefix=\"Error opening {0!r}: \".format(self.name))\n",
      "soundfile.LibsndfileError: Error opening '/workspace/musicClaGen/data/raw/fma_audio/fma_small/099/099134.mp3': File does not exist or is not a regular file (possibly a pipe?).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/parse.c:do_readahead():1083] warning: Cannot read next header, a one-frame stream? Duh...\n",
      "Training:  75%|███████▍  | 2398/3200 [32:51<10:44,  1.24it/s, loss=0.1600, lr=2.66e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 07:22:40,000 - ERROR - Error loading/processing track 133297: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 209, in __soundfile_load\n",
      "    context = sf.SoundFile(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 690, in __init__\n",
      "    self._file = self._open(file, mode_int, closefd)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1265, in _open\n",
      "    raise LibsndfileError(err, prefix=\"Error opening {0!r}: \".format(self.name))\n",
      "soundfile.LibsndfileError: Error opening '/workspace/musicClaGen/data/raw/fma_audio/fma_small/133/133297.mp3': File does not exist or is not a regular file (possibly a pipe?).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/parse.c:do_readahead():1083] warning: Cannot read next header, a one-frame stream? Duh...\n",
      "Training:  91%|█████████ | 2917/3200 [39:56<03:49,  1.23it/s, loss=0.2289, lr=2.56e-05]Note: Illegal Audio-MPEG-Header 0x00000000 at offset 22401.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 07:29:45,862 - ERROR - Error loading/processing track 98567: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 222, in __soundfile_load\n",
      "    y = sf_desc.read(frames=frame_duration, dtype=dtype, always_2d=False).T\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 942, in read\n",
      "    frames = self._array_io('read', out, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1394, in _array_io\n",
      "    return self._cdata_io(action, cdata, ctype, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1404, in _cdata_io\n",
      "    _error_check(self._errorcode)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1480, in _error_check\n",
      "    raise LibsndfileError(err, prefix=prefix)\n",
      "soundfile.LibsndfileError: Unspecified internal error.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|█████████▊| 3124/3200 [42:46<01:03,  1.20it/s, loss=0.2402, lr=2.51e-05][src/libmpg123/parse.c:do_readahead():1083] warning: Cannot read next header, a one-frame stream? Duh...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 07:32:35,082 - ERROR - Error loading/processing track 108925: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 209, in __soundfile_load\n",
      "    context = sf.SoundFile(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 690, in __init__\n",
      "    self._file = self._open(file, mode_int, closefd)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1265, in _open\n",
      "    raise LibsndfileError(err, prefix=\"Error opening {0!r}: \".format(self.name))\n",
      "soundfile.LibsndfileError: Error opening '/workspace/musicClaGen/data/raw/fma_audio/fma_small/108/108925.mp3': File does not exist or is not a regular file (possibly a pipe?).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed training epoch. Successful optimizer steps: 800\n",
      "Average Training Loss for Epoch: 0.2202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Loss: 0.2313\n",
      "  Validation Hamming Loss: 0.0691\n",
      "  Validation Jaccard Samples: 0.0000\n",
      "  Validation F1 Micro: 0.0000\n",
      "  Validation F1 Macro: 0.0000\n",
      "\n",
      "Epoch 4 finished.\n",
      "  Avg Train Loss: 0.2202\n",
      "  Validation Hamming Loss: 0.0691\n",
      "  Validation Jaccard Samples: 0.0000\n",
      "  Validation F1 Micro: 0.0000\n",
      "  Validation F1 Macro: 0.0000\n",
      "  Validation Eval Loss: 0.2313\n",
      "2025-05-04 07:37:11,034 - INFO - Validation metric did not improve (hamming_loss=0.0691). Best: 0.0691\n",
      "2025-05-04 07:37:11,035 - INFO - Epoch 4 finished in 47.37 minutes.\n",
      "2025-05-04 07:37:11,036 - INFO - \n",
      "--- Epoch 5/8 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 14/3200 [00:14<45:12,  1.17it/s, loss=0.1508, lr=2.50e-05] [src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
      "Training:  17%|█▋        | 534/3200 [07:19<36:44,  1.21it/s, loss=0.2463, lr=2.40e-05]Note: Illegal Audio-MPEG-Header 0x00000000 at offset 22401.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 07:44:30,756 - ERROR - Error loading/processing track 98567: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 222, in __soundfile_load\n",
      "    y = sf_desc.read(frames=frame_duration, dtype=dtype, always_2d=False).T\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 942, in read\n",
      "    frames = self._array_io('read', out, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1394, in _array_io\n",
      "    return self._cdata_io(action, cdata, ctype, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1404, in _cdata_io\n",
      "    _error_check(self._errorcode)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1480, in _error_check\n",
      "    raise LibsndfileError(err, prefix=prefix)\n",
      "soundfile.LibsndfileError: Unspecified internal error.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██▏       | 683/3200 [09:21<35:10,  1.19it/s, loss=0.2091, lr=2.37e-05][src/libmpg123/layer3.c:INT123_do_layer3():1844] error: dequantization failed!\n",
      "Training:  32%|███▏      | 1019/3200 [13:57<30:01,  1.21it/s, loss=0.2863, lr=2.30e-05][src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
      "Training:  38%|███▊      | 1215/3200 [16:38<25:52,  1.28it/s, loss=0.2194, lr=2.26e-05][src/libmpg123/parse.c:do_readahead():1083] warning: Cannot read next header, a one-frame stream? Duh...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 07:53:49,758 - ERROR - Error loading/processing track 108925: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 209, in __soundfile_load\n",
      "    context = sf.SoundFile(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 690, in __init__\n",
      "    self._file = self._open(file, mode_int, closefd)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1265, in _open\n",
      "    raise LibsndfileError(err, prefix=\"Error opening {0!r}: \".format(self.name))\n",
      "soundfile.LibsndfileError: Error opening '/workspace/musicClaGen/data/raw/fma_audio/fma_small/108/108925.mp3': File does not exist or is not a regular file (possibly a pipe?).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|████▎     | 1381/3200 [18:54<26:09,  1.16it/s, loss=0.1452, lr=2.23e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 07:56:05,359 - ERROR - Error loading/processing track 98565: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 222, in __soundfile_load\n",
      "    y = sf_desc.read(frames=frame_duration, dtype=dtype, always_2d=False).T\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 942, in read\n",
      "    frames = self._array_io('read', out, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1394, in _array_io\n",
      "    return self._cdata_io(action, cdata, ctype, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1404, in _cdata_io\n",
      "    _error_check(self._errorcode)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1480, in _error_check\n",
      "    raise LibsndfileError(err, prefix=prefix)\n",
      "soundfile.LibsndfileError: Unspecified internal error.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Illegal Audio-MPEG-Header 0x00000000 at offset 33361.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      "Training:  44%|████▍     | 1410/3200 [19:17<24:37,  1.21it/s, loss=0.3236, lr=2.23e-05][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (3360) too large for available bit count (3240)\n",
      "Training:  58%|█████▊    | 1845/3200 [25:13<18:10,  1.24it/s, loss=0.1544, lr=2.14e-05][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (3328) too large for available bit count (3240)\n",
      "Training:  64%|██████▍   | 2054/3200 [28:05<15:23,  1.24it/s, loss=0.2254, lr=2.10e-05][src/libmpg123/parse.c:do_readahead():1083] warning: Cannot read next header, a one-frame stream? Duh...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 08:05:16,761 - ERROR - Error loading/processing track 133297: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 209, in __soundfile_load\n",
      "    context = sf.SoundFile(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 690, in __init__\n",
      "    self._file = self._open(file, mode_int, closefd)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1265, in _open\n",
      "    raise LibsndfileError(err, prefix=\"Error opening {0!r}: \".format(self.name))\n",
      "soundfile.LibsndfileError: Error opening '/workspace/musicClaGen/data/raw/fma_audio/fma_small/133/133297.mp3': File does not exist or is not a regular file (possibly a pipe?).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|██████▌   | 2105/3200 [28:47<15:14,  1.20it/s, loss=0.1551, lr=2.09e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 08:05:58,421 - ERROR - Error loading/processing track 98569: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 222, in __soundfile_load\n",
      "    y = sf_desc.read(frames=frame_duration, dtype=dtype, always_2d=False).T\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 942, in read\n",
      "    frames = self._array_io('read', out, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1394, in _array_io\n",
      "    return self._cdata_io(action, cdata, ctype, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1404, in _cdata_io\n",
      "    _error_check(self._errorcode)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1480, in _error_check\n",
      "    raise LibsndfileError(err, prefix=prefix)\n",
      "soundfile.LibsndfileError: Unspecified internal error.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
      "Note: Illegal Audio-MPEG-Header 0x00000000 at offset 63168.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      "Training:  68%|██████▊   | 2178/3200 [29:46<13:29,  1.26it/s, loss=0.2239, lr=2.07e-05][src/libmpg123/parse.c:do_readahead():1083] warning: Cannot read next header, a one-frame stream? Duh...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 08:06:58,002 - ERROR - Error loading/processing track 99134: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 209, in __soundfile_load\n",
      "    context = sf.SoundFile(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 690, in __init__\n",
      "    self._file = self._open(file, mode_int, closefd)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1265, in _open\n",
      "    raise LibsndfileError(err, prefix=\"Error opening {0!r}: \".format(self.name))\n",
      "soundfile.LibsndfileError: Error opening '/workspace/musicClaGen/data/raw/fma_audio/fma_small/099/099134.mp3': File does not exist or is not a regular file (possibly a pipe?).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed training epoch. Successful optimizer steps: 800\n",
      "Average Training Loss for Epoch: 0.2201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Loss: 0.2311\n",
      "  Validation Hamming Loss: 0.0691\n",
      "  Validation Jaccard Samples: 0.0000\n",
      "  Validation F1 Micro: 0.0000\n",
      "  Validation F1 Macro: 0.0000\n",
      "\n",
      "Epoch 5 finished.\n",
      "  Avg Train Loss: 0.2201\n",
      "  Validation Hamming Loss: 0.0691\n",
      "  Validation Jaccard Samples: 0.0000\n",
      "  Validation F1 Micro: 0.0000\n",
      "  Validation F1 Macro: 0.0000\n",
      "  Validation Eval Loss: 0.2311\n",
      "2025-05-04 08:24:27,722 - INFO - Validation metric did not improve (hamming_loss=0.0691). Best: 0.0691\n",
      "2025-05-04 08:24:27,724 - INFO - Epoch 5 finished in 47.28 minutes.\n",
      "2025-05-04 08:24:27,725 - INFO - \n",
      "--- Epoch 6/8 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▌         | 184/3200 [02:34<40:31,  1.24it/s, loss=0.2857, lr=1.84e-05]Note: Illegal Audio-MPEG-Header 0x00000000 at offset 22401.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 08:27:02,802 - ERROR - Error loading/processing track 98567: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 222, in __soundfile_load\n",
      "    y = sf_desc.read(frames=frame_duration, dtype=dtype, always_2d=False).T\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 942, in read\n",
      "    frames = self._array_io('read', out, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1394, in _array_io\n",
      "    return self._cdata_io(action, cdata, ctype, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1404, in _cdata_io\n",
      "    _error_check(self._errorcode)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1480, in _error_check\n",
      "    raise LibsndfileError(err, prefix=prefix)\n",
      "soundfile.LibsndfileError: Unspecified internal error.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|█▏        | 363/3200 [05:02<39:31,  1.20it/s, loss=0.1584, lr=1.80e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 08:29:29,918 - ERROR - Error loading/processing track 108925: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 209, in __soundfile_load\n",
      "    context = sf.SoundFile(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 690, in __init__\n",
      "    self._file = self._open(file, mode_int, closefd)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1265, in _open\n",
      "    raise LibsndfileError(err, prefix=\"Error opening {0!r}: \".format(self.name))\n",
      "soundfile.LibsndfileError: Error opening '/workspace/musicClaGen/data/raw/fma_audio/fma_small/108/108925.mp3': File does not exist or is not a regular file (possibly a pipe?).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/parse.c:do_readahead():1083] warning: Cannot read next header, a one-frame stream? Duh...\n",
      "Training:  14%|█▍        | 464/3200 [06:25<38:06,  1.20it/s, loss=0.2868, lr=1.78e-05][src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
      "Note: Illegal Audio-MPEG-Header 0x00000000 at offset 63168.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 08:30:53,035 - ERROR - Error loading/processing track 98569: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 222, in __soundfile_load\n",
      "    y = sf_desc.read(frames=frame_duration, dtype=dtype, always_2d=False).T\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 942, in read\n",
      "    frames = self._array_io('read', out, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1394, in _array_io\n",
      "    return self._cdata_io(action, cdata, ctype, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1404, in _cdata_io\n",
      "    _error_check(self._errorcode)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1480, in _error_check\n",
      "    raise LibsndfileError(err, prefix=prefix)\n",
      "soundfile.LibsndfileError: Unspecified internal error.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|██▌       | 817/3200 [11:15<33:28,  1.19it/s, loss=0.1579, lr=1.72e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 08:35:42,835 - ERROR - Error loading/processing track 133297: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 209, in __soundfile_load\n",
      "    context = sf.SoundFile(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 690, in __init__\n",
      "    self._file = self._open(file, mode_int, closefd)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1265, in _open\n",
      "    raise LibsndfileError(err, prefix=\"Error opening {0!r}: \".format(self.name))\n",
      "soundfile.LibsndfileError: Error opening '/workspace/musicClaGen/data/raw/fma_audio/fma_small/133/133297.mp3': File does not exist or is not a regular file (possibly a pipe?).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/parse.c:do_readahead():1083] warning: Cannot read next header, a one-frame stream? Duh...\n",
      "Training:  27%|██▋       | 862/3200 [11:50<29:22,  1.33it/s, loss=0.1579, lr=1.71e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 08:36:18,664 - ERROR - Error loading/processing track 99134: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 209, in __soundfile_load\n",
      "    context = sf.SoundFile(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 690, in __init__\n",
      "    self._file = self._open(file, mode_int, closefd)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1265, in _open\n",
      "    raise LibsndfileError(err, prefix=\"Error opening {0!r}: \".format(self.name))\n",
      "soundfile.LibsndfileError: Error opening '/workspace/musicClaGen/data/raw/fma_audio/fma_small/099/099134.mp3': File does not exist or is not a regular file (possibly a pipe?).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/parse.c:do_readahead():1083] warning: Cannot read next header, a one-frame stream? Duh...\n",
      "Training:  61%|██████    | 1957/3200 [26:49<16:17,  1.27it/s, loss=0.2259, lr=1.49e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 08:51:16,807 - ERROR - Error loading/processing track 98565: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 222, in __soundfile_load\n",
      "    y = sf_desc.read(frames=frame_duration, dtype=dtype, always_2d=False).T\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 942, in read\n",
      "    frames = self._array_io('read', out, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1394, in _array_io\n",
      "    return self._cdata_io(action, cdata, ctype, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1404, in _cdata_io\n",
      "    _error_check(self._errorcode)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1480, in _error_check\n",
      "    raise LibsndfileError(err, prefix=prefix)\n",
      "soundfile.LibsndfileError: Unspecified internal error.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Illegal Audio-MPEG-Header 0x00000000 at offset 33361.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      "Training:  62%|██████▏   | 1970/3200 [26:59<17:05,  1.20it/s, loss=0.2360, lr=1.49e-05][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (3328) too large for available bit count (3240)\n",
      "Training:  62%|██████▏   | 1977/3200 [27:05<17:14,  1.18it/s, loss=0.2200, lr=1.49e-05][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (3360) too large for available bit count (3240)\n",
      "Training:  73%|███████▎  | 2325/3200 [31:49<12:15,  1.19it/s, loss=0.3179, lr=1.42e-05][src/libmpg123/layer3.c:INT123_do_layer3():1844] error: dequantization failed!\n",
      "Training:  73%|███████▎  | 2347/3200 [32:07<11:48,  1.20it/s, loss=0.3290, lr=1.42e-05][src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
      "Training:  95%|█████████▍| 3036/3200 [41:31<02:18,  1.18it/s, loss=0.2414, lr=1.28e-05][src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed training epoch. Successful optimizer steps: 800\n",
      "Average Training Loss for Epoch: 0.2199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Loss: 0.2310\n",
      "  Validation Hamming Loss: 0.0691\n",
      "  Validation Jaccard Samples: 0.0000\n",
      "  Validation F1 Micro: 0.0000\n",
      "  Validation F1 Macro: 0.0000\n",
      "\n",
      "Epoch 6 finished.\n",
      "  Avg Train Loss: 0.2199\n",
      "  Validation Hamming Loss: 0.0691\n",
      "  Validation Jaccard Samples: 0.0000\n",
      "  Validation F1 Micro: 0.0000\n",
      "  Validation F1 Macro: 0.0000\n",
      "  Validation Eval Loss: 0.2310\n",
      "2025-05-04 09:11:48,896 - INFO - Validation metric did not improve (hamming_loss=0.0691). Best: 0.0691\n",
      "2025-05-04 09:11:48,897 - INFO - Epoch 6 finished in 47.35 minutes.\n",
      "2025-05-04 09:11:48,898 - INFO - \n",
      "--- Epoch 7/8 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 37/3200 [00:32<40:42,  1.30it/s, loss=0.1552, lr=1.24e-05] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 09:12:21,565 - ERROR - Error loading/processing track 99134: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 209, in __soundfile_load\n",
      "    context = sf.SoundFile(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 690, in __init__\n",
      "    self._file = self._open(file, mode_int, closefd)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1265, in _open\n",
      "    raise LibsndfileError(err, prefix=\"Error opening {0!r}: \".format(self.name))\n",
      "soundfile.LibsndfileError: Error opening '/workspace/musicClaGen/data/raw/fma_audio/fma_small/099/099134.mp3': File does not exist or is not a regular file (possibly a pipe?).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/parse.c:do_readahead():1083] warning: Cannot read next header, a one-frame stream? Duh...\n",
      "Training:  37%|███▋      | 1174/3200 [16:02<27:05,  1.25it/s, loss=0.1572, lr=1.02e-05][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (3360) too large for available bit count (3240)\n",
      "Training:  42%|████▏     | 1358/3200 [18:34<25:09,  1.22it/s, loss=0.1615, lr=9.85e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 09:30:23,397 - ERROR - Error loading/processing track 98565: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 222, in __soundfile_load\n",
      "    y = sf_desc.read(frames=frame_duration, dtype=dtype, always_2d=False).T\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 942, in read\n",
      "    frames = self._array_io('read', out, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1394, in _array_io\n",
      "    return self._cdata_io(action, cdata, ctype, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1404, in _cdata_io\n",
      "    _error_check(self._errorcode)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1480, in _error_check\n",
      "    raise LibsndfileError(err, prefix=prefix)\n",
      "soundfile.LibsndfileError: Unspecified internal error.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Illegal Audio-MPEG-Header 0x00000000 at offset 33361.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      "Training:  50%|████▉     | 1586/3200 [21:41<21:25,  1.26it/s, loss=0.1603, lr=9.41e-06][src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
      "Training:  64%|██████▍   | 2043/3200 [27:58<16:40,  1.16it/s, loss=0.2278, lr=8.52e-06][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (3328) too large for available bit count (3240)\n",
      "Training:  66%|██████▋   | 2127/3200 [29:07<14:12,  1.26it/s, loss=0.1567, lr=8.35e-06][src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
      "Training:  75%|███████▍  | 2393/3200 [32:46<10:37,  1.27it/s, loss=0.2323, lr=7.83e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 09:44:35,300 - ERROR - Error loading/processing track 98569: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 222, in __soundfile_load\n",
      "    y = sf_desc.read(frames=frame_duration, dtype=dtype, always_2d=False).T\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 942, in read\n",
      "    frames = self._array_io('read', out, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1394, in _array_io\n",
      "    return self._cdata_io(action, cdata, ctype, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1404, in _cdata_io\n",
      "    _error_check(self._errorcode)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1480, in _error_check\n",
      "    raise LibsndfileError(err, prefix=prefix)\n",
      "soundfile.LibsndfileError: Unspecified internal error.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
      "Note: Illegal Audio-MPEG-Header 0x00000000 at offset 63168.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      "Training:  78%|███████▊  | 2480/3200 [33:58<10:58,  1.09it/s, loss=0.2661, lr=7.66e-06][src/libmpg123/layer3.c:INT123_do_layer3():1844] error: dequantization failed!\n",
      "Training:  88%|████████▊ | 2811/3200 [38:32<05:19,  1.22it/s, loss=0.1565, lr=7.02e-06]Note: Illegal Audio-MPEG-Header 0x00000000 at offset 22401.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 09:50:21,371 - ERROR - Error loading/processing track 98567: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 222, in __soundfile_load\n",
      "    y = sf_desc.read(frames=frame_duration, dtype=dtype, always_2d=False).T\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 942, in read\n",
      "    frames = self._array_io('read', out, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1394, in _array_io\n",
      "    return self._cdata_io(action, cdata, ctype, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1404, in _cdata_io\n",
      "    _error_check(self._errorcode)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1480, in _error_check\n",
      "    raise LibsndfileError(err, prefix=prefix)\n",
      "soundfile.LibsndfileError: Unspecified internal error.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 2985/3200 [40:55<03:08,  1.14it/s, loss=0.1544, lr=6.67e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 09:52:44,369 - ERROR - Error loading/processing track 108925: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 209, in __soundfile_load\n",
      "    context = sf.SoundFile(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 690, in __init__\n",
      "    self._file = self._open(file, mode_int, closefd)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1265, in _open\n",
      "    raise LibsndfileError(err, prefix=\"Error opening {0!r}: \".format(self.name))\n",
      "soundfile.LibsndfileError: Error opening '/workspace/musicClaGen/data/raw/fma_audio/fma_small/108/108925.mp3': File does not exist or is not a regular file (possibly a pipe?).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/parse.c:do_readahead():1083] warning: Cannot read next header, a one-frame stream? Duh...\n",
      "Training:  97%|█████████▋| 3119/3200 [42:46<01:07,  1.21it/s, loss=0.1569, lr=6.41e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 09:54:36,056 - ERROR - Error loading/processing track 133297: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 209, in __soundfile_load\n",
      "    context = sf.SoundFile(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 690, in __init__\n",
      "    self._file = self._open(file, mode_int, closefd)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1265, in _open\n",
      "    raise LibsndfileError(err, prefix=\"Error opening {0!r}: \".format(self.name))\n",
      "soundfile.LibsndfileError: Error opening '/workspace/musicClaGen/data/raw/fma_audio/fma_small/133/133297.mp3': File does not exist or is not a regular file (possibly a pipe?).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/parse.c:do_readahead():1083] warning: Cannot read next header, a one-frame stream? Duh...\n",
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed training epoch. Successful optimizer steps: 800\n",
      "Average Training Loss for Epoch: 0.2198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Loss: 0.2309\n",
      "  Validation Hamming Loss: 0.0691\n",
      "  Validation Jaccard Samples: 0.0000\n",
      "  Validation F1 Micro: 0.0000\n",
      "  Validation F1 Macro: 0.0000\n",
      "\n",
      "Epoch 7 finished.\n",
      "  Avg Train Loss: 0.2198\n",
      "  Validation Hamming Loss: 0.0691\n",
      "  Validation Jaccard Samples: 0.0000\n",
      "  Validation F1 Micro: 0.0000\n",
      "  Validation F1 Macro: 0.0000\n",
      "  Validation Eval Loss: 0.2309\n",
      "2025-05-04 09:59:13,633 - INFO - Validation metric did not improve (hamming_loss=0.0691). Best: 0.0691\n",
      "2025-05-04 09:59:13,635 - INFO - Epoch 7 finished in 47.41 minutes.\n",
      "2025-05-04 09:59:13,636 - INFO - \n",
      "--- Epoch 8/8 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 106/3200 [01:28<41:24,  1.25it/s, loss=0.3084, lr=6.05e-06]Note: Illegal Audio-MPEG-Header 0x00000000 at offset 22401.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 10:00:42,766 - ERROR - Error loading/processing track 98567: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 222, in __soundfile_load\n",
      "    y = sf_desc.read(frames=frame_duration, dtype=dtype, always_2d=False).T\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 942, in read\n",
      "    frames = self._array_io('read', out, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1394, in _array_io\n",
      "    return self._cdata_io(action, cdata, ctype, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1404, in _cdata_io\n",
      "    _error_check(self._errorcode)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1480, in _error_check\n",
      "    raise LibsndfileError(err, prefix=prefix)\n",
      "soundfile.LibsndfileError: Unspecified internal error.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|▊         | 276/3200 [03:45<40:06,  1.21it/s, loss=0.2274, lr=5.71e-06][src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
      "Training:  31%|███       | 980/3200 [13:23<29:58,  1.23it/s, loss=0.2360, lr=4.34e-06][src/libmpg123/parse.c:do_readahead():1083] warning: Cannot read next header, a one-frame stream? Duh...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 10:12:37,409 - ERROR - Error loading/processing track 99134: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 209, in __soundfile_load\n",
      "    context = sf.SoundFile(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 690, in __init__\n",
      "    self._file = self._open(file, mode_int, closefd)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1265, in _open\n",
      "    raise LibsndfileError(err, prefix=\"Error opening {0!r}: \".format(self.name))\n",
      "soundfile.LibsndfileError: Error opening '/workspace/musicClaGen/data/raw/fma_audio/fma_small/099/099134.mp3': File does not exist or is not a regular file (possibly a pipe?).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|█████▋    | 1805/3200 [24:41<19:09,  1.21it/s, loss=0.3048, lr=2.73e-06][src/libmpg123/parse.c:do_readahead():1083] warning: Cannot read next header, a one-frame stream? Duh...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 10:23:55,461 - ERROR - Error loading/processing track 133297: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 209, in __soundfile_load\n",
      "    context = sf.SoundFile(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 690, in __init__\n",
      "    self._file = self._open(file, mode_int, closefd)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1265, in _open\n",
      "    raise LibsndfileError(err, prefix=\"Error opening {0!r}: \".format(self.name))\n",
      "soundfile.LibsndfileError: Error opening '/workspace/musicClaGen/data/raw/fma_audio/fma_small/133/133297.mp3': File does not exist or is not a regular file (possibly a pipe?).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|█████▊    | 1859/3200 [25:25<17:40,  1.26it/s, loss=0.2259, lr=2.62e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 10:24:38,891 - ERROR - Error loading/processing track 98569: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 222, in __soundfile_load\n",
      "    y = sf_desc.read(frames=frame_duration, dtype=dtype, always_2d=False).T\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 942, in read\n",
      "    frames = self._array_io('read', out, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1394, in _array_io\n",
      "    return self._cdata_io(action, cdata, ctype, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1404, in _cdata_io\n",
      "    _error_check(self._errorcode)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1480, in _error_check\n",
      "    raise LibsndfileError(err, prefix=prefix)\n",
      "soundfile.LibsndfileError: Unspecified internal error.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
      "Note: Illegal Audio-MPEG-Header 0x00000000 at offset 63168.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      "Training:  73%|███████▎  | 2335/3200 [31:53<11:14,  1.28it/s, loss=0.2276, lr=1.70e-06][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (3328) too large for available bit count (3240)\n",
      "Training:  77%|███████▋  | 2472/3200 [33:44<09:45,  1.24it/s, loss=0.2252, lr=1.42e-06][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (3360) too large for available bit count (3240)\n",
      "Training:  82%|████████▏ | 2608/3200 [35:36<08:15,  1.20it/s, loss=0.1569, lr=1.16e-06][src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
      "Training:  84%|████████▍ | 2694/3200 [36:46<06:49,  1.24it/s, loss=0.2471, lr=9.92e-07][src/libmpg123/parse.c:do_readahead():1083] warning: Cannot read next header, a one-frame stream? Duh...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 10:36:00,401 - ERROR - Error loading/processing track 108925: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 209, in __soundfile_load\n",
      "    context = sf.SoundFile(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 690, in __init__\n",
      "    self._file = self._open(file, mode_int, closefd)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1265, in _open\n",
      "    raise LibsndfileError(err, prefix=\"Error opening {0!r}: \".format(self.name))\n",
      "soundfile.LibsndfileError: Error opening '/workspace/musicClaGen/data/raw/fma_audio/fma_small/108/108925.mp3': File does not exist or is not a regular file (possibly a pipe?).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|█████████▏| 2940/3200 [40:09<03:47,  1.14it/s, loss=0.1569, lr=5.08e-07]Note: Illegal Audio-MPEG-Header 0x00000000 at offset 33361.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 10:39:23,756 - ERROR - Error loading/processing track 98565: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 222, in __soundfile_load\n",
      "    y = sf_desc.read(frames=frame_duration, dtype=dtype, always_2d=False).T\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 942, in read\n",
      "    frames = self._array_io('read', out, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1394, in _array_io\n",
      "    return self._cdata_io(action, cdata, ctype, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1404, in _cdata_io\n",
      "    _error_check(self._errorcode)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1480, in _error_check\n",
      "    raise LibsndfileError(err, prefix=prefix)\n",
      "soundfile.LibsndfileError: Unspecified internal error.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_4431/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|█████████▌| 3046/3200 [41:36<02:04,  1.23it/s, loss=0.1575, lr=3.05e-07][src/libmpg123/layer3.c:INT123_do_layer3():1844] error: dequantization failed!\n",
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed training epoch. Successful optimizer steps: 800\n",
      "Average Training Loss for Epoch: 0.2196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Loss: 0.2310\n",
      "  Validation Hamming Loss: 0.0691\n",
      "  Validation Jaccard Samples: 0.0000\n",
      "  Validation F1 Micro: 0.0000\n",
      "  Validation F1 Macro: 0.0000\n",
      "\n",
      "Epoch 8 finished.\n",
      "  Avg Train Loss: 0.2196\n",
      "  Validation Hamming Loss: 0.0691\n",
      "  Validation Jaccard Samples: 0.0000\n",
      "  Validation F1 Micro: 0.0000\n",
      "  Validation F1 Macro: 0.0000\n",
      "  Validation Eval Loss: 0.2310\n",
      "2025-05-04 10:46:34,901 - INFO - Validation metric did not improve (hamming_loss=0.0691). Best: 0.0691\n",
      "2025-05-04 10:46:34,903 - INFO - Epoch 8 finished in 47.35 minutes.\n",
      "2025-05-04 10:46:34,904 - INFO - --- Training Finished in 378.94 minutes ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Cell 11: Run Full Training Loop\n",
    "\n",
    "# Clear CUDA cache and force garbage collection\n",
    "import gc\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Check memory usage before training\n",
    "print(f\"GPU memory allocated before training: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"GPU memory reserved before training: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "\n",
    "\n",
    "# Make sure model, criterion, optimizer, scheduler, dataloaders defined from previous cells\n",
    "num_epochs = config.MODEL_PARAMS[\"epochs\"] # Get actual epochs from config\n",
    "gradient_accumulation_steps = config.MODEL_PARAMS[\"gradient_accumulation_steps\"]\n",
    "metric_to_monitor = 'hamming_loss' # Metric to decide best model (lower is better)\n",
    "best_val_metric = float('inf')\n",
    "\n",
    "# --- Initialize GradScaler for AMP ---\n",
    "scaler = GradScaler(enabled=(device.type == 'cuda'))\n",
    "# ------------------------------------\n",
    "\n",
    "logging.info(f\"--- Starting FULL Training for {num_epochs} epochs ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Make sure model and criterion are on the correct device\n",
    "model.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    logging.info(f\"\\n--- Epoch {epoch+1}/{num_epochs} ---\")\n",
    "\n",
    "    # Run training for one epoch\n",
    "    train_loss = train_epoch(\n",
    "        model, train_dataloader, criterion, optimizer, device,\n",
    "        gradient_accumulation_steps, scaler, scheduler # Pass scaler and scheduler\n",
    "    )\n",
    "\n",
    "    # Run evaluation on validation set\n",
    "    eval_metrics = evaluate(model, val_dataloader, criterion, device)\n",
    "\n",
    "    print(f\"\\nEpoch {epoch+1} finished.\")\n",
    "    print(f\"  Avg Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "    if not eval_metrics:\n",
    "        logging.warning(f\"Epoch {epoch+1}: Evaluation failed, skipping checkpoint.\")\n",
    "        continue\n",
    "\n",
    "    # Log all validation metrics\n",
    "    for name, value in eval_metrics.items():\n",
    "        print(f\"  Validation {name.replace('_', ' ').title()}: {value:.4f}\")\n",
    "\n",
    "    # Save model checkpoint if validation metric improved\n",
    "    current_val_metric = eval_metrics.get(metric_to_monitor, float('inf'))\n",
    "    if current_val_metric < best_val_metric:\n",
    "        best_val_metric = current_val_metric\n",
    "        # Use a consistent name for the best model checkpoint\n",
    "        save_path = os.path.join(model_save_dir, f\"{model_checkpoint.replace('/', '_')}_finetuned_best.pth\")\n",
    "        try:\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            logging.info(f\"Validation metric improved ({metric_to_monitor}={current_val_metric:.4f}). Saved best model to {save_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to save model checkpoint: {e}\", exc_info=True)\n",
    "    else:\n",
    "         logging.info(f\"Validation metric did not improve ({metric_to_monitor}={current_val_metric:.4f}). Best: {best_val_metric:.4f}\")\n",
    "\n",
    "    epoch_duration = time.time() - epoch_start_time\n",
    "    logging.info(f\"Epoch {epoch+1} finished in {epoch_duration / 60:.2f} minutes.\")\n",
    "\n",
    "total_training_time = time.time() - start_time\n",
    "logging.info(f\"--- Training Finished in {total_training_time / 60:.2f} minutes ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "• Time: Took ~47 minutes for 1 epoch on the full `fma_small` training set (~6400 samples). This is a realistic time given the model size, 30s inputs, data loading, and AMP.\n",
    "\n",
    "• Errors During Training: The log shows several errors during the training loop:\n",
    "\n",
    "  • `ERROR - Error loading/processing track ...`\n",
    "  \n",
    "  `audioread.exceptions.NoBackendError`: This error occurred multiple times (tracks 133297, 99134, 98569, 98567, 98565, 108925). It indicates `librosa.load` failed. It first tries `soundfile` (which fails often with MP3s, sometimes due to file existence/permissions or internal errors), then falls back to `audioread`, which then fails because no suitable backend (like `ffmpeg`) was found or successfully used by `audioread`. This is despite installing `ffmpeg` earlier. It suggests `librosa`'s fallback mechanism isn't working reliably in this environment.\n",
    "\n",
    "  • `[src/libmpg123/...]: warning: Cannot read next header...`, `error: dequantization failed!`, `error: part2_3_length ... too large...`, `error: Giving up resync...`: These are lower-level MP3 decoding errors from the `mpg123` library, likely called by `audioread` or another backend. They indicate corrupted or non-standard MP3 files.\n",
    "\n",
    "\n",
    "When checked the documentation on fma github(https://github.com/mdeff/fma/wiki), these track IDs are flawed indeed, so everything is fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 10:46:34,932 - INFO - \n",
      "--- Evaluating on Test Set using Best Model ---\n",
      "2025-05-04 10:46:34,936 - INFO - Loading best model from /workspace/musicClaGen/models/facebook_w2v-bert-2.0_finetuned_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2BertForSequenceClassification were not initialized from the model checkpoint at facebook/w2v-bert-2.0 and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 10:46:41,758 - INFO - Model successfully loaded and moved to device\n",
      "2025-05-04 10:46:41,761 - INFO - Created safer test dataloader without worker processes\n",
      "2025-05-04 10:46:41,762 - INFO - Starting evaluation on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Loss: 0.1972\n",
      "  Validation Hamming Loss: 0.0574\n",
      "  Validation Jaccard Samples: 0.0000\n",
      "  Validation F1 Micro: 0.0000\n",
      "  Validation F1 Macro: 0.0000\n",
      "2025-05-04 10:55:42,077 - INFO - \n",
      "--- Final Test Set Results (completed in 540.31s) ---\n",
      "2025-05-04 10:55:42,078 - INFO - Test Hamming Loss: 0.0574\n",
      "2025-05-04 10:55:42,080 - INFO - Test Jaccard Samples: 0.0000\n",
      "2025-05-04 10:55:42,081 - INFO - Test F1 Micro: 0.0000\n",
      "2025-05-04 10:55:42,082 - INFO - Test F1 Macro: 0.0000\n",
      "2025-05-04 10:55:42,083 - INFO - Test Eval Loss: 0.1972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Cell 12: Evaluate Best Model on Test Set (Robust Version)\n",
    "\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from transformers import AutoModelForAudioClassification\n",
    "import logging\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "logging.info(\"\\n--- Evaluating on Test Set using Best Model ---\")\n",
    "\n",
    "# Construct path to the best saved model\n",
    "best_model_path = os.path.join(model_save_dir, f\"{model_checkpoint.replace('/', '_')}_finetuned_best.pth\")\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    try:\n",
    "        logging.info(f\"Loading best model from {best_model_path}\")\n",
    "        \n",
    "        # Re-initialize model with correct structure\n",
    "        model_reloaded = AutoModelForAudioClassification.from_pretrained(\n",
    "            model_checkpoint,\n",
    "            num_labels=num_labels,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        \n",
    "        # Load the saved state dict\n",
    "        model_reloaded.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "        model_reloaded.to(device)\n",
    "        model_reloaded.eval()\n",
    "        logging.info(\"Model successfully loaded and moved to device\")\n",
    "        \n",
    "        # Create a safer test dataloader with no workers (avoid multiprocessing issues)\n",
    "        safe_test_dataloader = DataLoader(\n",
    "            test_dataset, \n",
    "            batch_size=config.MODEL_PARAMS[\"batch_size\"],\n",
    "            shuffle=False,\n",
    "            collate_fn=data_collator,\n",
    "            num_workers=0,  # Use main process only - no worker processes\n",
    "            pin_memory=False  # Disable pinned memory to reduce memory usage\n",
    "        )\n",
    "        logging.info(\"Created safer test dataloader without worker processes\")\n",
    "        \n",
    "        # Run evaluation with extra error handling\n",
    "        logging.info(\"Starting evaluation on test set...\")\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            test_metrics = evaluate(model_reloaded, safe_test_dataloader, criterion, device)\n",
    "            eval_time = time.time() - start_time\n",
    "            \n",
    "            # Log test results\n",
    "            logging.info(f\"\\n--- Final Test Set Results (completed in {eval_time:.2f}s) ---\")\n",
    "            if test_metrics:\n",
    "                for metric_name, metric_value in test_metrics.items():\n",
    "                    logging.info(f\"Test {metric_name.replace('_', ' ').title()}: {metric_value:.4f}\")\n",
    "            else:\n",
    "                logging.info(\"Test evaluation failed to produce metrics.\")\n",
    "        \n",
    "        except RuntimeError as e:\n",
    "            if \"CUDA out of memory\" in str(e):\n",
    "                logging.error(\"CUDA out of memory during evaluation. Try reducing batch size.\")\n",
    "            elif \"DataLoader worker\" in str(e):\n",
    "                logging.error(f\"DataLoader worker error (should not happen with num_workers=0): {e}\")\n",
    "            else:\n",
    "                logging.error(f\"Runtime error during evaluation: {e}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during evaluation: {e}\", exc_info=True)\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load model: {e}\", exc_info=True)\n",
    "else:\n",
    "    logging.warning(f\"Best model checkpoint not found at {best_model_path}. Skipping final test evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 12: High-CPU-Utilization Aware Evaluation\n",
    "\n",
    "# import torch.nn as nn\n",
    "# import os\n",
    "# import psutil\n",
    "# import time\n",
    "# from transformers import AutoModelForAudioClassification\n",
    "# import logging\n",
    "# from torch.utils.data import DataLoader, Subset\n",
    "# import random\n",
    "\n",
    "# logging.info(\"\\n--- Evaluating on Test Set using Best Model ---\")\n",
    "\n",
    "# # Get CPU utilization\n",
    "# cpu_percent = psutil.cpu_percent(interval=1)\n",
    "# logging.info(f\"Current CPU utilization: {cpu_percent}%\")\n",
    "\n",
    "# # Determine safe worker count based on CPU utilization\n",
    "# if cpu_percent > 90:\n",
    "#     worker_count = 0  # No workers if CPU is nearly maxed out\n",
    "#     logging.warning(\"High CPU utilization detected. Using 0 workers for stability.\")\n",
    "# elif cpu_percent > 70:\n",
    "#     worker_count = 1  # Minimal workers if CPU is heavily used\n",
    "# else:\n",
    "#     worker_count = min(2, os.cpu_count() // 4)  # Conservative: 1/4 of cores with max of 2\n",
    "\n",
    "# # Construct path to the best saved model\n",
    "# best_model_path = os.path.join(model_save_dir, f\"{model_checkpoint.replace('/', '_')}_finetuned_best.pth\")\n",
    "\n",
    "# if os.path.exists(best_model_path):\n",
    "#     try:\n",
    "#         logging.info(f\"Loading best model from {best_model_path}\")\n",
    "        \n",
    "#         # Re-initialize model with correct structure\n",
    "#         model_reloaded = AutoModelForAudioClassification.from_pretrained(\n",
    "#             model_checkpoint,\n",
    "#             num_labels=num_labels,\n",
    "#             ignore_mismatched_sizes=True\n",
    "#         )\n",
    "        \n",
    "#         # Load the saved state dict\n",
    "#         model_reloaded.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "#         model_reloaded.to(device)\n",
    "#         model_reloaded.eval()\n",
    "#         logging.info(\"Model successfully loaded and moved to device\")\n",
    "        \n",
    "#         # Create optimized test dataloader\n",
    "#         safe_test_dataloader = DataLoader(\n",
    "#             test_dataset, \n",
    "#             batch_size=config.MODEL_PARAMS[\"batch_size\"],\n",
    "#             shuffle=False,\n",
    "#             collate_fn=data_collator,\n",
    "#             num_workers=worker_count,\n",
    "#             pin_memory=(device.type=='cuda' and worker_count > 0)\n",
    "#         )\n",
    "#         logging.info(f\"Created test dataloader with {worker_count} workers\")\n",
    "        \n",
    "#         # Run evaluation with error handling\n",
    "#         logging.info(\"Starting evaluation on test set...\")\n",
    "#         try:\n",
    "#             start_time = time.time()\n",
    "#             test_metrics = evaluate(model_reloaded, safe_test_dataloader, criterion, device)\n",
    "#             eval_time = time.time() - start_time\n",
    "            \n",
    "#             # Log test results\n",
    "#             logging.info(f\"\\n--- Final Test Set Results (completed in {eval_time:.2f}s) ---\")\n",
    "#             if test_metrics:\n",
    "#                 for metric_name, metric_value in test_metrics.items():\n",
    "#                     logging.info(f\"Test {metric_name.replace('_', ' ').title()}: {metric_value:.4f}\")\n",
    "#             else:\n",
    "#                 logging.info(\"Test evaluation failed to produce metrics.\")\n",
    "        \n",
    "#         except Exception as e:\n",
    "#             logging.error(f\"Error during evaluation: {e}\")\n",
    "#             logging.info(\"Falling back to zero workers and smaller batch size...\")\n",
    "            \n",
    "#             # Fallback DataLoader with zero workers and smaller batch size\n",
    "#             fallback_loader = DataLoader(\n",
    "#                 test_dataset,\n",
    "#                 batch_size=max(1, config.MODEL_PARAMS[\"batch_size\"] // 2),\n",
    "#                 shuffle=False,\n",
    "#                 collate_fn=data_collator,\n",
    "#                 num_workers=0\n",
    "#             )\n",
    "            \n",
    "#             try:\n",
    "#                 test_metrics = evaluate(model_reloaded, fallback_loader, criterion, device)\n",
    "#                 if test_metrics:\n",
    "#                     for metric_name, metric_value in test_metrics.items():\n",
    "#                         logging.info(f\"Test {metric_name.replace('_', ' ').title()}: {metric_value:.4f}\")\n",
    "#             except Exception as inner_e:\n",
    "#                 logging.error(f\"Fallback evaluation also failed: {inner_e}\")\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Failed to load model: {e}\", exc_info=True)\n",
    "# else:\n",
    "#     logging.warning(f\"Best model checkpoint not found at {best_model_path}. Skipping final test evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import psutil\n",
    "# import torch\n",
    "\n",
    "# def get_optimal_workers():\n",
    "#     # Check number of CPU cores\n",
    "#     cpu_count = os.cpu_count()\n",
    "    \n",
    "#     # Check available memory (GB)\n",
    "#     available_memory = psutil.virtual_memory().available / (1024 ** 3)\n",
    "    \n",
    "#     # Check if CUDA is available\n",
    "#     cuda_available = torch.cuda.is_available()\n",
    "    \n",
    "#     # Get GPU memory if available (GB)\n",
    "#     gpu_memory = 0\n",
    "#     if cuda_available:\n",
    "#         gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)\n",
    "        \n",
    "#     # Print system info\n",
    "#     print(f\"Available CPU cores: {cpu_count}\")\n",
    "#     print(f\"Available system memory: {available_memory:.2f} GB\")\n",
    "#     if cuda_available:\n",
    "#         print(f\"GPU memory: {gpu_memory:.2f} GB\")\n",
    "    \n",
    "#     # Conservative recommendation: half the cores is usually safe\n",
    "#     recommended = max(1, cpu_count // 2)\n",
    "    \n",
    "#     # Account for memory-intensive operations (each worker might need ~2-4GB)\n",
    "#     # Adjust this estimate based on your dataset's memory footprint\n",
    "#     memory_limited = max(1, int(available_memory / 4))\n",
    "    \n",
    "#     final_recommendation = min(recommended, memory_limited)\n",
    "#     print(f\"\\nRecommended num_workers: {final_recommendation}\")\n",
    "    \n",
    "#     return final_recommendation\n",
    "\n",
    "# # Get recommendation\n",
    "# optimal_workers = get_optimal_workers()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicClaGen_env22",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
