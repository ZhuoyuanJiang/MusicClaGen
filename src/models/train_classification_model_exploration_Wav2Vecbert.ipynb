{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhuoyuan/miniconda3/envs/musicClaGen_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT detected as: /home/zhuoyuan/CSprojects/musicClaGen\n",
      "Adding /home/zhuoyuan/CSprojects/musicClaGen to sys.path\n",
      "/home/zhuoyuan/CSprojects/musicClaGen\n",
      "Imports and basic setup complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "# Use AutoProcessor for Wav2Vec2-BERT - it bundles feature_extractor and tokenizer (if needed)\n",
    "from transformers import AutoModelForAudioClassification, AutoProcessor\n",
    "\n",
    "from torch.optim import AdamW\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import ast # For parsing string representations of lists/arrays\n",
    "import logging\n",
    "import time\n",
    "from sklearn.metrics import hamming_loss, jaccard_score, f1_score # Add more as needed\n",
    "from tqdm.notebook import tqdm # Use notebook version of tqdm\n",
    "import librosa # Needed for loading raw audio now\n",
    "\n",
    "\n",
    "\n",
    "# --- Project Setup ---\n",
    "# Detect if running in notebook or script to adjust path\n",
    "\n",
    "cwd = os.getcwd()\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(cwd, '../../')) # NOTE: remember to change if change the directory structure\n",
    "\n",
    "\n",
    "\n",
    "print(f\"PROJECT_ROOT detected as: {PROJECT_ROOT}\")\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    print(f\"Adding {PROJECT_ROOT} to sys.path\")\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "# --- Config and Utils ---\n",
    "try:\n",
    "    import config # Import your configuration file\n",
    "    # Optionally import utils if needed, e.g., for get_audio_path if not defined here\n",
    "    # import src.utils as utils\n",
    "except ModuleNotFoundError:\n",
    "     print(\"ERROR: Cannot import config or utils. Make sure PROJECT_ROOT is correct and src is importable.\")\n",
    "     # Or add src to path: sys.path.insert(0, os.path.join(PROJECT_ROOT, 'src'))\n",
    "     # import config\n",
    "     # import utils\n",
    "\n",
    "\n",
    "# --- Setup Logging ---\n",
    "for handler in logging.root.handlers[:]: logging.root.removeHandler(handler) # Clear previous\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    handlers=[logging.StreamHandler(sys.stdout)])\n",
    "\n",
    "print(\"Imports and basic setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-02 21:54:39,165 - INFO - Loaded 22 unified genres from /home/zhuoyuan/CSprojects/musicClaGen/data/processed/unified_genres.txt\n",
      "2025-05-02 21:54:39,166 - INFO - Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Cell 2\n",
    "# --- Load Config ---\n",
    "# Ensure config.py has the correct paths in the PATHS dict\n",
    "manifest_path = config.PATHS.get('SMALL_MULTILABEL_PATH', os.path.join(config.PATHS['PROCESSED_DATA_DIR'], 'small_subset_multihot.csv')) # Use .get for safety\n",
    "genre_list_path = config.PATHS.get('GENRE_LIST_PATH', os.path.join(config.PATHS['PROCESSED_DATA_DIR'], 'unified_genres.txt'))\n",
    "model_save_dir = config.PATHS['MODELS_DIR']\n",
    "\n",
    "# Ensure config.py has MODEL_PARAMS dict with model_checkpoint\n",
    "model_checkpoint = config.MODEL_PARAMS['model_checkpoint'] # e.g., \"facebook/w2v-bert-2.0\" - VERIFY!\n",
    "learning_rate = config.MODEL_PARAMS['learning_rate']\n",
    "batch_size = config.MODEL_PARAMS['batch_size'] # Use the small BS for notebook test\n",
    "num_epochs_debug = 1 # <<<--- RUN ONLY 1 EPOCH FOR DEBUGGING ---<<<\n",
    "weight_decay = config.MODEL_PARAMS['weight_decay']\n",
    "gradient_accumulation_steps = config.MODEL_PARAMS['gradient_accumulation_steps']\n",
    "\n",
    "# --- Load unified genre list ---\n",
    "try:\n",
    "    with open(genre_list_path, 'r') as f:\n",
    "        unified_genres = [line.strip() for line in f if line.strip()]\n",
    "    num_labels = len(unified_genres) # should be the number of labels defined in the unified_genres.txt file, in this case it should be 22.\n",
    "    logging.info(f\"Loaded {num_labels} unified genres from {genre_list_path}\")\n",
    "    if num_labels == 0: raise ValueError(\"Genre list is empty!\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to load or process unified genre list: {e}\", exc_info=True)\n",
    "    raise SystemExit(\"Cannot proceed without genre list.\")\n",
    "\n",
    "# --- Setup Device ---\n",
    "device = torch.device(config.DEVICE if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(f\"Using device: {device}\")\n",
    "if not torch.cuda.is_available() and config.DEVICE==\"cuda\":\n",
    "     logging.warning(\"CUDA selected but not available, falling back to CPU.\")\n",
    "\n",
    "# --- Create Save Directory ---\n",
    "os.makedirs(model_save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zhuoyuan/CSprojects/musicClaGen/data/processed/small_subset_multihot.csv\n"
     ]
    }
   ],
   "source": [
    "print(manifest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FMARawAudioDataset class defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Dataset Class Definition (Raw Audio Version)\n",
    "\n",
    "\n",
    "\n",
    "# Define(recollect)the regex parser from preprocess.py if needed, \n",
    "# otherwise use ast.literal_eval---\n",
    "\n",
    "import re\n",
    "\n",
    "def parse_numpy_array_string(array_str):\n",
    "    \"\"\"\n",
    "    Parse strings like '[np.float32(1.0), np.float32(0.0), ...]' into a list of integers.\n",
    "    This is needed because ast.literal_eval cannot handle 'np.float32()' in the string.\n",
    "    \"\"\"\n",
    "    if not isinstance(array_str, str):\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Extract all the float values using regular expressions\n",
    "        float_matches = re.findall(r'np\\.float32\\((\\d+\\.\\d+)\\)', array_str)\n",
    "        \n",
    "        # Convert matches to integers (1.0 -> 1, 0.0 -> 0)\n",
    "        values = []\n",
    "        for match in float_matches:\n",
    "            value = float(match)\n",
    "            # Convert to integer if it's 0.0 or 1.0\n",
    "            if value == 1.0:\n",
    "                values.append(1)\n",
    "            elif value == 0.0:\n",
    "                values.append(0)\n",
    "            else:\n",
    "                values.append(value)  # Keep as float if not 0 or 1\n",
    "                \n",
    "        return values\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error parsing array string: {e}\")\n",
    "        return []\n",
    "\n",
    "class FMARawAudioDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads raw audio waveforms and labels from manifest, uses Hugging Face\n",
    "    feature extractor (like ASTFeatureExtractor or Wav2Vec2Processor) on the fly.\n",
    "    \"\"\"\n",
    "    def __init__(self, manifest_path, feature_extractor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            manifest_path (str): Path to the final manifest CSV file.\n",
    "            feature_extractor: Initialized Hugging Face AutoFeatureExtractor or AutoProcessor.\n",
    "        \"\"\"\n",
    "        logging.info(f\"Initializing FMARawAudioDataset from: {manifest_path}\")\n",
    "        if feature_extractor is None:\n",
    "             raise ValueError(\"FMARawAudioDataset requires a feature_extractor/processor instance.\")\n",
    "\n",
    "        self.feature_extractor = feature_extractor\n",
    "        # Get target sampling rate directly from the extractor/processor\n",
    "        try:\n",
    "             # Works for Wav2Vec2Processor, ASTFeatureExtractor, etc.\n",
    "             self.target_sr = self.feature_extractor.sampling_rate\n",
    "             logging.info(f\"Target sampling rate set from feature extractor: {self.target_sr} Hz\")\n",
    "        except AttributeError:\n",
    "             logging.warning(\"Could not get sampling_rate from feature_extractor, using config.\")\n",
    "             # Fallback to config if needed, but ensuring match is crucial\n",
    "             self.target_sr = config.PREPROCESSING_PARAMS['sample_rate']\n",
    "\n",
    "\n",
    "        logging.info(f\"Loading manifest from: {manifest_path}\")\n",
    "        try:\n",
    "            self.manifest = pd.read_csv(manifest_path)\n",
    "            # Ensure index is set if needed elsewhere, or use default range index\n",
    "            if 'track_id' in self.manifest.columns:\n",
    "                 self.manifest = self.manifest.set_index('track_id', drop=False)\n",
    "\n",
    "            # --- Parse the 'multi_hot_label' string back into a list ---\n",
    "            # Here: if we decide to use raw audio, we use regex parser; \n",
    "            #       if we decide to use mel spectrogram, we use ast.literal_eval\n",
    "\n",
    "            # Choose the correct parser based on how labels were saved in the CSV\n",
    "            # If saved as '[1.0, 0.0,...]' use ast.literal_eval\n",
    "            # label_parser = ast.literal_eval\n",
    "            # If saved as '[np.float32(1.0)...]' uncomment and use regex parser\n",
    "            label_parser = parse_numpy_array_string\n",
    "\n",
    "            self.manifest['multi_hot_label'] = self.manifest['multi_hot_label'].apply(label_parser)\n",
    "            logging.info(f\"Loaded and parsed manifest with {len(self.manifest)} entries.\")\n",
    "            # Check the first parsed label\n",
    "            logging.info(f\"Example parsed label (first entry): {self.manifest['multi_hot_label'].iloc[0]}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading or parsing manifest {manifest_path}: {e}\", exc_info=True)\n",
    "            raise\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.manifest)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Loads raw audio for index idx, processes it with the feature extractor,\n",
    "        and returns the processed inputs and labels.\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx): idx = idx.tolist() # Handle tensor indices\n",
    "\n",
    "        # Get the row data from the manifest\n",
    "        row = self.manifest.iloc[idx]\n",
    "        track_id = row.get('track_id', self.manifest.index[idx]) # Get track_id safely\n",
    "        label_vector = row['multi_hot_label'] # Already parsed list/array\n",
    "\n",
    "        # Construct absolute audio path if necessary\n",
    "        audio_path = row['audio_path']\n",
    "\n",
    "        #NOTE: originally, the mel-spectrogram's path is relative  but the raw audio's path is absolute, so we need to make sure the audio_path is absolute\n",
    "        # So we are check if the audio_path is absolute or relative in case we load the wrong data, if it's relative, we need to join it with the PROJECT_ROOT\n",
    "        if not os.path.isabs(audio_path):\n",
    "             # Assumes path in manifest is relative to PROJECT_ROOT\n",
    "             audio_path = os.path.join(config.PROJECT_ROOT, audio_path)\n",
    "\n",
    "        try:\n",
    "            # --- 1. Load RAW Audio Waveform ---\n",
    "            # Load full 30s clip at the TARGET sample rate required by the processor\n",
    "            waveform, loaded_sr = librosa.load(\n",
    "                audio_path,\n",
    "                sr=self.target_sr, # Use processor's sampling rate\n",
    "                duration=30.0     # Load the full 30 seconds\n",
    "            )\n",
    "            # Ensure minimum length if needed (though duration should handle it)\n",
    "            min_samples = int(0.1 * self.target_sr) # Example: require at least 0.1s\n",
    "            if len(waveform) < min_samples:\n",
    "                 raise ValueError(f\"Audio signal for track {track_id} too short after loading.\")\n",
    "\n",
    "            # --- 2. Apply Feature Extractor ---\n",
    "            # Pass the raw waveform numpy array\n",
    "            # The extractor handles normalization, padding/truncation, tensor conversion\n",
    "            \n",
    "            max_length = 5000\n",
    "\n",
    "            inputs = self.feature_extractor(\n",
    "                waveform,\n",
    "                sampling_rate=self.target_sr,\n",
    "                return_tensors=\"pt\",\n",
    "                return_attention_mask=True # Request attention mask\n",
    "            )\n",
    "\n",
    "            # --- 3. Prepare Outputs ---\n",
    "            # Squeeze unnecessary batch dimension added by the extractor\n",
    "            # Key name ('input_values', 'input_features') depends on the specific extractor\n",
    "            feature_tensor = inputs.get('input_values', inputs.get('input_features'))\n",
    "            if feature_tensor is None:\n",
    "                raise KeyError(\"Expected 'input_values' or 'input_features' key from feature_extractor output.\")\n",
    "            feature_tensor = feature_tensor.squeeze(0) # Remove batch dim -> [Channels?, Freq?, Time] or [SeqLen, Dim]\n",
    "\n",
    "            attention_mask = inputs.get('attention_mask', None)\n",
    "            if attention_mask is not None:\n",
    "                 attention_mask = attention_mask.squeeze(0)\n",
    "\n",
    "            # Convert label list/array to float tensor for BCE loss\n",
    "            label_tensor = torch.tensor(label_vector, dtype=torch.float32)\n",
    "\n",
    "            # Return dictionary matching model's expected input names\n",
    "            model_input_dict = {\"labels\": label_tensor}\n",
    "            # Use the key the feature extractor provided\n",
    "            if 'input_values' in inputs:\n",
    "                 model_input_dict['input_values'] = feature_tensor\n",
    "            elif 'input_features' in inputs:\n",
    "                 model_input_dict['input_features'] = feature_tensor\n",
    "\n",
    "            if attention_mask is not None:\n",
    "                 model_input_dict['attention_mask'] = attention_mask\n",
    "\n",
    "            return model_input_dict\n",
    "\n",
    "        except FileNotFoundError:\n",
    "             logging.error(f\"Audio file not found for track {track_id} at {audio_path}\")\n",
    "             raise # Or implement skipping logic with collate_fn\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading/processing track {track_id} at {audio_path}: {e}\", exc_info=True)\n",
    "            raise # Or implement skipping logic\n",
    "\n",
    "\n",
    "print(\"FMARawAudioDataset class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "facebook/w2v-bert-2.0\n"
     ]
    }
   ],
   "source": [
    "print(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataCollatorAudio defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3.5: Define Data Collator for Padding (Corrected Padding Logic)\n",
    "\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Union\n",
    "# from transformers.feature_extraction_utils import BatchFeature # Not strictly needed here\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorAudio:\n",
    "    \"\"\"\n",
    "    Data collator that dynamically pads the inputs received Feature Extractor.\n",
    "    Correctly handles padding for [SequenceLength, FeatureDim] tensors.\n",
    "    \"\"\"\n",
    "    padding_value: float = 0.0 # Standard padding for features/audio\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # features is a list of dicts like [{'input_values': tensor1, 'labels': label1, 'attention_mask': mask1}, ...]\n",
    "\n",
    "        # --- Pad 'input_values' (or 'input_features') ---\n",
    "        input_key = 'input_values' if 'input_values' in features[0] else 'input_features'\n",
    "        input_features = [d[input_key] for d in features]\n",
    "\n",
    "        # Determine max sequence length *in this batch* (assuming shape [SeqLen, FeatureDim])\n",
    "        # Add check for empty list\n",
    "        if not input_features:\n",
    "             return {}\n",
    "        max_len = max(feat.shape[0] for feat in input_features) # <<<--- Get length of FIRST dimension\n",
    "\n",
    "        # Pad each feature tensor to max_len along the sequence dimension (first dim)\n",
    "        padded_features = []\n",
    "        for feat in input_features:\n",
    "            # feat shape is [SeqLen, FeatureDim]\n",
    "            num_frames = feat.shape[0]\n",
    "            num_features = feat.shape[1] # Should be consistent (e.g., 160)\n",
    "            pad_width = max_len - num_frames\n",
    "\n",
    "            # Pad argument format for 2D tensor: (pad_left_dim1, pad_right_dim1, pad_left_dim0, pad_right_dim0)\n",
    "            # We only want to pad the end of the sequence dimension (dim 0)\n",
    "            # (0, 0) means no padding on left/right of feature dim (dim 1)\n",
    "            # (0, pad_width) means 0 padding before seq dim (dim 0), pad_width padding after\n",
    "            padded_feat = torch.nn.functional.pad(feat, (0, 0, 0, pad_width), mode='constant', value=self.padding_value)\n",
    "            # Verify shape after padding\n",
    "            # print(f\"Original shape: {feat.shape}, Padded shape: {padded_feat.shape}, Target max_len: {max_len}\")\n",
    "            padded_features.append(padded_feat)\n",
    "\n",
    "        # Stack the padded features into a batch tensor\n",
    "        # Now all tensors in padded_features should have shape [max_len, FeatureDim]\n",
    "        try:\n",
    "             batch_input_features = torch.stack(padded_features) # Shape: [BatchSize, max_len, FeatureDim]\n",
    "        except RuntimeError as e:\n",
    "             logging.error(f\"RuntimeError during torch.stack. Shapes in batch might still differ or be incompatible.\")\n",
    "             # Print shapes for debugging\n",
    "             for i, p_feat in enumerate(padded_features): logging.error(f\" Padded shape {i}: {p_feat.shape}\")\n",
    "             raise e\n",
    "\n",
    "\n",
    "        # --- Prepare Batch Dictionary ---\n",
    "        batch = {\"input_values\": batch_input_features}\n",
    "\n",
    "        # --- Pad 'attention_mask' if present ---\n",
    "        # Attention mask usually has shape [SeqLen]\n",
    "        if \"attention_mask\" in features[0] and features[0][\"attention_mask\"] is not None:\n",
    "            attention_masks = [d[\"attention_mask\"] for d in features]\n",
    "            padded_masks = []\n",
    "            for mask in attention_masks:\n",
    "                 pad_width = max_len - mask.shape[-1] # Pad last dimension (the sequence length)\n",
    "                 # Pad argument format for 1D tensor: (pad_left, pad_right)\n",
    "                 padded_mask = torch.nn.functional.pad(mask, (0, pad_width), mode='constant', value=0) # Pad attention mask with 0\n",
    "                 padded_masks.append(padded_mask)\n",
    "            batch[\"attention_mask\"] = torch.stack(padded_masks) # Shape: [BatchSize, max_len]\n",
    "\n",
    "        # --- Stack Labels ---\n",
    "        labels = [d[\"labels\"] for d in features]\n",
    "        batch[\"labels\"] = torch.stack(labels) # Shape: [BatchSize, num_labels]\n",
    "\n",
    "        return batch\n",
    "\n",
    "# Create an instance of the collator (do this in Cell 4)\n",
    "# data_collator = DataCollatorAudio()\n",
    "# print(\"DataCollatorAudio defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-02 21:54:39,221 - INFO - Loading feature extractor for: facebook/w2v-bert-2.0\n",
      "2025-05-02 21:54:39,419 - INFO - Feature extractor loaded successfully.\n",
      "Feature extractor expects sample rate: 16000\n",
      "2025-05-02 21:54:39,420 - INFO - Initializing FMARawAudioDataset from: /home/zhuoyuan/CSprojects/musicClaGen/data/processed/small_subset_multihot.csv\n",
      "2025-05-02 21:54:39,421 - INFO - Target sampling rate set from feature extractor: 16000 Hz\n",
      "2025-05-02 21:54:39,421 - INFO - Loading manifest from: /home/zhuoyuan/CSprojects/musicClaGen/data/processed/small_subset_multihot.csv\n",
      "2025-05-02 21:54:39,505 - INFO - Loaded and parsed manifest with 8000 entries.\n",
      "2025-05-02 21:54:39,505 - INFO - Example parsed label (first entry): [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "2025-05-02 21:54:39,506 - INFO - Creating DEBUG DataLoaders with small subsets and custom collator...\n",
      "DataCollatorAudio instance created.\n",
      "2025-05-02 21:54:39,510 - INFO - DEBUG Dataset sizes: Train=16, Val=8\n",
      "2025-05-02 21:54:39,510 - INFO - DEBUG DataLoaders with custom collator created.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load Feature Extractor, Create DataLoaders with Custom Collator\n",
    "\n",
    "from transformers import AutoFeatureExtractor # Use the correct class\n",
    "from torch.utils.data import DataLoader, Subset # Ensure Subset is imported\n",
    "# Ensure FMARawAudioDataset and DataCollatorAudio are defined in previous cells\n",
    "\n",
    "# --- Load Feature Extractor ---\n",
    "# (Using model_checkpoint defined in Cell 2)\n",
    "logging.info(f\"Loading feature extractor for: {model_checkpoint}\")\n",
    "try:\n",
    "    # Load the feature extractor associated with Wav2Vec2-BERT\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint)\n",
    "    logging.info(\"Feature extractor loaded successfully.\")\n",
    "    # Log the expected sample rate\n",
    "    processor_sr = feature_extractor.sampling_rate\n",
    "    print(f\"Feature extractor expects sample rate: {processor_sr}\")\n",
    "    # Ensure config matches extractor's expected rate\n",
    "    if config.PREPROCESSING_PARAMS['sample_rate'] != processor_sr:\n",
    "         logging.warning(f\"Config sample rate ({config.PREPROCESSING_PARAMS['sample_rate']}) differs from feature extractor ({processor_sr}). Ensure audio loading uses {processor_sr} Hz.\")\n",
    "         # Update config value if necessary, or ensure Dataset uses processor_sr\n",
    "         # config.PREPROCESSING_PARAMS['sample_rate'] = processor_sr # Be careful modifying config dynamically\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Could not load feature extractor for {model_checkpoint}. Cannot proceed. Error: {e}\", exc_info=True)\n",
    "    raise SystemExit # Stop execution if extractor fails\n",
    "\n",
    "# --- Create Full Dataset ---\n",
    "# Ensure FMARawAudioDataset __init__ accepts feature_extractor\n",
    "try:\n",
    "    # Pass the loaded feature_extractor instance\n",
    "    full_dataset = FMARawAudioDataset(manifest_path, feature_extractor=feature_extractor)\n",
    "    manifest_df = full_dataset.manifest\n",
    "except Exception as e:\n",
    "     logging.error(\"Failed to instantiate FMARawAudioDataset.\", exc_info=True)\n",
    "     raise SystemExit\n",
    "\n",
    "# --- Create SMALLER DEBUG Datasets ---\n",
    "logging.info(\"Creating DEBUG DataLoaders with small subsets and custom collator...\")\n",
    "try:\n",
    "    # Get indices for the splits from the manifest\n",
    "    train_indices = manifest_df[manifest_df['split'] == 'training'].index[:16].tolist() # Small subset for debug\n",
    "    val_indices = manifest_df[manifest_df['split'] == 'validation'].index[:8].tolist()  # Small subset for debug\n",
    "\n",
    "    # Create Subset instances\n",
    "    debug_train_dataset = Subset(full_dataset, train_indices)\n",
    "    debug_val_dataset = Subset(full_dataset, val_indices)\n",
    "\n",
    "    # --- Create Data Collator Instance ---\n",
    "    # (Assumes DataCollatorAudio class is defined in Cell 3.5)\n",
    "    data_collator = DataCollatorAudio()\n",
    "    print(\"DataCollatorAudio instance created.\")\n",
    "\n",
    "    # --- Create DataLoaders using the custom collate_fn ---\n",
    "    debug_train_dataloader = DataLoader(\n",
    "        debug_train_dataset,\n",
    "        batch_size=batch_size, # Use small batch_size from config\n",
    "        shuffle=True,\n",
    "        collate_fn=data_collator # Apply custom padding at batch level\n",
    "        # num_workers=4, # Optional: Add workers later for performance\n",
    "        # pin_memory=True # Optional: Add if using GPU\n",
    "    )\n",
    "    debug_val_dataloader = DataLoader(\n",
    "        debug_val_dataset,\n",
    "        batch_size=batch_size, # Use small batch_size from config\n",
    "        shuffle=False, # No need to shuffle validation data\n",
    "        collate_fn=data_collator # Apply custom padding at batch level\n",
    "        # num_workers=4,\n",
    "        # pin_memory=True\n",
    "    )\n",
    "    logging.info(f\"DEBUG Dataset sizes: Train={len(debug_train_dataset)}, Val={len(debug_val_dataset)}\")\n",
    "    logging.info(\"DEBUG DataLoaders with custom collator created.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to create DEBUG datasets/dataloaders: {e}\", exc_info=True)\n",
    "    raise SystemExit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-02 21:54:39,518 - INFO - Loading pre-trained Wav2Vec2-BERT model: facebook/w2v-bert-2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2BertForSequenceClassification were not initialized from the model checkpoint at facebook/w2v-bert-2.0 and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-02 21:54:40,333 - INFO - Model loaded initially.\n",
      "2025-05-02 21:54:40,334 - INFO - Found classifier attribute 'classifier' of type <class 'torch.nn.modules.linear.Linear'>\n",
      "2025-05-02 21:54:40,334 - INFO - Replacing classifier head 'classifier'. Original out: 22, New out: 22\n",
      "Successfully replaced classifier head 'classifier'.\n",
      "2025-05-02 21:54:45,302 - INFO - Wav2Vec2-BERT Model loaded and moved to device.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Load Wav2Vec2-BERT Model and Modify Head\n",
    "\n",
    "import torch.nn as nn # Ensure nn is imported\n",
    "from transformers import AutoModelForAudioClassification\n",
    "\n",
    "logging.info(f\"Loading pre-trained Wav2Vec2-BERT model: {model_checkpoint}\")\n",
    "try:\n",
    "    # Load the model configured for audio classification\n",
    "    model = AutoModelForAudioClassification.from_pretrained(\n",
    "        model_checkpoint,\n",
    "        num_labels=num_labels,\n",
    "        ignore_mismatched_sizes=True # Essential for replacing the head\n",
    "    )\n",
    "    logging.info(\"Model loaded initially.\")\n",
    "\n",
    "    # --- Explicit Head Replacement (Recommended) ---\n",
    "    # Though I have defined num_labels = num_labels on previous step, I want to explicitly replace it again to ensure the head is correct.\n",
    "    # If the above code is correct, the explicitly approach below might seem redundant but.\n",
    "    \n",
    "    # I MUST verify the correct attribute name for the classifier head for Wav2Vec2-BERT. \n",
    "    # Common names include 'classifier', 'projector','classification_head'. Use print(model) after loading to check.\n",
    "    classifier_attr = 'classifier' # <<<--- VERIFY THIS ATTRIBUTE NAME ---<<<\n",
    "\n",
    "    if hasattr(model, classifier_attr):\n",
    "        original_classifier = getattr(model, classifier_attr)\n",
    "        logging.info(f\"Found classifier attribute '{classifier_attr}' of type {type(original_classifier)}\")\n",
    "\n",
    "        # Check if it's a simple Linear layer or potentially a sequence/projection\n",
    "        if isinstance(original_classifier, nn.Linear):\n",
    "            in_features = original_classifier.in_features\n",
    "            logging.info(f\"Replacing classifier head '{classifier_attr}'. Original out: {original_classifier.out_features}, New out: {num_labels}\")\n",
    "            setattr(model, classifier_attr, nn.Linear(in_features, num_labels))\n",
    "            print(f\"Successfully replaced classifier head '{classifier_attr}'.\")\n",
    "        # Add checks here if Wav2Vec2-BERT uses a different common head structure\n",
    "        # elif isinstance(original_classifier, nn.Sequential): ... etc.\n",
    "        else:\n",
    "             logging.warning(f\"Classifier head '{classifier_attr}' is not nn.Linear ({type(original_classifier)}). Attempting replacement might fail or need adjustment.\")\n",
    "             # If you know the structure (e.g., model.projector + model.classifier), adjust accordingly.\n",
    "             # For now, we assume a direct replacement might work or the implicit loading handled it.\n",
    "\n",
    "    else:\n",
    "         logging.warning(f\"Could not automatically find classifier attribute '{classifier_attr}'. Ensure head size ({num_labels}) was correctly set via 'num_labels' argument during loading or modify manually.\")\n",
    "\n",
    "    model.to(device)\n",
    "    logging.info(\"Wav2Vec2-BERT Model loaded and moved to device.\")\n",
    "    # print(model) # Uncomment this line and run to inspect the model structure and find the classifier name\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to load model '{model_checkpoint}': {e}\", exc_info=True)\n",
    "    raise SystemExit # Stop if model loading fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the correct attribute name for the classifier head for Wav2Vec2-BERT.\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer, Loss, Metrics Functoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-02 21:54:45,359 - INFO - Optimizer AdamW defined with LR=5e-05, Weight Decay=0.01\n",
      "2025-05-02 21:54:45,360 - INFO - Loss function BCEWithLogitsLoss defined.\n",
      "Optimizer, Loss, and compute_metrics function defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Define Optimizer, Loss Function, and Metrics Calculation\n",
    "\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import hamming_loss, jaccard_score, f1_score # Make sure these are imported\n",
    "\n",
    "# --- Optimizer ---\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "logging.info(f\"Optimizer AdamW defined with LR={learning_rate}, Weight Decay={weight_decay}\")\n",
    "\n",
    "# --- Loss Function ---\n",
    "# Use BCEWithLogitsLoss for multi-label classification (includes Sigmoid)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "logging.info(\"Loss function BCEWithLogitsLoss defined.\")\n",
    "\n",
    "# --- Metrics Function ---\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"Calculates multi-label metrics from logits and labels.\"\"\"\n",
    "    logits, labels = eval_preds\n",
    "    # Ensure inputs are numpy arrays on CPU\n",
    "    logits_np = logits.detach().cpu().numpy() if isinstance(logits, torch.Tensor) else logits\n",
    "    labels_np = labels.detach().cpu().numpy() if isinstance(labels, torch.Tensor) else labels\n",
    "\n",
    "    # Apply sigmoid and threshold\n",
    "    probs = 1 / (1 + np.exp(-logits_np)) # Manual sigmoid\n",
    "    preds = (probs > 0.5).astype(int)\n",
    "    labels_np = labels_np.astype(int) # Ensure labels are integers\n",
    "\n",
    "    if labels_np.shape != preds.shape:\n",
    "         logging.error(f\"Shape mismatch in compute_metrics! Labels: {labels_np.shape}, Preds: {preds.shape}\")\n",
    "         # Return default metrics indicating failure\n",
    "         return {'hamming_loss': 1.0, 'jaccard_samples': 0.0, 'f1_micro': 0.0, 'f1_macro': 0.0}\n",
    "\n",
    "    metrics = {}\n",
    "    try:\n",
    "        metrics['hamming_loss'] = hamming_loss(labels_np, preds)\n",
    "        # Use average='samples' for Jaccard in multi-label scenario\n",
    "        metrics['jaccard_samples'] = jaccard_score(labels_np, preds, average='samples', zero_division=0)\n",
    "        metrics['f1_micro'] = f1_score(labels_np, preds, average='micro', zero_division=0)\n",
    "        metrics['f1_macro'] = f1_score(labels_np, preds, average='macro', zero_division=0)\n",
    "        # Optional: Add Accuracy (subset accuracy)\n",
    "        # metrics['accuracy'] = accuracy_score(labels_np, preds) # This is exact match accuracy\n",
    "    except Exception as e:\n",
    "         logging.error(f\"Error calculating metrics: {e}\")\n",
    "         metrics = {'hamming_loss': 1.0, 'jaccard_samples': 0.0, 'f1_micro': 0.0, 'f1_macro': 0.0}\n",
    "\n",
    "    # Log inside the main evaluate function now for better context\n",
    "    # logging.info(f\"Metrics: Hamming={metrics['hamming_loss']:.4f}, Jaccard(samples)={metrics['jaccard_samples']:.4f}, F1 Micro={metrics['f1_micro']:.4f}, F1 Macro={metrics['f1_macro']:.4f}\")\n",
    "    return metrics\n",
    "\n",
    "print(\"Optimizer, Loss, and compute_metrics function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_epoch function defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Define Training Function for One Epoch\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, gradient_accumulation_steps):\n",
    "    model.train() # Set model to training mode\n",
    "    total_loss = 0\n",
    "    num_samples = 0\n",
    "    optimizer.zero_grad() # Zero gradients once before the epoch loop\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        try:\n",
    "            # Prepare model inputs from batch dictionary, move to device\n",
    "            model_inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'} # Exclude labels from model input dict\n",
    "            labels = batch['labels'].to(device) # Keep labels separate\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(**model_inputs)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(logits, labels) # BCEWithLogitsLoss expects float labels\n",
    "\n",
    "            # Scale loss for gradient accumulation\n",
    "            # Check for NaN loss\n",
    "            if torch.isnan(loss):\n",
    "                logging.warning(f\"NaN loss detected at step {step}. Skipping batch.\")\n",
    "                # It's crucial to zero grad even if skipping optimizer step\n",
    "                # but only if accumulation cycle isn't finishing\n",
    "                if (step + 1) % gradient_accumulation_steps != 0:\n",
    "                     # If we don't zero now, subsequent losses in cycle add to NaN\n",
    "                     model.zero_grad() # Or optimizer.zero_grad() if just zeroing grads tracked by it\n",
    "                continue # Skip backward and optimizer step for this batch\n",
    "\n",
    "            scaled_loss = loss / gradient_accumulation_steps\n",
    "            scaled_loss.backward()\n",
    "\n",
    "            # Accumulate total loss correctly (use the original loss value)\n",
    "            # Multiply by batch size in this step for correct averaging later\n",
    "            batch_size_actual = labels.size(0)\n",
    "            total_loss += loss.item() * batch_size_actual\n",
    "            num_samples += batch_size_actual\n",
    "\n",
    "            # Optimizer step (only when accumulated enough steps)\n",
    "            if (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == len(dataloader):\n",
    "                # Optional: Gradient Clipping\n",
    "                # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad() # Zero gradients *after* optimizer step\n",
    "\n",
    "            # Update progress bar description with non-scaled loss\n",
    "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "        except Exception as e:\n",
    "             logging.error(f\"Error during training step {step}, batch keys: {batch.keys()}. Error: {e}\", exc_info=True)\n",
    "             # Optionally add more debugging here (e.g., print shapes)\n",
    "             # Consider skipping batch or stopping training\n",
    "             # Example: check shapes\n",
    "             for k, v in batch.items(): logging.error(f\"Batch item '{k}' shape: {v.shape}\")\n",
    "             continue # Skip this batch on error\n",
    "\n",
    "    # Handle final optimizer step if total steps not divisible by accumulation steps\n",
    "    # This check might be redundant if the optimizer.step() in the loop handles the last step correctly\n",
    "    # if len(dataloader) % gradient_accumulation_steps != 0:\n",
    "    #      optimizer.step()\n",
    "    #      optimizer.zero_grad()\n",
    "\n",
    "    # Calculate average loss over all samples processed\n",
    "    avg_loss = total_loss / num_samples if num_samples > 0 else 0\n",
    "    print(f\"\\nAverage Training Loss for Epoch: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "print(\"train_epoch function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate function defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Define Evaluation Function\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    total_loss = 0\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient calculations\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            try:\n",
    "                # Prepare model inputs from batch dictionary, move to device\n",
    "                model_inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(**model_inputs)\n",
    "                logits = outputs.logits\n",
    "\n",
    "                # Calculate loss\n",
    "                loss = criterion(logits, labels)\n",
    "                total_loss += loss.item() * labels.size(0) # Weighted average by batch size\n",
    "\n",
    "                # Store logits and labels for metric calculation\n",
    "                all_logits.append(logits.cpu())\n",
    "                all_labels.append(labels.cpu())\n",
    "            except Exception as e:\n",
    "                 logging.error(f\"Error during evaluation batch: {e}\", exc_info=True)\n",
    "                 continue # Skip batch on error\n",
    "\n",
    "    if not all_logits or not all_labels:\n",
    "        logging.warning(\"Evaluation yielded no results (possibly all batches failed).\")\n",
    "        return {} # Return empty metrics dictionary\n",
    "\n",
    "    # Calculate average loss over all samples\n",
    "    avg_loss = total_loss / len(dataloader.dataset) if len(dataloader.dataset) > 0 else 0\n",
    "\n",
    "    # Concatenate results from all batches\n",
    "    all_logits_cat = torch.cat(all_logits, dim=0)\n",
    "    all_labels_cat = torch.cat(all_labels, dim=0)\n",
    "\n",
    "    # Calculate metrics using the helper function\n",
    "    eval_preds = (all_logits_cat, all_labels_cat)\n",
    "    metrics = compute_metrics(eval_preds)\n",
    "    metrics['eval_loss'] = avg_loss # Add loss to metrics dict\n",
    "\n",
    "    # Log metrics\n",
    "    print(f\"\\nValidation Loss: {avg_loss:.4f}\")\n",
    "    for name, value in metrics.items():\n",
    "         if name != 'eval_loss': print(f\"  Validation {name.replace('_', ' ').title()}: {value:.4f}\")\n",
    "\n",
    "    return metrics # Return dictionary of all metrics\n",
    "\n",
    "print(\"evaluate function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Debug Training Run for 1 epoch ---\n",
      "\n",
      "--- Debug Epoch 1/1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [1499, 160] at entry 0 and [1498, 160] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Debug Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs_debug\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Run training step for one epoch on the debug training data\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug_train_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Use the SMALL debug dataloader\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Pass grad accum steps\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Run evaluation step on the debug validation data\u001b[39;00m\n\u001b[1;32m     27\u001b[0m eval_metrics \u001b[38;5;241m=\u001b[39m evaluate(\n\u001b[1;32m     28\u001b[0m     model,\n\u001b[1;32m     29\u001b[0m     debug_val_dataloader, \u001b[38;5;66;03m# Use the SMALL debug dataloader\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     criterion,\n\u001b[1;32m     31\u001b[0m     device\n\u001b[1;32m     32\u001b[0m )\n",
      "Cell \u001b[0;32mIn[13], line 10\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, criterion, optimizer, device, gradient_accumulation_steps)\u001b[0m\n\u001b[1;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# Zero gradients once before the epoch loop\u001b[39;00m\n\u001b[1;32m      9\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(progress_bar):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;66;03m# Prepare model inputs from batch dictionary, move to device\u001b[39;00m\n\u001b[1;32m     13\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m} \u001b[38;5;66;03m# Exclude labels from model input dict\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/musicClaGen_env/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/musicClaGen_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/musicClaGen_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/musicClaGen_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 36\u001b[0m, in \u001b[0;36mDataCollatorAudio.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     33\u001b[0m     padded_features\u001b[38;5;241m.\u001b[39mappend(padded_feat)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Stack the padded features into a batch tensor\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m batch_input_features \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadded_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# --- Prepare Batch Dictionary ---\u001b[39;00m\n\u001b[1;32m     39\u001b[0m batch \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_values\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch_input_features} \u001b[38;5;66;03m# Use standard key name\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1499, 160] at entry 0 and [1498, 160] at entry 1"
     ]
    }
   ],
   "source": [
    "# Cell 9: Run ONE Epoch for Debugging\n",
    "\n",
    "from tqdm import tqdm # Ensure tqdm is imported\n",
    "\n",
    "# Ensure model, criterion, optimizer, dataloaders etc. are defined from previous cells\n",
    "print(f\"\\n--- Starting Debug Training Run for {num_epochs_debug} epoch ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Make sure model and criterion are on the correct device\n",
    "model.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "for epoch in range(num_epochs_debug): # num_epochs_debug was set to 1 in Cell 2\n",
    "    print(f\"\\n--- Debug Epoch {epoch+1}/{num_epochs_debug} ---\")\n",
    "\n",
    "    # Run training step for one epoch on the debug training data\n",
    "    train_loss = train_epoch(\n",
    "        model,\n",
    "        debug_train_dataloader, # Use the SMALL debug dataloader\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        device,\n",
    "        gradient_accumulation_steps # Pass grad accum steps\n",
    "    )\n",
    "\n",
    "    # Run evaluation step on the debug validation data\n",
    "    eval_metrics = evaluate(\n",
    "        model,\n",
    "        debug_val_dataloader, # Use the SMALL debug dataloader\n",
    "        criterion,\n",
    "        device\n",
    "    )\n",
    "\n",
    "    print(f\"\\nDebug Epoch {epoch+1} finished.\")\n",
    "    print(f\"  Avg Train Loss: {train_loss:.4f}\")\n",
    "    if eval_metrics:\n",
    "        # Print all collected metrics\n",
    "        for name, value in eval_metrics.items():\n",
    "            print(f\"  Validation {name.replace('_', ' ').title()}: {value:.4f}\")\n",
    "    else:\n",
    "        print(\"  Validation failed to produce metrics.\")\n",
    "\n",
    "    # Optional: Save model after this 1 epoch for inspection\n",
    "    save_path = os.path.join(model_save_dir, f\"wav2vec2bert_debug_epoch_{epoch+1}.pth\") # <<<--- Corrected filename\n",
    "    try:\n",
    "         torch.save(model.state_dict(), save_path)\n",
    "         logging.info(f\"Saved debug model checkpoint to {save_path}\")\n",
    "    except Exception as e:\n",
    "         logging.error(f\"Failed to save debug model checkpoint: {e}\", exc_info=True)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\n--- Debug Run Finished in {end_time - start_time:.2f} seconds ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicClaGen_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
