{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhuoyuan/miniconda3/envs/musicClaGen_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT detected as: /home/zhuoyuan/CSprojects/musicClaGen\n",
      "Adding /home/zhuoyuan/CSprojects/musicClaGen to sys.path\n",
      "/home/zhuoyuan/CSprojects/musicClaGen\n",
      "Imports and basic setup complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "# Use AutoProcessor for Wav2Vec2-BERT - it bundles feature_extractor and tokenizer (if needed)\n",
    "from transformers import AutoModelForAudioClassification, AutoProcessor\n",
    "\n",
    "from torch.optim import AdamW\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import ast # For parsing string representations of lists/arrays\n",
    "import logging\n",
    "import time\n",
    "from sklearn.metrics import hamming_loss, jaccard_score, f1_score # Add more as needed\n",
    "from tqdm.notebook import tqdm # Use notebook version of tqdm\n",
    "import librosa # Needed for loading raw audio now\n",
    "\n",
    "\n",
    "\n",
    "# --- Project Setup ---\n",
    "# Detect if running in notebook or script to adjust path\n",
    "\n",
    "cwd = os.getcwd()\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(cwd, '../../')) # NOTE: remember to change if change the directory structure\n",
    "\n",
    "\n",
    "\n",
    "print(f\"PROJECT_ROOT detected as: {PROJECT_ROOT}\")\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    print(f\"Adding {PROJECT_ROOT} to sys.path\")\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "# --- Config and Utils ---\n",
    "try:\n",
    "    import config # Import your configuration file\n",
    "    # Optionally import utils if needed, e.g., for get_audio_path if not defined here\n",
    "    # import src.utils as utils\n",
    "except ModuleNotFoundError:\n",
    "     print(\"ERROR: Cannot import config or utils. Make sure PROJECT_ROOT is correct and src is importable.\")\n",
    "     # Or add src to path: sys.path.insert(0, os.path.join(PROJECT_ROOT, 'src'))\n",
    "     # import config\n",
    "     # import utils\n",
    "\n",
    "\n",
    "# --- Setup Logging ---\n",
    "for handler in logging.root.handlers[:]: logging.root.removeHandler(handler) # Clear previous\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    handlers=[logging.StreamHandler(sys.stdout)])\n",
    "\n",
    "print(\"Imports and basic setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-02 22:09:34,766 - INFO - Loaded 22 unified genres from /home/zhuoyuan/CSprojects/musicClaGen/data/processed/unified_genres.txt\n",
      "2025-05-02 22:09:34,767 - INFO - Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Cell 2\n",
    "# --- Load Config ---\n",
    "# Ensure config.py has the correct paths in the PATHS dict\n",
    "manifest_path = config.PATHS.get('SMALL_MULTILABEL_PATH', os.path.join(config.PATHS['PROCESSED_DATA_DIR'], 'small_subset_multihot.csv')) # Use .get for safety\n",
    "genre_list_path = config.PATHS.get('GENRE_LIST_PATH', os.path.join(config.PATHS['PROCESSED_DATA_DIR'], 'unified_genres.txt'))\n",
    "model_save_dir = config.PATHS['MODELS_DIR']\n",
    "\n",
    "# Ensure config.py has MODEL_PARAMS dict with model_checkpoint\n",
    "model_checkpoint = config.MODEL_PARAMS['model_checkpoint'] # e.g., \"facebook/w2v-bert-2.0\" - VERIFY!\n",
    "learning_rate = config.MODEL_PARAMS['learning_rate']\n",
    "batch_size = config.MODEL_PARAMS['batch_size'] # Use the small BS for notebook test\n",
    "num_epochs_debug = 1 # <<<--- RUN ONLY 1 EPOCH FOR DEBUGGING ---<<<\n",
    "weight_decay = config.MODEL_PARAMS['weight_decay']\n",
    "gradient_accumulation_steps = config.MODEL_PARAMS['gradient_accumulation_steps']\n",
    "\n",
    "# --- Load unified genre list ---\n",
    "try:\n",
    "    with open(genre_list_path, 'r') as f:\n",
    "        unified_genres = [line.strip() for line in f if line.strip()]\n",
    "    num_labels = len(unified_genres) # should be the number of labels defined in the unified_genres.txt file, in this case it should be 22.\n",
    "    logging.info(f\"Loaded {num_labels} unified genres from {genre_list_path}\")\n",
    "    if num_labels == 0: raise ValueError(\"Genre list is empty!\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to load or process unified genre list: {e}\", exc_info=True)\n",
    "    raise SystemExit(\"Cannot proceed without genre list.\")\n",
    "\n",
    "# --- Setup Device ---\n",
    "device = torch.device(config.DEVICE if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(f\"Using device: {device}\")\n",
    "if not torch.cuda.is_available() and config.DEVICE==\"cuda\":\n",
    "     logging.warning(\"CUDA selected but not available, falling back to CPU.\")\n",
    "\n",
    "# --- Create Save Directory ---\n",
    "os.makedirs(model_save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zhuoyuan/CSprojects/musicClaGen/data/processed/small_subset_multihot.csv\n"
     ]
    }
   ],
   "source": [
    "print(manifest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FMARawAudioDataset class defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Dataset Class Definition (Raw Audio Version)\n",
    "\n",
    "\n",
    "\n",
    "# Define(recollect)the regex parser from preprocess.py if needed, \n",
    "# otherwise use ast.literal_eval---\n",
    "\n",
    "import re\n",
    "\n",
    "def parse_numpy_array_string(array_str):\n",
    "    \"\"\"\n",
    "    Parse strings like '[np.float32(1.0), np.float32(0.0), ...]' into a list of integers.\n",
    "    This is needed because ast.literal_eval cannot handle 'np.float32()' in the string.\n",
    "    \"\"\"\n",
    "    if not isinstance(array_str, str):\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Extract all the float values using regular expressions\n",
    "        float_matches = re.findall(r'np\\.float32\\((\\d+\\.\\d+)\\)', array_str)\n",
    "        \n",
    "        # Convert matches to integers (1.0 -> 1, 0.0 -> 0)\n",
    "        values = []\n",
    "        for match in float_matches:\n",
    "            value = float(match)\n",
    "            # Convert to integer if it's 0.0 or 1.0\n",
    "            if value == 1.0:\n",
    "                values.append(1)\n",
    "            elif value == 0.0:\n",
    "                values.append(0)\n",
    "            else:\n",
    "                values.append(value)  # Keep as float if not 0 or 1\n",
    "                \n",
    "        return values\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error parsing array string: {e}\")\n",
    "        return []\n",
    "\n",
    "class FMARawAudioDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads raw audio waveforms and labels from manifest, uses Hugging Face\n",
    "    feature extractor (like ASTFeatureExtractor or Wav2Vec2Processor) on the fly.\n",
    "    \"\"\"\n",
    "    def __init__(self, manifest_path, feature_extractor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            manifest_path (str): Path to the final manifest CSV file.\n",
    "            feature_extractor: Initialized Hugging Face AutoFeatureExtractor or AutoProcessor.\n",
    "        \"\"\"\n",
    "        logging.info(f\"Initializing FMARawAudioDataset from: {manifest_path}\")\n",
    "        if feature_extractor is None:\n",
    "             raise ValueError(\"FMARawAudioDataset requires a feature_extractor/processor instance.\")\n",
    "\n",
    "        self.feature_extractor = feature_extractor\n",
    "        # Get target sampling rate directly from the extractor/processor\n",
    "        try:\n",
    "             # Works for Wav2Vec2Processor, ASTFeatureExtractor, etc.\n",
    "             self.target_sr = self.feature_extractor.sampling_rate\n",
    "             logging.info(f\"Target sampling rate set from feature extractor: {self.target_sr} Hz\")\n",
    "        except AttributeError:\n",
    "             logging.warning(\"Could not get sampling_rate from feature_extractor, using config.\")\n",
    "             # Fallback to config if needed, but ensuring match is crucial\n",
    "             self.target_sr = config.PREPROCESSING_PARAMS['sample_rate']\n",
    "\n",
    "\n",
    "        logging.info(f\"Loading manifest from: {manifest_path}\")\n",
    "        try:\n",
    "            self.manifest = pd.read_csv(manifest_path)\n",
    "            # Ensure index is set if needed elsewhere, or use default range index\n",
    "            if 'track_id' in self.manifest.columns:\n",
    "                 self.manifest = self.manifest.set_index('track_id', drop=False)\n",
    "\n",
    "            # --- Parse the 'multi_hot_label' string back into a list ---\n",
    "            # Here: if we decide to use raw audio, we use regex parser; \n",
    "            #       if we decide to use mel spectrogram, we use ast.literal_eval\n",
    "\n",
    "            # Choose the correct parser based on how labels were saved in the CSV\n",
    "            # If saved as '[1.0, 0.0,...]' use ast.literal_eval\n",
    "            # label_parser = ast.literal_eval\n",
    "            # If saved as '[np.float32(1.0)...]' uncomment and use regex parser\n",
    "            label_parser = parse_numpy_array_string\n",
    "\n",
    "            self.manifest['multi_hot_label'] = self.manifest['multi_hot_label'].apply(label_parser)\n",
    "            logging.info(f\"Loaded and parsed manifest with {len(self.manifest)} entries.\")\n",
    "            # Check the first parsed label\n",
    "            logging.info(f\"Example parsed label (first entry): {self.manifest['multi_hot_label'].iloc[0]}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading or parsing manifest {manifest_path}: {e}\", exc_info=True)\n",
    "            raise\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.manifest)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Loads raw audio for index idx, processes it with the feature extractor,\n",
    "        and returns the processed inputs and labels.\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx): idx = idx.tolist() # Handle tensor indices\n",
    "\n",
    "        # Get the row data from the manifest\n",
    "        row = self.manifest.iloc[idx]\n",
    "        track_id = row.get('track_id', self.manifest.index[idx]) # Get track_id safely\n",
    "        label_vector = row['multi_hot_label'] # Already parsed list/array\n",
    "\n",
    "        # Construct absolute audio path if necessary\n",
    "        audio_path = row['audio_path']\n",
    "\n",
    "        #NOTE: originally, the mel-spectrogram's path is relative  but the raw audio's path is absolute, so we need to make sure the audio_path is absolute\n",
    "        # So we are check if the audio_path is absolute or relative in case we load the wrong data, if it's relative, we need to join it with the PROJECT_ROOT\n",
    "        if not os.path.isabs(audio_path):\n",
    "             # Assumes path in manifest is relative to PROJECT_ROOT\n",
    "             audio_path = os.path.join(config.PROJECT_ROOT, audio_path)\n",
    "\n",
    "        try:\n",
    "            # --- 1. Load RAW Audio Waveform ---\n",
    "            # Load full 30s clip at the TARGET sample rate required by the processor\n",
    "            waveform, loaded_sr = librosa.load(\n",
    "                audio_path,\n",
    "                sr=self.target_sr, # Use processor's sampling rate\n",
    "                duration=30.0     # Load the full 30 seconds\n",
    "            )\n",
    "            # Ensure minimum length if needed (though duration should handle it)\n",
    "            min_samples = int(0.1 * self.target_sr) # Example: require at least 0.1s\n",
    "            if len(waveform) < min_samples:\n",
    "                 raise ValueError(f\"Audio signal for track {track_id} too short after loading.\")\n",
    "\n",
    "            # --- 2. Apply Feature Extractor ---\n",
    "            # Pass the raw waveform numpy array\n",
    "            # The extractor handles normalization, padding/truncation, tensor conversion\n",
    "            \n",
    "            max_length = 5000\n",
    "\n",
    "            inputs = self.feature_extractor(\n",
    "                waveform,\n",
    "                sampling_rate=self.target_sr,\n",
    "                return_tensors=\"pt\",\n",
    "                return_attention_mask=True # Request attention mask\n",
    "            )\n",
    "\n",
    "            # --- 3. Prepare Outputs ---\n",
    "            # Squeeze unnecessary batch dimension added by the extractor\n",
    "            # Key name ('input_values', 'input_features') depends on the specific extractor\n",
    "            feature_tensor = inputs.get('input_values', inputs.get('input_features'))\n",
    "            if feature_tensor is None:\n",
    "                raise KeyError(\"Expected 'input_values' or 'input_features' key from feature_extractor output.\")\n",
    "            feature_tensor = feature_tensor.squeeze(0) # Remove batch dim -> [Channels?, Freq?, Time] or [SeqLen, Dim]\n",
    "\n",
    "            attention_mask = inputs.get('attention_mask', None)\n",
    "            if attention_mask is not None:\n",
    "                 attention_mask = attention_mask.squeeze(0)\n",
    "\n",
    "            # Convert label list/array to float tensor for BCE loss\n",
    "            label_tensor = torch.tensor(label_vector, dtype=torch.float32)\n",
    "\n",
    "            # Return dictionary matching model's expected input names\n",
    "            model_input_dict = {\"labels\": label_tensor}\n",
    "            # Use the key the feature extractor provided\n",
    "            if 'input_values' in inputs:\n",
    "                 model_input_dict['input_values'] = feature_tensor\n",
    "            elif 'input_features' in inputs:\n",
    "                 model_input_dict['input_features'] = feature_tensor\n",
    "\n",
    "            if attention_mask is not None:\n",
    "                 model_input_dict['attention_mask'] = attention_mask\n",
    "\n",
    "            return model_input_dict\n",
    "\n",
    "        except FileNotFoundError:\n",
    "             logging.error(f\"Audio file not found for track {track_id} at {audio_path}\")\n",
    "             raise # Or implement skipping logic with collate_fn\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading/processing track {track_id} at {audio_path}: {e}\", exc_info=True)\n",
    "            raise # Or implement skipping logic\n",
    "\n",
    "\n",
    "print(\"FMARawAudioDataset class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "facebook/w2v-bert-2.0\n"
     ]
    }
   ],
   "source": [
    "print(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3.5: Define Data Collator for Padding (Corrected Padding Logic)\n",
    "\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Union\n",
    "# from transformers.feature_extraction_utils import BatchFeature # Not strictly needed here\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorAudio:\n",
    "    \"\"\"\n",
    "    Data collator that dynamically pads the inputs received Feature Extractor.\n",
    "    Correctly handles padding for [SequenceLength, FeatureDim] tensors.\n",
    "    \"\"\"\n",
    "    padding_value: float = 0.0 # Standard padding for features/audio\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # features is a list of dicts like [{'input_values': tensor1, 'labels': label1, 'attention_mask': mask1}, ...]\n",
    "\n",
    "        # --- Pad 'input_values' (or 'input_features') ---\n",
    "        input_key = 'input_values' if 'input_values' in features[0] else 'input_features'\n",
    "        input_features = [d[input_key] for d in features]\n",
    "\n",
    "        # Determine max sequence length *in this batch* (assuming shape [SeqLen, FeatureDim])\n",
    "        # Add check for empty list\n",
    "        if not input_features:\n",
    "             return {}\n",
    "        max_len = max(feat.shape[0] for feat in input_features) # <<<--- Get length of FIRST dimension\n",
    "\n",
    "        # Pad each feature tensor to max_len along the sequence dimension (first dim)\n",
    "        padded_features = []\n",
    "        for feat in input_features:\n",
    "            # feat shape is [SeqLen, FeatureDim]\n",
    "            num_frames = feat.shape[0]\n",
    "            num_features = feat.shape[1] # Should be consistent (e.g., 160)\n",
    "            pad_width = max_len - num_frames\n",
    "\n",
    "            # Pad argument format for 2D tensor: (pad_left_dim1, pad_right_dim1, pad_left_dim0, pad_right_dim0)\n",
    "            # We only want to pad the end of the sequence dimension (dim 0)\n",
    "            # (0, 0) means no padding on left/right of feature dim (dim 1)\n",
    "            # (0, pad_width) means 0 padding before seq dim (dim 0), pad_width padding after\n",
    "            padded_feat = torch.nn.functional.pad(feat, (0, 0, 0, pad_width), mode='constant', value=self.padding_value)\n",
    "            # Verify shape after padding\n",
    "            # print(f\"Original shape: {feat.shape}, Padded shape: {padded_feat.shape}, Target max_len: {max_len}\")\n",
    "            padded_features.append(padded_feat)\n",
    "\n",
    "        # Stack the padded features into a batch tensor\n",
    "        # Now all tensors in padded_features should have shape [max_len, FeatureDim]\n",
    "        try:\n",
    "             batch_input_features = torch.stack(padded_features) # Shape: [BatchSize, max_len, FeatureDim]\n",
    "        except RuntimeError as e:\n",
    "             logging.error(f\"RuntimeError during torch.stack. Shapes in batch might still differ or be incompatible.\")\n",
    "             # Print shapes for debugging\n",
    "             for i, p_feat in enumerate(padded_features): logging.error(f\" Padded shape {i}: {p_feat.shape}\")\n",
    "             raise e\n",
    "\n",
    "\n",
    "        # --- Prepare Batch Dictionary ---\n",
    "        batch = {\"input_values\": batch_input_features}\n",
    "\n",
    "        # --- Pad 'attention_mask' if present ---\n",
    "        # Attention mask usually has shape [SeqLen]\n",
    "        if \"attention_mask\" in features[0] and features[0][\"attention_mask\"] is not None:\n",
    "            attention_masks = [d[\"attention_mask\"] for d in features]\n",
    "            padded_masks = []\n",
    "            for mask in attention_masks:\n",
    "                 pad_width = max_len - mask.shape[-1] # Pad last dimension (the sequence length)\n",
    "                 # Pad argument format for 1D tensor: (pad_left, pad_right)\n",
    "                 padded_mask = torch.nn.functional.pad(mask, (0, pad_width), mode='constant', value=0) # Pad attention mask with 0\n",
    "                 padded_masks.append(padded_mask)\n",
    "            batch[\"attention_mask\"] = torch.stack(padded_masks) # Shape: [BatchSize, max_len]\n",
    "\n",
    "        # --- Stack Labels ---\n",
    "        labels = [d[\"labels\"] for d in features]\n",
    "        batch[\"labels\"] = torch.stack(labels) # Shape: [BatchSize, num_labels]\n",
    "\n",
    "        return batch\n",
    "\n",
    "# Create an instance of the collator (do this in Cell 4)\n",
    "# data_collator = DataCollatorAudio()\n",
    "# print(\"DataCollatorAudio defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-02 22:09:34,826 - INFO - Loading feature extractor for: facebook/w2v-bert-2.0\n",
      "2025-05-02 22:09:34,968 - INFO - Feature extractor loaded successfully.\n",
      "Feature extractor expects sample rate: 16000\n",
      "2025-05-02 22:09:34,969 - INFO - Initializing FMARawAudioDataset from: /home/zhuoyuan/CSprojects/musicClaGen/data/processed/small_subset_multihot.csv\n",
      "2025-05-02 22:09:34,969 - INFO - Target sampling rate set from feature extractor: 16000 Hz\n",
      "2025-05-02 22:09:34,969 - INFO - Loading manifest from: /home/zhuoyuan/CSprojects/musicClaGen/data/processed/small_subset_multihot.csv\n",
      "2025-05-02 22:09:35,036 - INFO - Loaded and parsed manifest with 8000 entries.\n",
      "2025-05-02 22:09:35,037 - INFO - Example parsed label (first entry): [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "2025-05-02 22:09:35,037 - INFO - Creating DEBUG DataLoaders with small subsets and custom collator...\n",
      "DataCollatorAudio instance created.\n",
      "2025-05-02 22:09:35,041 - INFO - DEBUG Dataset sizes: Train=16, Val=8\n",
      "2025-05-02 22:09:35,042 - INFO - DEBUG DataLoaders with custom collator created.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load Feature Extractor, Create DataLoaders with Custom Collator\n",
    "\n",
    "from transformers import AutoFeatureExtractor # Use the correct class\n",
    "from torch.utils.data import DataLoader, Subset # Ensure Subset is imported\n",
    "# Ensure FMARawAudioDataset and DataCollatorAudio are defined in previous cells\n",
    "\n",
    "# --- Load Feature Extractor ---\n",
    "# (Using model_checkpoint defined in Cell 2)\n",
    "logging.info(f\"Loading feature extractor for: {model_checkpoint}\")\n",
    "try:\n",
    "    # Load the feature extractor associated with Wav2Vec2-BERT\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint)\n",
    "    logging.info(\"Feature extractor loaded successfully.\")\n",
    "    # Log the expected sample rate\n",
    "    processor_sr = feature_extractor.sampling_rate\n",
    "    print(f\"Feature extractor expects sample rate: {processor_sr}\")\n",
    "    # Ensure config matches extractor's expected rate\n",
    "    if config.PREPROCESSING_PARAMS['sample_rate'] != processor_sr:\n",
    "         logging.warning(f\"Config sample rate ({config.PREPROCESSING_PARAMS['sample_rate']}) differs from feature extractor ({processor_sr}). Ensure audio loading uses {processor_sr} Hz.\")\n",
    "         # Update config value if necessary, or ensure Dataset uses processor_sr\n",
    "         # config.PREPROCESSING_PARAMS['sample_rate'] = processor_sr # Be careful modifying config dynamically\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Could not load feature extractor for {model_checkpoint}. Cannot proceed. Error: {e}\", exc_info=True)\n",
    "    raise SystemExit # Stop execution if extractor fails\n",
    "\n",
    "# --- Create Full Dataset ---\n",
    "# Ensure FMARawAudioDataset __init__ accepts feature_extractor\n",
    "try:\n",
    "    # Pass the loaded feature_extractor instance\n",
    "    full_dataset = FMARawAudioDataset(manifest_path, feature_extractor=feature_extractor)\n",
    "    manifest_df = full_dataset.manifest\n",
    "except Exception as e:\n",
    "     logging.error(\"Failed to instantiate FMARawAudioDataset.\", exc_info=True)\n",
    "     raise SystemExit\n",
    "\n",
    "# --- Create SMALLER DEBUG Datasets ---\n",
    "logging.info(\"Creating DEBUG DataLoaders with small subsets and custom collator...\")\n",
    "try:\n",
    "    # Get indices for the splits from the manifest\n",
    "    train_indices = manifest_df[manifest_df['split'] == 'training'].index[:16].tolist() # Small subset for debug\n",
    "    val_indices = manifest_df[manifest_df['split'] == 'validation'].index[:8].tolist()  # Small subset for debug\n",
    "\n",
    "    # Create Subset instances\n",
    "    debug_train_dataset = Subset(full_dataset, train_indices)\n",
    "    debug_val_dataset = Subset(full_dataset, val_indices)\n",
    "\n",
    "    # --- Create Data Collator Instance ---\n",
    "    # (Assumes DataCollatorAudio class is defined in Cell 3.5)\n",
    "    data_collator = DataCollatorAudio()\n",
    "    print(\"DataCollatorAudio instance created.\")\n",
    "\n",
    "    # --- Create DataLoaders using the custom collate_fn ---\n",
    "    debug_train_dataloader = DataLoader(\n",
    "        debug_train_dataset,\n",
    "        batch_size=batch_size, # Use small batch_size from config\n",
    "        shuffle=True,\n",
    "        collate_fn=data_collator # Apply custom padding at batch level\n",
    "        # num_workers=4, # Optional: Add workers later for performance\n",
    "        # pin_memory=True # Optional: Add if using GPU\n",
    "    )\n",
    "    debug_val_dataloader = DataLoader(\n",
    "        debug_val_dataset,\n",
    "        batch_size=batch_size, # Use small batch_size from config\n",
    "        shuffle=False, # No need to shuffle validation data\n",
    "        collate_fn=data_collator # Apply custom padding at batch level\n",
    "        # num_workers=4,\n",
    "        # pin_memory=True\n",
    "    )\n",
    "    logging.info(f\"DEBUG Dataset sizes: Train={len(debug_train_dataset)}, Val={len(debug_val_dataset)}\")\n",
    "    logging.info(\"DEBUG DataLoaders with custom collator created.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to create DEBUG datasets/dataloaders: {e}\", exc_info=True)\n",
    "    raise SystemExit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inspect' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43minspect\u001b[49m\u001b[38;5;241m.\u001b[39msignature(model\u001b[38;5;241m.\u001b[39mforward))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inspect' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-02 22:09:35,051 - INFO - Loading pre-trained Wav2Vec2-BERT model: facebook/w2v-bert-2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2BertForSequenceClassification were not initialized from the model checkpoint at facebook/w2v-bert-2.0 and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-02 22:09:35,830 - INFO - Model loaded initially.\n",
      "2025-05-02 22:09:35,831 - INFO - Found classifier attribute 'classifier' of type <class 'torch.nn.modules.linear.Linear'>\n",
      "2025-05-02 22:09:35,832 - INFO - Replacing classifier head 'classifier'. Original out: 22, New out: 22\n",
      "Successfully replaced classifier head 'classifier'.\n",
      "2025-05-02 22:09:39,498 - INFO - Wav2Vec2-BERT Model loaded and moved to device.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Load Wav2Vec2-BERT Model and Modify Head\n",
    "\n",
    "import torch.nn as nn # Ensure nn is imported\n",
    "from transformers import AutoModelForAudioClassification\n",
    "\n",
    "logging.info(f\"Loading pre-trained Wav2Vec2-BERT model: {model_checkpoint}\")\n",
    "try:\n",
    "    # Load the model configured for audio classification\n",
    "    model = AutoModelForAudioClassification.from_pretrained(\n",
    "        model_checkpoint,\n",
    "        num_labels=num_labels,\n",
    "        ignore_mismatched_sizes=True # Essential for replacing the head\n",
    "    )\n",
    "    logging.info(\"Model loaded initially.\")\n",
    "\n",
    "    # --- Explicit Head Replacement (Recommended) ---\n",
    "    # Though I have defined num_labels = num_labels on previous step, I want to explicitly replace it again to ensure the head is correct.\n",
    "    # If the above code is correct, the explicitly approach below might seem redundant but.\n",
    "    \n",
    "    # I MUST verify the correct attribute name for the classifier head for Wav2Vec2-BERT. \n",
    "    # Common names include 'classifier', 'projector','classification_head'. Use print(model) after loading to check.\n",
    "    classifier_attr = 'classifier' # <<<--- VERIFY THIS ATTRIBUTE NAME ---<<<\n",
    "\n",
    "    if hasattr(model, classifier_attr):\n",
    "        original_classifier = getattr(model, classifier_attr)\n",
    "        logging.info(f\"Found classifier attribute '{classifier_attr}' of type {type(original_classifier)}\")\n",
    "\n",
    "        # Check if it's a simple Linear layer or potentially a sequence/projection\n",
    "        if isinstance(original_classifier, nn.Linear):\n",
    "            in_features = original_classifier.in_features\n",
    "            logging.info(f\"Replacing classifier head '{classifier_attr}'. Original out: {original_classifier.out_features}, New out: {num_labels}\")\n",
    "            setattr(model, classifier_attr, nn.Linear(in_features, num_labels))\n",
    "            print(f\"Successfully replaced classifier head '{classifier_attr}'.\")\n",
    "        # Add checks here if Wav2Vec2-BERT uses a different common head structure\n",
    "        # elif isinstance(original_classifier, nn.Sequential): ... etc.\n",
    "        else:\n",
    "             logging.warning(f\"Classifier head '{classifier_attr}' is not nn.Linear ({type(original_classifier)}). Attempting replacement might fail or need adjustment.\")\n",
    "             # If you know the structure (e.g., model.projector + model.classifier), adjust accordingly.\n",
    "             # For now, we assume a direct replacement might work or the implicit loading handled it.\n",
    "\n",
    "    else:\n",
    "         logging.warning(f\"Could not automatically find classifier attribute '{classifier_attr}'. Ensure head size ({num_labels}) was correctly set via 'num_labels' argument during loading or modify manually.\")\n",
    "\n",
    "    model.to(device)\n",
    "    logging.info(\"Wav2Vec2-BERT Model loaded and moved to device.\")\n",
    "    # print(model) # Uncomment this line and run to inspect the model structure and find the classifier name\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to load model '{model_checkpoint}': {e}\", exc_info=True)\n",
    "    raise SystemExit # Stop if model loading fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the correct attribute name for the classifier head for Wav2Vec2-BERT.\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer, Loss, Metrics Functoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-02 22:09:39,525 - INFO - Optimizer AdamW defined with LR=5e-05, Weight Decay=0.01\n",
      "2025-05-02 22:09:39,527 - INFO - Loss function BCEWithLogitsLoss defined.\n",
      "Optimizer, Loss, and compute_metrics function defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Define Optimizer, Loss Function, and Metrics Calculation\n",
    "\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import hamming_loss, jaccard_score, f1_score # Make sure these are imported\n",
    "\n",
    "# --- Optimizer ---\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "logging.info(f\"Optimizer AdamW defined with LR={learning_rate}, Weight Decay={weight_decay}\")\n",
    "\n",
    "# --- Loss Function ---\n",
    "# Use BCEWithLogitsLoss for multi-label classification (includes Sigmoid)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "logging.info(\"Loss function BCEWithLogitsLoss defined.\")\n",
    "\n",
    "# --- Metrics Function ---\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"Calculates multi-label metrics from logits and labels.\"\"\"\n",
    "    logits, labels = eval_preds\n",
    "    # Ensure inputs are numpy arrays on CPU\n",
    "    logits_np = logits.detach().cpu().numpy() if isinstance(logits, torch.Tensor) else logits\n",
    "    labels_np = labels.detach().cpu().numpy() if isinstance(labels, torch.Tensor) else labels\n",
    "\n",
    "    # Apply sigmoid and threshold\n",
    "    probs = 1 / (1 + np.exp(-logits_np)) # Manual sigmoid\n",
    "    preds = (probs > 0.5).astype(int)\n",
    "    labels_np = labels_np.astype(int) # Ensure labels are integers\n",
    "\n",
    "    if labels_np.shape != preds.shape:\n",
    "         logging.error(f\"Shape mismatch in compute_metrics! Labels: {labels_np.shape}, Preds: {preds.shape}\")\n",
    "         # Return default metrics indicating failure\n",
    "         return {'hamming_loss': 1.0, 'jaccard_samples': 0.0, 'f1_micro': 0.0, 'f1_macro': 0.0}\n",
    "\n",
    "    metrics = {}\n",
    "    try:\n",
    "        metrics['hamming_loss'] = hamming_loss(labels_np, preds)\n",
    "        # Use average='samples' for Jaccard in multi-label scenario\n",
    "        metrics['jaccard_samples'] = jaccard_score(labels_np, preds, average='samples', zero_division=0)\n",
    "        metrics['f1_micro'] = f1_score(labels_np, preds, average='micro', zero_division=0)\n",
    "        metrics['f1_macro'] = f1_score(labels_np, preds, average='macro', zero_division=0)\n",
    "        # Optional: Add Accuracy (subset accuracy)\n",
    "        # metrics['accuracy'] = accuracy_score(labels_np, preds) # This is exact match accuracy\n",
    "    except Exception as e:\n",
    "         logging.error(f\"Error calculating metrics: {e}\")\n",
    "         metrics = {'hamming_loss': 1.0, 'jaccard_samples': 0.0, 'f1_micro': 0.0, 'f1_macro': 0.0}\n",
    "\n",
    "    # Log inside the main evaluate function now for better context\n",
    "    # logging.info(f\"Metrics: Hamming={metrics['hamming_loss']:.4f}, Jaccard(samples)={metrics['jaccard_samples']:.4f}, F1 Micro={metrics['f1_micro']:.4f}, F1 Macro={metrics['f1_macro']:.4f}\")\n",
    "    return metrics\n",
    "\n",
    "print(\"Optimizer, Loss, and compute_metrics function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_epoch function defined with added logging.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Define Training Function for One Epoch (Corrected Model Input)\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, gradient_accumulation_steps):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_samples = 0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        try:\n",
    "            # --- CORRECTED INPUT PREPARATION ---\n",
    "            # Explicitly use the keys the model expects.\n",
    "            # Assume the feature extractor output key is 'input_values'\n",
    "            # Assume the model's forward method expects 'input_features'\n",
    "            # You MUST verify 'input_features' is the correct key for Wav2Vec2BertForSequenceClassification\n",
    "            expected_model_input_key = \"input_features\" # <<<--- VERIFY THIS KEY NAME\n",
    "\n",
    "            if 'input_values' not in batch: # Check if extractor output key is different\n",
    "                 raise KeyError(\"Batch dictionary missing 'input_values' from Dataset/Extractor.\")\n",
    "\n",
    "            # Build the dictionary for the model's forward pass\n",
    "            model_inputs = {\n",
    "                expected_model_input_key: batch['input_values'].to(device) # Map dataset output key to model input key\n",
    "            }\n",
    "            if 'attention_mask' in batch and batch['attention_mask'] is not None:\n",
    "                 model_inputs['attention_mask'] = batch['attention_mask'].to(device)\n",
    "            # --- END CORRECTION ---\n",
    "\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(**model_inputs) # Pass the correctly named arguments\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            # ... (rest of loss scaling, backward, optimizer step remains the same) ...\n",
    "            if torch.isnan(loss):\n",
    "                logging.warning(f\"NaN loss detected at step {step}. Skipping batch.\")\n",
    "                if (step + 1) % gradient_accumulation_steps != 0: model.zero_grad()\n",
    "                continue\n",
    "            scaled_loss = loss / gradient_accumulation_steps\n",
    "            scaled_loss.backward()\n",
    "            batch_size_actual = labels.size(0)\n",
    "            total_loss += loss.item() * batch_size_actual\n",
    "            num_samples += batch_size_actual\n",
    "            if (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == len(dataloader):\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "        except Exception as e:\n",
    "             logging.error(f\"Error during training step {step}, batch keys: {batch.keys()}. Error: {e}\", exc_info=True)\n",
    "             continue\n",
    "\n",
    "    if (step + 1) % gradient_accumulation_steps != 0 and num_samples > 0: # Ensure step was defined\n",
    "         optimizer.step(); optimizer.zero_grad()\n",
    "\n",
    "    avg_loss = total_loss / num_samples if num_samples > 0 else 0\n",
    "    print(f\"\\nAverage Training Loss for Epoch: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "print(\"train_epoch function updated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate function defined with added logging.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Define Evaluation Function (Corrected Model Input)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(tqdm(dataloader, desc=\"Evaluating\", leave=False)):\n",
    "            try:\n",
    "                # --- CORRECTED INPUT PREPARATION ---\n",
    "                expected_model_input_key = \"input_features\" # <<<--- VERIFY THIS KEY NAME\n",
    "\n",
    "                if 'input_values' not in batch:\n",
    "                     raise KeyError(\"Batch dictionary missing 'input_values' from Dataset/Extractor.\")\n",
    "\n",
    "                model_inputs = {\n",
    "                    expected_model_input_key: batch['input_values'].to(device)\n",
    "                }\n",
    "                if 'attention_mask' in batch and batch['attention_mask'] is not None:\n",
    "                     model_inputs['attention_mask'] = batch['attention_mask'].to(device)\n",
    "                # --- END CORRECTION ---\n",
    "\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(**model_inputs) # Pass the correctly named arguments\n",
    "                logits = outputs.logits\n",
    "\n",
    "                # Calculate loss\n",
    "                loss = criterion(logits, labels)\n",
    "                total_loss += loss.item() * labels.size(0)\n",
    "                num_samples += labels.size(0)\n",
    "\n",
    "                all_logits.append(logits.cpu())\n",
    "                all_labels.append(labels.cpu())\n",
    "            except Exception as e:\n",
    "                 logging.error(f\"Error during evaluation step {step}, batch keys: {batch.keys()}. Error: {e}\", exc_info=True)\n",
    "                 continue # Skip batch\n",
    "\n",
    "    if not all_logits or not all_labels or num_samples == 0:\n",
    "        logging.warning(\"Evaluation yielded no results (all batches failed or empty dataloader?).\")\n",
    "        return {}\n",
    "\n",
    "    avg_loss = total_loss / num_samples\n",
    "\n",
    "    all_logits_cat = torch.cat(all_logits, dim=0)\n",
    "    all_labels_cat = torch.cat(all_labels, dim=0)\n",
    "\n",
    "    eval_preds = (all_logits_cat, all_labels_cat)\n",
    "    metrics = compute_metrics(eval_preds)\n",
    "    metrics['eval_loss'] = avg_loss\n",
    "\n",
    "    print(f\"\\nValidation Loss: {avg_loss:.4f}\")\n",
    "    for name, value in metrics.items():\n",
    "         if name != 'eval_loss': print(f\"  Validation {name.replace('_', ' ').title()}: {value:.4f}\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "print(\"evaluate function updated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Debug Training Run for 1 epoch ---\n",
      "\n",
      "--- Debug Epoch 1/1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-02 22:09:40,966 - INFO - Train Batch Input 'input_values' shape: torch.Size([2, 1498, 160])\n",
      "2025-05-02 22:09:40,974 - INFO - Train Batch Input 'attention_mask' shape: torch.Size([2, 1498])\n",
      "2025-05-02 22:09:40,975 - INFO - Train Batch Labels shape: torch.Size([2, 22])\n",
      "2025-05-02 22:09:40,975 - ERROR - Error during training step 0, batch keys: dict_keys(['input_values', 'attention_mask', 'labels']). Error: Wav2Vec2BertForSequenceClassification.forward() got an unexpected keyword argument 'input_values'\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_578021/1632541273.py\", line 21, in train_epoch\n",
      "    outputs = model(**model_inputs)\n",
      "  File \"/home/zhuoyuan/miniconda3/envs/musicClaGen_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/zhuoyuan/miniconda3/envs/musicClaGen_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "TypeError: Wav2Vec2BertForSequenceClassification.forward() got an unexpected keyword argument 'input_values'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|█▎        | 1/8 [00:01<00:09,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-02 22:09:41,208 - ERROR - Error during training step 1, batch keys: dict_keys(['input_values', 'attention_mask', 'labels']). Error: Wav2Vec2BertForSequenceClassification.forward() got an unexpected keyword argument 'input_values'\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_578021/1632541273.py\", line 21, in train_epoch\n",
      "    outputs = model(**model_inputs)\n",
      "  File \"/home/zhuoyuan/miniconda3/envs/musicClaGen_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/zhuoyuan/miniconda3/envs/musicClaGen_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "TypeError: Wav2Vec2BertForSequenceClassification.forward() got an unexpected keyword argument 'input_values'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▌       | 2/8 [00:01<00:04,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-02 22:09:41,465 - ERROR - Error during training step 2, batch keys: dict_keys(['input_values', 'attention_mask', 'labels']). Error: Wav2Vec2BertForSequenceClassification.forward() got an unexpected keyword argument 'input_values'\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_578021/1632541273.py\", line 21, in train_epoch\n",
      "    outputs = model(**model_inputs)\n",
      "  File \"/home/zhuoyuan/miniconda3/envs/musicClaGen_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/zhuoyuan/miniconda3/envs/musicClaGen_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "TypeError: Wav2Vec2BertForSequenceClassification.forward() got an unexpected keyword argument 'input_values'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|███▊      | 3/8 [00:01<00:02,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-02 22:09:41,672 - ERROR - Error during training step 3, batch keys: dict_keys(['input_values', 'attention_mask', 'labels']). Error: Wav2Vec2BertForSequenceClassification.forward() got an unexpected keyword argument 'input_values'\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_578021/1632541273.py\", line 21, in train_epoch\n",
      "    outputs = model(**model_inputs)\n",
      "  File \"/home/zhuoyuan/miniconda3/envs/musicClaGen_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/zhuoyuan/miniconda3/envs/musicClaGen_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "TypeError: Wav2Vec2BertForSequenceClassification.forward() got an unexpected keyword argument 'input_values'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 4/8 [00:02<00:01,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-02 22:09:41,883 - ERROR - Error during training step 4, batch keys: dict_keys(['input_values', 'attention_mask', 'labels']). Error: Wav2Vec2BertForSequenceClassification.forward() got an unexpected keyword argument 'input_values'\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_578021/1632541273.py\", line 21, in train_epoch\n",
      "    outputs = model(**model_inputs)\n",
      "  File \"/home/zhuoyuan/miniconda3/envs/musicClaGen_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/zhuoyuan/miniconda3/envs/musicClaGen_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "TypeError: Wav2Vec2BertForSequenceClassification.forward() got an unexpected keyword argument 'input_values'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|██████▎   | 5/8 [00:02<00:00,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-02 22:09:42,102 - ERROR - Error during training step 5, batch keys: dict_keys(['input_values', 'attention_mask', 'labels']). Error: Wav2Vec2BertForSequenceClassification.forward() got an unexpected keyword argument 'input_values'\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_578021/1632541273.py\", line 21, in train_epoch\n",
      "    outputs = model(**model_inputs)\n",
      "  File \"/home/zhuoyuan/miniconda3/envs/musicClaGen_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/zhuoyuan/miniconda3/envs/musicClaGen_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "TypeError: Wav2Vec2BertForSequenceClassification.forward() got an unexpected keyword argument 'input_values'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|███████▌  | 6/8 [00:02<00:00,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-02 22:09:42,314 - ERROR - Error during training step 6, batch keys: dict_keys(['input_values', 'attention_mask', 'labels']). Error: Wav2Vec2BertForSequenceClassification.forward() got an unexpected keyword argument 'input_values'\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_578021/1632541273.py\", line 21, in train_epoch\n",
      "    outputs = model(**model_inputs)\n",
      "  File \"/home/zhuoyuan/miniconda3/envs/musicClaGen_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/zhuoyuan/miniconda3/envs/musicClaGen_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "TypeError: Wav2Vec2BertForSequenceClassification.forward() got an unexpected keyword argument 'input_values'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|████████▊ | 7/8 [00:02<00:00,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-02 22:09:42,519 - ERROR - Error during training step 7, batch keys: dict_keys(['input_values', 'attention_mask', 'labels']). Error: Wav2Vec2BertForSequenceClassification.forward() got an unexpected keyword argument 'input_values'\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_578021/1632541273.py\", line 21, in train_epoch\n",
      "    outputs = model(**model_inputs)\n",
      "  File \"/home/zhuoyuan/miniconda3/envs/musicClaGen_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/zhuoyuan/miniconda3/envs/musicClaGen_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "TypeError: Wav2Vec2BertForSequenceClassification.forward() got an unexpected keyword argument 'input_values'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Training Loss for Epoch: 0.0000 (Total Loss: 0, Samples: 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-02 22:09:42,737 - INFO - Eval Batch Input 'input_values' shape: torch.Size([2, 1499, 160])\n",
      "2025-05-02 22:09:42,738 - INFO - Eval Batch Input 'attention_mask' shape: torch.Size([2, 1499])\n",
      "2025-05-02 22:09:42,738 - INFO - Eval Batch Labels shape: torch.Size([2, 22])\n",
      "2025-05-02 22:09:42,739 - ERROR - Error during evaluation step 0, batch keys: dict_keys(['input_values', 'attention_mask', 'labels']). Error: Wav2Vec2BertForSequenceClassification.forward() got an unexpected keyword argument 'input_values'\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_578021/1040071444.py\", line 22, in evaluate\n",
      "    outputs = model(**model_inputs)\n",
      "  File \"/home/zhuoyuan/miniconda3/envs/musicClaGen_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/zhuoyuan/miniconda3/envs/musicClaGen_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "TypeError: Wav2Vec2BertForSequenceClassification.forward() got an unexpected keyword argument 'input_values'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  25%|██▌       | 1/4 [00:00<00:00,  4.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-02 22:09:42,949 - ERROR - Error during evaluation step 1, batch keys: dict_keys(['input_values', 'attention_mask', 'labels']). Error: Wav2Vec2BertForSequenceClassification.forward() got an unexpected keyword argument 'input_values'\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_578021/1040071444.py\", line 22, in evaluate\n",
      "    outputs = model(**model_inputs)\n",
      "  File \"/home/zhuoyuan/miniconda3/envs/musicClaGen_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/zhuoyuan/miniconda3/envs/musicClaGen_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "TypeError: Wav2Vec2BertForSequenceClassification.forward() got an unexpected keyword argument 'input_values'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  50%|█████     | 2/4 [00:00<00:00,  4.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-02 22:09:43,218 - ERROR - Error during evaluation step 2, batch keys: dict_keys(['input_values', 'attention_mask', 'labels']). Error: Wav2Vec2BertForSequenceClassification.forward() got an unexpected keyword argument 'input_values'\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_578021/1040071444.py\", line 22, in evaluate\n",
      "    outputs = model(**model_inputs)\n",
      "  File \"/home/zhuoyuan/miniconda3/envs/musicClaGen_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/zhuoyuan/miniconda3/envs/musicClaGen_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "TypeError: Wav2Vec2BertForSequenceClassification.forward() got an unexpected keyword argument 'input_values'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  75%|███████▌  | 3/4 [00:00<00:00,  4.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-02 22:09:43,448 - ERROR - Error during evaluation step 3, batch keys: dict_keys(['input_values', 'attention_mask', 'labels']). Error: Wav2Vec2BertForSequenceClassification.forward() got an unexpected keyword argument 'input_values'\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_578021/1040071444.py\", line 22, in evaluate\n",
      "    outputs = model(**model_inputs)\n",
      "  File \"/home/zhuoyuan/miniconda3/envs/musicClaGen_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/zhuoyuan/miniconda3/envs/musicClaGen_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "TypeError: Wav2Vec2BertForSequenceClassification.forward() got an unexpected keyword argument 'input_values'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-02 22:09:43,450 - WARNING - Evaluation yielded no results (all batches failed or empty dataloader?).\n",
      "\n",
      "Debug Epoch 1 finished.\n",
      "  Avg Train Loss: 0.0000\n",
      "  Validation failed to produce metrics.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-02 22:09:47,856 - INFO - Saved debug model checkpoint to /home/zhuoyuan/CSprojects/musicClaGen/models/wav2vec2bert_debug_epoch_1.pth\n",
      "\n",
      "--- Debug Run Finished in 8.29 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Run ONE Epoch for Debugging\n",
    "\n",
    "from tqdm import tqdm # Ensure tqdm is imported\n",
    "\n",
    "# Ensure model, criterion, optimizer, dataloaders etc. are defined from previous cells\n",
    "print(f\"\\n--- Starting Debug Training Run for {num_epochs_debug} epoch ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Make sure model and criterion are on the correct device\n",
    "model.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "for epoch in range(num_epochs_debug): # num_epochs_debug was set to 1 in Cell 2\n",
    "    print(f\"\\n--- Debug Epoch {epoch+1}/{num_epochs_debug} ---\")\n",
    "\n",
    "    # Run training step for one epoch on the debug training data\n",
    "    train_loss = train_epoch(\n",
    "        model,\n",
    "        debug_train_dataloader, # Use the SMALL debug dataloader\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        device,\n",
    "        gradient_accumulation_steps # Pass grad accum steps\n",
    "    )\n",
    "\n",
    "    # Run evaluation step on the debug validation data\n",
    "    eval_metrics = evaluate(\n",
    "        model,\n",
    "        debug_val_dataloader, # Use the SMALL debug dataloader\n",
    "        criterion,\n",
    "        device\n",
    "    )\n",
    "\n",
    "    print(f\"\\nDebug Epoch {epoch+1} finished.\")\n",
    "    print(f\"  Avg Train Loss: {train_loss:.4f}\")\n",
    "    if eval_metrics:\n",
    "        # Print all collected metrics\n",
    "        for name, value in eval_metrics.items():\n",
    "            print(f\"  Validation {name.replace('_', ' ').title()}: {value:.4f}\")\n",
    "    else:\n",
    "        print(\"  Validation failed to produce metrics.\")\n",
    "\n",
    "    # Optional: Save model after this 1 epoch for inspection\n",
    "    save_path = os.path.join(model_save_dir, f\"wav2vec2bert_debug_epoch_{epoch+1}.pth\") # <<<--- Corrected filename\n",
    "    try:\n",
    "         torch.save(model.state_dict(), save_path)\n",
    "         logging.info(f\"Saved debug model checkpoint to {save_path}\")\n",
    "    except Exception as e:\n",
    "         logging.error(f\"Failed to save debug model checkpoint: {e}\", exc_info=True)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\n--- Debug Run Finished in {end_time - start_time:.2f} seconds ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicClaGen_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
