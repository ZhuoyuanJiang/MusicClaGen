{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Set up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT detected as: /workspace/musicClaGen\n",
      "Adding /workspace/musicClaGen to sys.path\n",
      "/workspace/musicClaGen\n",
      "Imports and basic setup complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "# Use AutoProcessor for Wav2Vec2-BERT - it bundles feature_extractor and tokenizer (if needed)\n",
    "from transformers import AutoModelForAudioClassification, AutoProcessor\n",
    "\n",
    "from torch.optim import AdamW\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import ast # For parsing string representations of lists/arrays\n",
    "import logging\n",
    "import time\n",
    "from sklearn.metrics import hamming_loss, jaccard_score, f1_score # Add more as needed\n",
    "from tqdm.notebook import tqdm # Use notebook version of tqdm\n",
    "import librosa # Needed for loading raw audio now\n",
    "\n",
    "\n",
    "\n",
    "# --- Project Setup ---\n",
    "# Detect if running in notebook or script to adjust path\n",
    "\n",
    "cwd = os.getcwd()\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(cwd, '../../')) # NOTE: remember to change if change the directory structure\n",
    "\n",
    "\n",
    "\n",
    "print(f\"PROJECT_ROOT detected as: {PROJECT_ROOT}\")\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    print(f\"Adding {PROJECT_ROOT} to sys.path\")\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "# --- Config and Utils ---\n",
    "try:\n",
    "    import config # Import your configuration file\n",
    "    # Optionally import utils if needed, e.g., for get_audio_path if not defined here\n",
    "    # import src.utils as utils\n",
    "except ModuleNotFoundError:\n",
    "     print(\"ERROR: Cannot import config or utils. Make sure PROJECT_ROOT is correct and src is importable.\")\n",
    "     # Or add src to path: sys.path.insert(0, os.path.join(PROJECT_ROOT, 'src'))\n",
    "     # import config\n",
    "     # import utils\n",
    "\n",
    "\n",
    "# --- Setup Logging ---\n",
    "for handler in logging.root.handlers[:]: logging.root.removeHandler(handler) # Clear previous\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    handlers=[logging.StreamHandler(sys.stdout)])\n",
    "\n",
    "print(\"Imports and basic setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 15:10:22,028 - INFO - Loaded 22 unified genres from /workspace/musicClaGen/data/processed/unified_genres.txt\n",
      "2025-05-04 15:10:22,030 - INFO - Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Cell 2\n",
    "# --- Load Config ---\n",
    "# Ensure config.py has the correct paths in the PATHS dict\n",
    "manifest_path = config.PATHS.get('SMALL_MULTILABEL_PATH', os.path.join(config.PATHS['PROCESSED_DATA_DIR'], 'small_subset_multihot.csv')) # Use .get for safety\n",
    "genre_list_path = config.PATHS.get('GENRE_LIST_PATH', os.path.join(config.PATHS['PROCESSED_DATA_DIR'], 'unified_genres.txt'))\n",
    "model_save_dir = config.PATHS['MODELS_DIR']\n",
    "\n",
    "# Ensure config.py has MODEL_PARAMS dict with model_checkpoint\n",
    "model_checkpoint = config.MODEL_PARAMS['model_checkpoint'] # e.g., \"facebook/w2v-bert-2.0\" - VERIFY!\n",
    "learning_rate = config.MODEL_PARAMS['learning_rate']\n",
    "batch_size = config.MODEL_PARAMS['batch_size'] # Use the small BS for notebook test\n",
    "num_epochs_debug = 1 # <<<--- RUN ONLY 1 EPOCH FOR DEBUGGING ---<<<\n",
    "weight_decay = config.MODEL_PARAMS['weight_decay']\n",
    "gradient_accumulation_steps = config.MODEL_PARAMS['gradient_accumulation_steps']\n",
    "\n",
    "# --- Load unified genre list ---\n",
    "try:\n",
    "    with open(genre_list_path, 'r') as f:\n",
    "        unified_genres = [line.strip() for line in f if line.strip()]\n",
    "    num_labels = len(unified_genres) # should be the number of labels defined in the unified_genres.txt file, in this case it should be 22.\n",
    "    logging.info(f\"Loaded {num_labels} unified genres from {genre_list_path}\")\n",
    "    if num_labels == 0: raise ValueError(\"Genre list is empty!\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to load or process unified genre list: {e}\", exc_info=True)\n",
    "    raise SystemExit(\"Cannot proceed without genre list.\")\n",
    "\n",
    "# --- Setup Device ---\n",
    "device = torch.device(config.DEVICE if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(f\"Using device: {device}\")\n",
    "if not torch.cuda.is_available() and config.DEVICE==\"cuda\":\n",
    "     logging.warning(\"CUDA selected but not available, falling back to CPU.\")\n",
    "\n",
    "# --- Create Save Directory ---\n",
    "os.makedirs(model_save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/musicClaGen/data/processed/small_subset_multihot.csv\n"
     ]
    }
   ],
   "source": [
    "print(manifest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 15:10:22,547 - INFO - TrainingLogger class defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell: Comprehensive Training Logger Setup\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "class TrainingLogger:\n",
    "    \"\"\"Comprehensive training logger and metrics tracker\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir, model_name, config=None):\n",
    "        \"\"\"\n",
    "        Initialize logger with output directory and model name\n",
    "        \n",
    "        Args:\n",
    "            output_dir: Base directory for saving logs and checkpoints\n",
    "            model_name: Name of the model being trained\n",
    "            config: Dictionary of configuration parameters\n",
    "        \"\"\"\n",
    "        # Create timestamp for unique run identification\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.run_name = f\"{model_name}_{timestamp}\"\n",
    "        \n",
    "        # Create output directories\n",
    "        self.base_dir = os.path.join(output_dir, self.run_name)\n",
    "        self.checkpoint_dir = os.path.join(self.base_dir, \"checkpoints\")\n",
    "        self.log_dir = os.path.join(self.base_dir, \"logs\")\n",
    "        self.plot_dir = os.path.join(self.base_dir, \"plots\")\n",
    "        \n",
    "        os.makedirs(self.base_dir, exist_ok=True)\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "        os.makedirs(self.plot_dir, exist_ok=True)\n",
    "        \n",
    "        # Initialize metrics storage\n",
    "        self.metrics = {\n",
    "            \"train_loss\": [],\n",
    "            \"val_loss\": [],\n",
    "            \"hamming_loss\": [],\n",
    "            \"jaccard_samples\": [],\n",
    "            \"f1_micro\": [],\n",
    "            \"f1_macro\": [],\n",
    "            \"learning_rate\": [],\n",
    "            \"epochs\": [],\n",
    "            \"steps\": [],\n",
    "            \"best_metrics\": {},\n",
    "            \"training_time\": 0\n",
    "        }\n",
    "        \n",
    "        # Save configuration\n",
    "        self.config = config\n",
    "        if config:\n",
    "            with open(os.path.join(self.base_dir, \"config.json\"), 'w') as f:\n",
    "                json.dump(config, f, indent=2)\n",
    "        \n",
    "        # Setup file logger\n",
    "        self.setup_file_logger()\n",
    "        \n",
    "        # Log initialization\n",
    "        logging.info(f\"Initialized training run: {self.run_name}\")\n",
    "        logging.info(f\"Output directory: {self.base_dir}\")\n",
    "    \n",
    "    def setup_file_logger(self):\n",
    "        \"\"\"Setup file logging\"\"\"\n",
    "        log_file = os.path.join(self.log_dir, \"training.log\")\n",
    "        file_handler = logging.FileHandler(log_file)\n",
    "        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "        \n",
    "        # Add file handler to root logger\n",
    "        root_logger = logging.getLogger()\n",
    "        root_logger.addHandler(file_handler)\n",
    "    \n",
    "    def log_epoch(self, epoch, train_loss, val_metrics, learning_rate, step):\n",
    "        \"\"\"Log metrics for an epoch\"\"\"\n",
    "        self.metrics[\"epochs\"].append(epoch)\n",
    "        self.metrics[\"train_loss\"].append(train_loss)\n",
    "        self.metrics[\"val_loss\"].append(val_metrics.get(\"eval_loss\", 0))\n",
    "        self.metrics[\"hamming_loss\"].append(val_metrics.get(\"hamming_loss\", 0))\n",
    "        self.metrics[\"jaccard_samples\"].append(val_metrics.get(\"jaccard_samples\", 0))\n",
    "        self.metrics[\"f1_micro\"].append(val_metrics.get(\"f1_micro\", 0))\n",
    "        self.metrics[\"f1_macro\"].append(val_metrics.get(\"f1_macro\", 0))\n",
    "        self.metrics[\"learning_rate\"].append(learning_rate)\n",
    "        self.metrics[\"steps\"].append(step)\n",
    "        \n",
    "        # Log to file\n",
    "        logging.info(f\"Epoch {epoch} metrics:\")\n",
    "        logging.info(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        for name, value in val_metrics.items():\n",
    "            if isinstance(value, (int, float)):\n",
    "                logging.info(f\"  {name.replace('_', ' ').title()}: {value:.4f}\")\n",
    "        \n",
    "        # Save metrics after each epoch\n",
    "        self.save_metrics()\n",
    "        \n",
    "        # Generate plots\n",
    "        self.generate_plots()\n",
    "    \n",
    "    def update_best_metrics(self, epoch, step, val_metrics, model_path):\n",
    "        \"\"\"Update best metrics if current results are better\"\"\"\n",
    "        current_metric = val_metrics.get(\"hamming_loss\", float('inf'))\n",
    "        \n",
    "        # For hamming loss, lower is better\n",
    "        if not self.metrics[\"best_metrics\"] or current_metric < self.metrics[\"best_metrics\"].get(\"hamming_loss\", float('inf')):\n",
    "            self.metrics[\"best_metrics\"] = {\n",
    "                \"epoch\": epoch,\n",
    "                \"step\": step,\n",
    "                \"model_checkpoint\": model_path,\n",
    "                **{k: v for k, v in val_metrics.items() if isinstance(v, (int, float))}\n",
    "            }\n",
    "            logging.info(f\"New best model at epoch {epoch}, step {step} with hamming_loss: {current_metric:.4f}\")\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def save_metrics(self):\n",
    "        \"\"\"Save metrics to JSON file\"\"\"\n",
    "        metrics_file = os.path.join(self.log_dir, \"metrics.json\")\n",
    "        with open(metrics_file, 'w') as f:\n",
    "            json.dump(self.metrics, f, indent=2)\n",
    "    \n",
    "    def save_trainer_state(self, epoch, step, optimizer_state=None, scheduler_state=None):\n",
    "        \"\"\"Save trainer state\"\"\"\n",
    "        trainer_state = {\n",
    "            \"epoch\": epoch,\n",
    "            \"step\": step,\n",
    "            \"best_metrics\": self.metrics[\"best_metrics\"],\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"training_time\": self.metrics[\"training_time\"]\n",
    "        }\n",
    "        \n",
    "        # Add optimizer and scheduler states if provided\n",
    "        if optimizer_state:\n",
    "            optimizer_state_file = os.path.join(self.log_dir, \"optimizer_state.pt\")\n",
    "            torch.save(optimizer_state, optimizer_state_file)\n",
    "            trainer_state[\"optimizer_state_file\"] = optimizer_state_file\n",
    "            \n",
    "        if scheduler_state:\n",
    "            scheduler_state_file = os.path.join(self.log_dir, \"scheduler_state.pt\")\n",
    "            torch.save(scheduler_state, scheduler_state_file)\n",
    "            trainer_state[\"scheduler_state_file\"] = scheduler_state_file\n",
    "            \n",
    "        state_file = os.path.join(self.base_dir, \"trainer_state.json\")\n",
    "        with open(state_file, 'w') as f:\n",
    "            json.dump(trainer_state, f, indent=2)\n",
    "    \n",
    "    def generate_plots(self):\n",
    "        \"\"\"Generate and save plots of training metrics\"\"\"\n",
    "        # Loss plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.metrics[\"epochs\"], self.metrics[\"train_loss\"], label=\"Train Loss\")\n",
    "        plt.plot(self.metrics[\"epochs\"], self.metrics[\"val_loss\"], label=\"Validation Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training and Validation Loss\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(os.path.join(self.plot_dir, \"loss_plot.png\"))\n",
    "        plt.close()\n",
    "        \n",
    "        # Metrics plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(self.metrics[\"epochs\"], self.metrics[\"hamming_loss\"])\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Hamming Loss\")\n",
    "        plt.title(\"Hamming Loss\")\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(self.metrics[\"epochs\"], self.metrics[\"jaccard_samples\"])\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Jaccard Score\")\n",
    "        plt.title(\"Jaccard Score (Samples)\")\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(self.metrics[\"epochs\"], self.metrics[\"f1_micro\"])\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"F1 Score\")\n",
    "        plt.title(\"F1 Score (Micro)\")\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(self.metrics[\"epochs\"], self.metrics[\"f1_macro\"])\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"F1 Score\")\n",
    "        plt.title(\"F1 Score (Macro)\")\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.plot_dir, \"metrics_plot.png\"))\n",
    "        plt.close()\n",
    "        \n",
    "        # Learning rate plot\n",
    "        if len(self.metrics[\"steps\"]) > 0:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(self.metrics[\"steps\"], self.metrics[\"learning_rate\"])\n",
    "            plt.xlabel(\"Step\")\n",
    "            plt.ylabel(\"Learning Rate\")\n",
    "            plt.title(\"Learning Rate Schedule\")\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(self.plot_dir, \"lr_plot.png\"))\n",
    "            plt.close()\n",
    "    \n",
    "    def save_model_checkpoint(self, model, epoch, step, optimizer=None, scheduler=None, is_best=False):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        checkpoint_name = f\"checkpoint-{epoch}\"\n",
    "        checkpoint_path = os.path.join(self.checkpoint_dir, checkpoint_name)\n",
    "        os.makedirs(checkpoint_path, exist_ok=True)\n",
    "        \n",
    "        # Save model state dict\n",
    "        model_path = os.path.join(checkpoint_path, \"model.pth\")\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        \n",
    "        # Save optimizer and scheduler if provided\n",
    "        if optimizer:\n",
    "            optimizer_path = os.path.join(checkpoint_path, \"optimizer.pth\")\n",
    "            torch.save(optimizer.state_dict(), optimizer_path)\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler_path = os.path.join(checkpoint_path, \"scheduler.pth\")\n",
    "            torch.save(scheduler.state_dict(), scheduler_path)\n",
    "        \n",
    "        # Save model config\n",
    "        if hasattr(model, 'config'):\n",
    "            config_path = os.path.join(checkpoint_path, \"config.json\")\n",
    "            with open(config_path, 'w') as f:\n",
    "                json.dump(model.config.to_dict(), f, indent=2)\n",
    "        \n",
    "        # Create a symbolic link or copy for the best model\n",
    "        if is_best:\n",
    "            best_path = os.path.join(self.base_dir, \"best_model\")\n",
    "            # If we can use symlinks\n",
    "            try:\n",
    "                if os.path.exists(best_path):\n",
    "                    if os.path.islink(best_path):\n",
    "                        os.unlink(best_path)\n",
    "                    else:\n",
    "                        os.rmdir(best_path)\n",
    "                os.symlink(checkpoint_path, best_path)\n",
    "                logging.info(f\"Created symbolic link to best model at {best_path}\")\n",
    "            except (OSError, NotImplementedError):\n",
    "                # Fallback: copy the model file\n",
    "                best_model_path = os.path.join(self.base_dir, \"best_model.pth\")\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "                logging.info(f\"Saved copy of best model to {best_model_path}\")\n",
    "            \n",
    "        logging.info(f\"Saved model checkpoint to {checkpoint_path}\")\n",
    "        return checkpoint_path\n",
    "    \n",
    "    def finish_training(self, total_time):\n",
    "        \"\"\"Log end of training and final metrics\"\"\"\n",
    "        self.metrics[\"training_time\"] = total_time\n",
    "        logging.info(f\"Training completed in {total_time:.2f} seconds\")\n",
    "        logging.info(f\"Best model: {self.metrics['best_metrics'].get('model_checkpoint', 'None')}\")\n",
    "        \n",
    "        # Save final metrics\n",
    "        self.save_metrics()\n",
    "        \n",
    "        # Generate final plots\n",
    "        self.generate_plots()\n",
    "        \n",
    "        # Save final trainer state\n",
    "        self.save_trainer_state(\n",
    "            epoch=self.metrics[\"epochs\"][-1] if self.metrics[\"epochs\"] else 0,\n",
    "            step=self.metrics[\"steps\"][-1] if self.metrics[\"steps\"] else 0\n",
    "        )\n",
    "\n",
    "logging.info(\"TrainingLogger class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dataset Class Definition + Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 3: Dataset Class Definition (Raw Audio Version) This cell uses the regex parser to parse the multi_hot_label string back into a list of integers.\n",
    "\n",
    "\n",
    "\n",
    "# # Define(recollect)the regex parser from preprocess.py if needed, \n",
    "# # otherwise use ast.literal_eval--- \n",
    "# # NOTE: After changing usage.ipynb 05/03/2025, should fall back to ast.literal_eval now. Clean code later\n",
    "\n",
    "# import re\n",
    "\n",
    "# def parse_numpy_array_string(array_str):\n",
    "#     \"\"\"\n",
    "#     Parse strings like '[np.float32(1.0), np.float32(0.0), ...]' into a list of integers.\n",
    "#     This is needed because ast.literal_eval cannot handle 'np.float32()' in the string.\n",
    "#     \"\"\"\n",
    "#     if not isinstance(array_str, str):\n",
    "#         return []\n",
    "    \n",
    "#     try:\n",
    "#         # Extract all the float values using regular expressions\n",
    "#         float_matches = re.findall(r'np\\.float32\\((\\d+\\.\\d+)\\)', array_str)\n",
    "        \n",
    "#         # Convert matches to integers (1.0 -> 1, 0.0 -> 0)\n",
    "#         values = []\n",
    "#         for match in float_matches:\n",
    "#             value = float(match)\n",
    "#             # Convert to integer if it's 0.0 or 1.0\n",
    "#             if value == 1.0:\n",
    "#                 values.append(1)\n",
    "#             elif value == 0.0:\n",
    "#                 values.append(0)\n",
    "#             else:\n",
    "#                 values.append(value)  # Keep as float if not 0 or 1\n",
    "                \n",
    "#         return values\n",
    "#     except Exception as e:\n",
    "#         logging.warning(f\"Error parsing array string: {e}\")\n",
    "#         return []\n",
    "\n",
    "# class FMARawAudioDataset(Dataset):\n",
    "#     \"\"\"\n",
    "#     Loads raw audio waveforms and labels from manifest, uses Hugging Face\n",
    "#     feature extractor (like ASTFeatureExtractor or Wav2Vec2Processor) on the fly.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, manifest_path, feature_extractor):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             manifest_path (str): Path to the final manifest CSV file.\n",
    "#             feature_extractor: Initialized Hugging Face AutoFeatureExtractor or AutoProcessor.\n",
    "#         \"\"\"\n",
    "#         logging.info(f\"Initializing FMARawAudioDataset from: {manifest_path}\")\n",
    "#         if feature_extractor is None:\n",
    "#              raise ValueError(\"FMARawAudioDataset requires a feature_extractor/processor instance.\")\n",
    "\n",
    "#         self.feature_extractor = feature_extractor\n",
    "#         # Get target sampling rate directly from the extractor/processor\n",
    "#         try:\n",
    "#              # Works for Wav2Vec2Processor, ASTFeatureExtractor, etc.\n",
    "#              self.target_sr = self.feature_extractor.sampling_rate\n",
    "#              logging.info(f\"Target sampling rate set from feature extractor: {self.target_sr} Hz\")\n",
    "#         except AttributeError:\n",
    "#              logging.warning(\"Could not get sampling_rate from feature_extractor, using config.\")\n",
    "#              # Fallback to config if needed, but ensuring match is crucial\n",
    "#              self.target_sr = config.PREPROCESSING_PARAMS['sample_rate']\n",
    "\n",
    "\n",
    "#         logging.info(f\"Loading manifest from: {manifest_path}\")\n",
    "#         try:\n",
    "#             self.manifest = pd.read_csv(manifest_path)\n",
    "#             # Ensure index is set if needed elsewhere, or use default range index\n",
    "#             if 'track_id' in self.manifest.columns:\n",
    "#                  self.manifest = self.manifest.set_index('track_id', drop=False)\n",
    "\n",
    "#             # --- Parse the 'multi_hot_label' string back into a list ---\n",
    "#             # Here: if we decide to use raw audio, we use regex parser; \n",
    "#             #       if we decide to use mel spectrogram, we use ast.literal_eval\n",
    "\n",
    "#             # Choose the correct parser based on how labels were saved in the CSV\n",
    "#             # If saved as '[1.0, 0.0,...]' use ast.literal_eval\n",
    "#             # label_parser = ast.literal_eval\n",
    "#             # If saved as '[np.float32(1.0)...]' uncomment and use regex parser\n",
    "#             label_parser = parse_numpy_array_string\n",
    "\n",
    "#             self.manifest['multi_hot_label'] = self.manifest['multi_hot_label'].apply(label_parser)\n",
    "#             logging.info(f\"Loaded and parsed manifest with {len(self.manifest)} entries.\")\n",
    "#             # Check the first parsed label\n",
    "#             logging.info(f\"Example parsed label (first entry): {self.manifest['multi_hot_label'].iloc[0]}\")\n",
    "\n",
    "#         except Exception as e:\n",
    "#             logging.error(f\"Error loading or parsing manifest {manifest_path}: {e}\", exc_info=True)\n",
    "#             raise\n",
    "\n",
    "#     def __len__(self):\n",
    "#         \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "#         return len(self.manifest)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         \"\"\"\n",
    "#         Loads raw audio for index idx, processes it with the feature extractor,\n",
    "#         and returns the processed inputs and labels.\n",
    "#         \"\"\"\n",
    "#         if torch.is_tensor(idx): idx = idx.tolist() # Handle tensor indices\n",
    "\n",
    "#         # Get the row data from the manifest\n",
    "#         row = self.manifest.iloc[idx]\n",
    "#         track_id = row.get('track_id', self.manifest.index[idx]) # Get track_id safely\n",
    "#         label_vector = row['multi_hot_label'] # Already parsed list/array\n",
    "\n",
    "#         # Construct absolute audio path if necessary\n",
    "#         audio_path = row['audio_path']\n",
    "\n",
    "#         #NOTE: originally, the mel-spectrogram's path is relative  but the raw audio's path is absolute, so we need to make sure the audio_path is absolute\n",
    "#         # So we are check if the audio_path is absolute or relative in case we load the wrong data, if it's relative, we need to join it with the PROJECT_ROOT\n",
    "#         if not os.path.isabs(audio_path):\n",
    "#              # Assumes path in manifest is relative to PROJECT_ROOT\n",
    "#              audio_path = os.path.join(config.PROJECT_ROOT, audio_path)\n",
    "\n",
    "#         try:\n",
    "#             # --- 1. Load RAW Audio Waveform ---\n",
    "#             # Load full 30s clip at the TARGET sample rate required by the processor\n",
    "#             waveform, loaded_sr = librosa.load(\n",
    "#                 audio_path,\n",
    "#                 sr=self.target_sr, # Use processor's sampling rate\n",
    "#                 duration=30.0     # Load the full 30 seconds\n",
    "#             )\n",
    "#             # Ensure minimum length if needed (though duration should handle it)\n",
    "#             min_samples = int(0.1 * self.target_sr) # Example: require at least 0.1s\n",
    "#             if len(waveform) < min_samples:\n",
    "#                  raise ValueError(f\"Audio signal for track {track_id} too short after loading.\")\n",
    "\n",
    "#             # --- 2. Apply Feature Extractor ---\n",
    "#             # Pass the raw waveform numpy array\n",
    "#             # The extractor handles normalization, padding/truncation, tensor conversion\n",
    "            \n",
    "#             max_length = 5000\n",
    "\n",
    "#             inputs = self.feature_extractor(\n",
    "#                 waveform,\n",
    "#                 sampling_rate=self.target_sr,\n",
    "#                 return_tensors=\"pt\",\n",
    "#                 return_attention_mask=True # Request attention mask\n",
    "#             )\n",
    "\n",
    "#             # --- 3. Prepare Outputs ---\n",
    "#             # Squeeze unnecessary batch dimension added by the extractor\n",
    "#             # Key name ('input_values', 'input_features') depends on the specific extractor\n",
    "#             feature_tensor = inputs.get('input_values', inputs.get('input_features'))\n",
    "#             if feature_tensor is None:\n",
    "#                 raise KeyError(\"Expected 'input_values' or 'input_features' key from feature_extractor output.\")\n",
    "#             feature_tensor = feature_tensor.squeeze(0) # Remove batch dim -> [Channels?, Freq?, Time] or [SeqLen, Dim]\n",
    "\n",
    "#             attention_mask = inputs.get('attention_mask', None)\n",
    "#             if attention_mask is not None:\n",
    "#                  attention_mask = attention_mask.squeeze(0)\n",
    "\n",
    "#             # Convert label list/array to float tensor for BCE loss\n",
    "#             label_tensor = torch.tensor(label_vector, dtype=torch.float32)\n",
    "\n",
    "#             # Return dictionary matching model's expected input names\n",
    "#             model_input_dict = {\"labels\": label_tensor}\n",
    "#             # Use the key the feature extractor provided\n",
    "#             if 'input_values' in inputs:\n",
    "#                  model_input_dict['input_values'] = feature_tensor\n",
    "#             elif 'input_features' in inputs:\n",
    "#                  model_input_dict['input_features'] = feature_tensor\n",
    "\n",
    "#             if attention_mask is not None:\n",
    "#                  model_input_dict['attention_mask'] = attention_mask\n",
    "\n",
    "#             return model_input_dict\n",
    "\n",
    "#         except FileNotFoundError:\n",
    "#              logging.error(f\"Audio file not found for track {track_id} at {audio_path}\")\n",
    "#              raise # Or implement skipping logic with collate_fn\n",
    "#         except Exception as e:\n",
    "#             logging.error(f\"Error loading/processing track {track_id} at {audio_path}: {e}\", exc_info=True)\n",
    "#             raise # Or implement skipping logic\n",
    "\n",
    "\n",
    "# print(\"FMARawAudioDataset class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FMARawAudioDataset class defined (using raw audio, feature extractor, ast.literal_eval for labels, .loc access).\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Dataset Class Definition (Raw Audio Version - This cell uses the ast.literal_eval parser to parse the multi_hot_label string back into a list of integers.)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import ast # For parsing label string '[1.0, 0.0,...]'\n",
    "import re  # Keep import for the commented out function below\n",
    "import logging\n",
    "import librosa\n",
    "# Ensure config is imported from a previous cell or uncomment:\n",
    "# import config\n",
    "\n",
    "# --- Optional: Keep custom parser commented out for reference ---\n",
    "# # Define(recollect)the regex parser from preprocess.py if needed,\n",
    "# # otherwise use ast.literal_eval---\n",
    "# # NOTE: After changing usage.ipynb 05/03/2025, should fall back to ast.literal_eval now. Clean code later\n",
    "# def parse_numpy_array_string(array_str):\n",
    "#     \"\"\"\n",
    "#     Parse strings like '[np.float32(1.0), np.float32(0.0), ...]' into a list of integers.\n",
    "#     This is needed because ast.literal_eval cannot handle 'np.float32()' in the string.\n",
    "#     \"\"\"\n",
    "#     if not isinstance(array_str, str): return []\n",
    "#     try:\n",
    "#         # Match digits, optionally followed by a decimal and more digits\n",
    "#         float_matches = re.findall(r'np\\.float32\\(([\\d\\.]+)\\)', array_str)\n",
    "#         values = []\n",
    "#         for match_str in float_matches:\n",
    "#             value = float(match_str) # Convert string match to float\n",
    "#             values.append(1.0 if value == 1.0 else 0.0) # Store as float 0.0 or 1.0\n",
    "#         return values\n",
    "#     except Exception as e:\n",
    "#         logging.warning(f\"Error parsing array string: {e}\")\n",
    "#         return []\n",
    "# --- End commented out parser ---\n",
    "\n",
    "\n",
    "class FMARawAudioDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads raw audio waveforms and labels from manifest, uses Hugging Face\n",
    "    feature extractor (like ASTFeatureExtractor or Wav2Vec2Processor/AutoFeatureExtractor) on the fly.\n",
    "    Assumes padding/truncation will be handled by a collate function.\n",
    "    \"\"\"\n",
    "    def __init__(self, manifest_path, feature_extractor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            manifest_path (str): Path to the final manifest CSV file (e.g., small_subset_multihot.csv).\n",
    "            feature_extractor: Initialized Hugging Face AutoFeatureExtractor instance.\n",
    "        \"\"\"\n",
    "        # Ensure num_labels is available globally or passed if needed for verification\n",
    "        global num_labels\n",
    "        if 'num_labels' not in globals():\n",
    "             logging.error(\"Global variable 'num_labels' not found. Load it first (e.g., from Cell 2).\")\n",
    "             # Alternative: pass num_labels as an argument to __init__\n",
    "\n",
    "        logging.info(f\"Initializing FMARawAudioDataset from: {manifest_path}\")\n",
    "        if feature_extractor is None:\n",
    "             raise ValueError(\"FMARawAudioDataset requires a feature_extractor instance.\")\n",
    "\n",
    "        self.feature_extractor = feature_extractor\n",
    "        try:\n",
    "             self.target_sr = self.feature_extractor.sampling_rate\n",
    "             logging.info(f\"Target sampling rate set from feature extractor: {self.target_sr} Hz\")\n",
    "        except AttributeError:\n",
    "             logging.error(\"Could not get sampling_rate from feature_extractor.\", exc_info=True)\n",
    "             raise\n",
    "\n",
    "        logging.info(f\"Loading manifest from: {manifest_path}\")\n",
    "        try:\n",
    "            self.manifest = pd.read_csv(manifest_path)\n",
    "            # Set index to track_id AFTER loading, keep column too if needed elsewhere\n",
    "            if 'track_id' in self.manifest.columns:\n",
    "                 self.manifest = self.manifest.set_index('track_id', drop=False) # Keep column if row.get('track_id'...) is used\n",
    "            else:\n",
    "                 logging.warning(\"Manifest CSV does not contain 'track_id' column. Using DataFrame index.\")\n",
    "                 # Make sure index IS the track_id\n",
    "                 if not pd.api.types.is_integer_dtype(self.manifest.index):\n",
    "                      logging.warning(\"Manifest index is not integer type. Ensure it matches track IDs.\")\n",
    "\n",
    "\n",
    "            # --- Parse the 'multi_hot_label' string back into a list ---\n",
    "            # NOTE: After changing usage.ipynb 05/03/2025, should fall back to ast.literal_eval now. Clean code later\n",
    "            # Use ast.literal_eval assuming labels were saved as standard list strings '[1.0, 0.0,...]'\n",
    "            logging.info(\"Attempting to parse 'multi_hot_label' column using ast.literal_eval...\")\n",
    "            label_parser = ast.literal_eval # <<<--- Using ast.literal_eval\n",
    "            # label_parser = parse_numpy_array_string # Keep commented out as requested\n",
    "\n",
    "            label_col_name = 'multi_hot_label'\n",
    "            if label_col_name not in self.manifest.columns:\n",
    "                 raise KeyError(f\"Column '{label_col_name}' not found in manifest CSV at {manifest_path}\")\n",
    "\n",
    "            self.manifest[label_col_name] = self.manifest[label_col_name].apply(label_parser)\n",
    "\n",
    "            # --- Verification step ---\n",
    "            first_label = self.manifest[label_col_name].iloc[0] # Use iloc[0] here to get FIRST row for checking\n",
    "            if not isinstance(first_label, list):\n",
    "                 raise TypeError(f\"Parsed label is not a list, check parser/CSV format. Got type: {type(first_label)}\")\n",
    "            # Check length against num_labels loaded in Cell 2\n",
    "            if len(first_label) != num_labels:\n",
    "                 logging.error(f\"FATAL: Parsed label length ({len(first_label)}) does not match expected num_labels ({num_labels}). Check parsing or unified_genres.txt.\")\n",
    "                 raise ValueError(\"Parsed label length mismatch.\")\n",
    "            logging.info(f\"Example parsed label verified (type {type(first_label)}, length {len(first_label)}): {str(first_label)[:100]}...\")\n",
    "            # --- End Verification ---\n",
    "\n",
    "            logging.info(f\"Loaded and parsed manifest with {len(self.manifest)} entries.\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "             logging.error(f\"Manifest file not found: {manifest_path}\", exc_info=True)\n",
    "             raise\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading or parsing manifest {manifest_path}: {e}\", exc_info=True)\n",
    "            raise\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.manifest)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Loads raw audio for index 'idx' (which is the track_id/index label),\n",
    "        processes it with the feature extractor,\n",
    "        and returns the processed inputs and labels.\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx): idx = idx.tolist() # Handle tensor indices\n",
    "\n",
    "        # --- Use idx directly as track_id BEFORE main try block ---\n",
    "        track_id = idx\n",
    "        # ---------------------------------------------------------\n",
    "\n",
    "        try:\n",
    "            # --- Get the row data using .loc with the track_id ---\n",
    "            row = self.manifest.loc[track_id] # Use .loc with the index label (track_id)\n",
    "            # ------------------------------------------------------\n",
    "\n",
    "            # --- Get required data from the row ---\n",
    "            multi_hot_label = row['multi_hot_label']\n",
    "            audio_path = row['audio_path']\n",
    "            # ---------------------------------------\n",
    "\n",
    "            # Construct absolute audio path if necessary (keep your NOTE)\n",
    "            # NOTE: originally, the mel-spectrogram's path is relative  but the raw audio's path is absolute, so we need to make sure the audio_path is absolute\n",
    "            # So we are check if the audio_path is absolute or relative in case we load the wrong data, if it's relative, we need to join it with the PROJECT_ROOT\n",
    "            if not os.path.isabs(audio_path):\n",
    "                audio_path = os.path.join(config.PROJECT_ROOT, audio_path)\n",
    "\n",
    "            # --- 1. Load RAW Audio Waveform ---\n",
    "            waveform, loaded_sr = librosa.load(\n",
    "                audio_path,\n",
    "                sr=self.target_sr, # Use extractor's sampling rate\n",
    "                duration=30.0      # Load the full 30 seconds\n",
    "            )\n",
    "            min_samples = int(0.1 * self.target_sr)\n",
    "            if len(waveform) < min_samples:\n",
    "                 logging.warning(f\"Audio signal for track {track_id} too short, returning None.\")\n",
    "                 return None # Requires collate_fn to handle None\n",
    "\n",
    "            # --- 2. Apply Feature Extractor ---\n",
    "            # Let the Data Collator handle padding/truncation later\n",
    "            inputs = self.feature_extractor(\n",
    "                waveform,\n",
    "                sampling_rate=self.target_sr,\n",
    "                return_tensors=\"pt\",\n",
    "                # REMOVED padding/truncation/max_length args\n",
    "                return_attention_mask=True # Keep requesting mask\n",
    "            )\n",
    "\n",
    "            # --- 3. Prepare Outputs ---\n",
    "            feature_tensor = inputs.get('input_values', inputs.get('input_features'))\n",
    "            if feature_tensor is None:\n",
    "                raise KeyError(f\"Expected 'input_values' or 'input_features' key from feature_extractor output. Got keys: {inputs.keys()}\")\n",
    "            feature_tensor = feature_tensor.squeeze(0)\n",
    "\n",
    "            attention_mask = inputs.get('attention_mask', None)\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = attention_mask.squeeze(0)\n",
    "\n",
    "            # Convert label list to float tensor\n",
    "            label_tensor = torch.tensor(multi_hot_label, dtype=torch.float32)\n",
    "\n",
    "            # Return dictionary\n",
    "            model_input_dict = {\"labels\": label_tensor}\n",
    "            input_key = 'input_values' if 'input_values' in inputs else 'input_features'\n",
    "            model_input_dict[input_key] = feature_tensor\n",
    "            if attention_mask is not None:\n",
    "                model_input_dict['attention_mask'] = attention_mask\n",
    "\n",
    "            return model_input_dict\n",
    "\n",
    "        except KeyError:\n",
    "             # This might catch if track_id wasn't found by .loc (handled above),\n",
    "             # or if column names like 'multi_hot_label', 'audio_path' are wrong in CSV\n",
    "             logging.error(f\"KeyError accessing data for track {track_id}. Check manifest columns.\", exc_info=True)\n",
    "             return None\n",
    "        except FileNotFoundError:\n",
    "             logging.error(f\"Audio file not found for track {track_id} at {audio_path}\")\n",
    "             return None\n",
    "        except Exception as e:\n",
    "            # Use the track_id obtained safely before the try block\n",
    "            logging.error(f\"Error loading/processing track {track_id}: {e}\", exc_info=True)\n",
    "            return None # Return None on generic error\n",
    "\n",
    "print(\"FMARawAudioDataset class defined (using raw audio, feature extractor, ast.literal_eval for labels, .loc access).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "facebook/w2v-bert-2.0\n"
     ]
    }
   ],
   "source": [
    "print(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3.5: Define Data Collator for Padding (Handles None values)\n",
    "\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Union\n",
    "import logging # Add logging\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorAudio:\n",
    "    \"\"\"\n",
    "    Data collator that dynamically pads the inputs received Feature Extractor.\n",
    "    Handles None values returned by the Dataset on error.\n",
    "    \"\"\"\n",
    "    padding_value: float = 0.0 # Standard padding for features/audio\n",
    "\n",
    "    def __call__(self, features: List[Optional[Dict[str, Union[List[int], torch.Tensor]]]]) -> Dict[str, torch.Tensor]:\n",
    "        # features is a list of dicts OR None values from __getitem__\n",
    "\n",
    "        # --- Filter out None entries ---\n",
    "        valid_features = [f for f in features if f is not None]\n",
    "        if not valid_features:\n",
    "             # If all samples in the batch failed, return an empty dictionary\n",
    "             # The training loop should ideally handle this (e.g., skip batch)\n",
    "             logging.warning(\"Collate function received empty batch after filtering Nones.\")\n",
    "             return {}\n",
    "        # -----------------------------\n",
    "\n",
    "        # --- Determine keys and pad based on valid features ---\n",
    "        input_key = 'input_values' if 'input_values' in valid_features[0] else 'input_features'\n",
    "        input_features = [d[input_key] for d in valid_features]\n",
    "\n",
    "        # Determine sequence length dimension based on the FIRST valid tensor\n",
    "        seq_len_dim = -1\n",
    "        if len(input_features[0].shape) == 2:\n",
    "            seq_len_dim = 0 if input_features[0].shape[0] > input_features[0].shape[1] else -1\n",
    "        elif len(input_features[0].shape) == 1:\n",
    "             seq_len_dim = 0\n",
    "        else:\n",
    "             logging.warning(f\"Unexpected tensor shape {input_features[0].shape}, assuming seq len is last dim.\")\n",
    "\n",
    "        max_len = max(feat.shape[seq_len_dim] for feat in input_features)\n",
    "\n",
    "        # Pad each feature tensor to max_len\n",
    "        padded_features = []\n",
    "        for feat in input_features:\n",
    "            pad_width = max_len - feat.shape[seq_len_dim]\n",
    "            if seq_len_dim == 0 and len(feat.shape)==2: padding = (0, 0, 0, pad_width) # Pad SeqLen dim (dim 0)\n",
    "            else: padding = (0, pad_width) # Pad last dim (SeqLen)\n",
    "\n",
    "            padded_feat = torch.nn.functional.pad(feat, padding, mode='constant', value=self.padding_value)\n",
    "            padded_features.append(padded_feat)\n",
    "\n",
    "        # Stack the padded features\n",
    "        batch_input_features = torch.stack(padded_features)\n",
    "        batch = {input_key: batch_input_features} # Use the correct key\n",
    "\n",
    "        # Pad 'attention_mask' if present\n",
    "        if \"attention_mask\" in valid_features[0] and valid_features[0][\"attention_mask\"] is not None:\n",
    "            attention_masks = [d[\"attention_mask\"] for d in valid_features]\n",
    "            # Assuming mask is 1D [SeqLen] or 2D [1, SeqLen] etc. - pad last dim\n",
    "            max_mask_len = max(m.shape[-1] for m in attention_masks)\n",
    "            padded_masks = []\n",
    "            for mask in attention_masks:\n",
    "                 pad_width = max_mask_len - mask.shape[-1]\n",
    "                 padded_mask = torch.nn.functional.pad(mask, (0, pad_width), mode='constant', value=0)\n",
    "                 padded_masks.append(padded_mask)\n",
    "            batch[\"attention_mask\"] = torch.stack(padded_masks)\n",
    "\n",
    "        # Stack Labels\n",
    "        labels = [d[\"labels\"] for d in valid_features]\n",
    "        batch[\"labels\"] = torch.stack(labels)\n",
    "\n",
    "        return batch\n",
    "\n",
    "# Create an instance of the collator (do this in Cell 4)\n",
    "# data_collator = DataCollatorAudio()\n",
    "# print(\"DataCollatorAudio defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Load Feature Extractor, Create DataLoaders with Custom Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 15:10:22,648 - INFO - Loading feature extractor for: facebook/w2v-bert-2.0\n",
      "2025-05-04 15:10:22,779 - INFO - Feature extractor loaded successfully.\n",
      "Feature extractor expects sample rate: 16000\n",
      "2025-05-04 15:10:22,781 - INFO - Initializing FMARawAudioDataset from: /workspace/musicClaGen/data/processed/small_subset_multihot.csv\n",
      "2025-05-04 15:10:22,783 - INFO - Target sampling rate set from feature extractor: 16000 Hz\n",
      "2025-05-04 15:10:22,784 - INFO - Loading manifest from: /workspace/musicClaGen/data/processed/small_subset_multihot.csv\n",
      "2025-05-04 15:10:22,821 - INFO - Attempting to parse 'multi_hot_label' column using ast.literal_eval...\n",
      "2025-05-04 15:10:23,141 - INFO - Example parsed label verified (type <class 'list'>, length 22): [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "2025-05-04 15:10:23,142 - INFO - Loaded and parsed manifest with 8000 entries.\n",
      "2025-05-04 15:10:23,143 - INFO - Creating DEBUG DataLoaders with small subsets and custom collator...\n",
      "DataCollatorAudio instance created.\n",
      "2025-05-04 15:10:23,149 - INFO - DEBUG Dataset sizes: Train=16, Val=8\n",
      "2025-05-04 15:10:23,150 - INFO - DEBUG DataLoaders with custom collator created.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load Feature Extractor, Create DataLoaders with Custom Collator\n",
    "\n",
    "from transformers import AutoFeatureExtractor # Use the correct class\n",
    "\n",
    "# Ensure FMARawAudioDataset and DataCollatorAudio are defined in previous cells\n",
    "\n",
    "# --- Load Feature Extractor ---\n",
    "# (Using model_checkpoint defined in Cell 2)\n",
    "logging.info(f\"Loading feature extractor for: {model_checkpoint}\")\n",
    "try:\n",
    "    # Load the feature extractor associated with Wav2Vec2-BERT\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint)\n",
    "    logging.info(\"Feature extractor loaded successfully.\")\n",
    "    # Log the expected sample rate\n",
    "    processor_sr = feature_extractor.sampling_rate\n",
    "    print(f\"Feature extractor expects sample rate: {processor_sr}\")\n",
    "    # Ensure config matches extractor's expected rate\n",
    "    if config.PREPROCESSING_PARAMS['sample_rate'] != processor_sr:\n",
    "         logging.warning(f\"Config sample rate ({config.PREPROCESSING_PARAMS['sample_rate']}) differs from feature extractor ({processor_sr}). Ensure audio loading uses {processor_sr} Hz.\")\n",
    "         # Update config value if necessary, or ensure Dataset uses processor_sr\n",
    "         # config.PREPROCESSING_PARAMS['sample_rate'] = processor_sr # Be careful modifying config dynamically\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Could not load feature extractor for {model_checkpoint}. Cannot proceed. Error: {e}\", exc_info=True)\n",
    "    raise SystemExit # Stop execution if extractor fails\n",
    "\n",
    "# --- Create Full Dataset ---\n",
    "# Ensure FMARawAudioDataset __init__ accepts feature_extractor\n",
    "try:\n",
    "    # Pass the loaded feature_extractor instance\n",
    "    full_dataset = FMARawAudioDataset(manifest_path, feature_extractor=feature_extractor)\n",
    "    manifest_df = full_dataset.manifest\n",
    "except Exception as e:\n",
    "     logging.error(\"Failed to instantiate FMARawAudioDataset.\", exc_info=True)\n",
    "     raise SystemExit\n",
    "\n",
    "# --- Create SMALLER DEBUG Datasets ---\n",
    "logging.info(\"Creating DEBUG DataLoaders with small subsets and custom collator...\")\n",
    "try:\n",
    "    # Get indices for the splits from the manifest\n",
    "    train_indices = manifest_df[manifest_df['split'] == 'training'].index[:16].tolist() # Small subset for debug\n",
    "    val_indices = manifest_df[manifest_df['split'] == 'validation'].index[:8].tolist()  # Small subset for debug\n",
    "\n",
    "    # Create Subset instances\n",
    "    debug_train_dataset = Subset(full_dataset, train_indices)\n",
    "    debug_val_dataset = Subset(full_dataset, val_indices)\n",
    "\n",
    "    # --- Create Data Collator Instance ---\n",
    "    # (Assumes DataCollatorAudio class is defined in Cell 3.5)\n",
    "    data_collator = DataCollatorAudio()\n",
    "    print(\"DataCollatorAudio instance created.\")\n",
    "\n",
    "    # --- Create DataLoaders using the custom collate_fn ---\n",
    "    debug_train_dataloader = DataLoader(\n",
    "        debug_train_dataset,\n",
    "        batch_size=batch_size, # Use small batch_size from config\n",
    "        shuffle=True,\n",
    "        collate_fn=data_collator # Apply custom padding at batch level\n",
    "        # num_workers=4, # Optional: Add workers later for performance\n",
    "        # pin_memory=True # Optional: Add if using GPU\n",
    "    )\n",
    "    debug_val_dataloader = DataLoader(\n",
    "        debug_val_dataset,\n",
    "        batch_size=batch_size, # Use small batch_size from config\n",
    "        shuffle=False, # No need to shuffle validation data\n",
    "        collate_fn=data_collator # Apply custom padding at batch level\n",
    "        # num_workers=4,\n",
    "        # pin_memory=True\n",
    "    )\n",
    "    logging.info(f\"DEBUG Dataset sizes: Train={len(debug_train_dataset)}, Val={len(debug_val_dataset)}\")\n",
    "    logging.info(\"DEBUG DataLoaders with custom collator created.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to create DEBUG datasets/dataloaders: {e}\", exc_info=True)\n",
    "    raise SystemExit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5: Load Wav2Vec2-BERT Model and Modify Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 15:10:23,168 - INFO - Loading pre-trained Wav2Vec2-BERT model: facebook/w2v-bert-2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2BertForSequenceClassification were not initialized from the model checkpoint at facebook/w2v-bert-2.0 and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 15:10:25,708 - INFO - Model loaded initially.\n",
      "2025-05-04 15:10:25,709 - INFO - Found classifier attribute 'classifier' of type <class 'torch.nn.modules.linear.Linear'>\n",
      "2025-05-04 15:10:25,711 - INFO - Replacing classifier head 'classifier'. Original out: 22, New out: 22\n",
      "Successfully replaced classifier head 'classifier'.\n",
      "2025-05-04 15:10:26,725 - INFO - Wav2Vec2-BERT Model loaded and moved to device.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Load Wav2Vec2-BERT Model and Modify Head\n",
    "\n",
    "import torch.nn as nn # Ensure nn is imported\n",
    "from transformers import AutoModelForAudioClassification\n",
    "\n",
    "logging.info(f\"Loading pre-trained Wav2Vec2-BERT model: {model_checkpoint}\")\n",
    "try:\n",
    "    # Load the model configured for audio classification\n",
    "    model = AutoModelForAudioClassification.from_pretrained(\n",
    "        model_checkpoint,\n",
    "        num_labels=num_labels,\n",
    "        ignore_mismatched_sizes=True # Essential for replacing the head\n",
    "    )\n",
    "    logging.info(\"Model loaded initially.\")\n",
    "\n",
    "    # --- Explicit Head Replacement (Recommended) ---\n",
    "    # Though I have defined num_labels = num_labels on previous step, I want to explicitly replace it again to ensure the head is correct.\n",
    "    # If the above code is correct, the explicitly approach below might seem redundant but.\n",
    "    \n",
    "    # I MUST verify the correct attribute name for the classifier head for Wav2Vec2-BERT. \n",
    "    # Common names include 'classifier', 'projector','classification_head'. Use print(model) after loading to check.\n",
    "    classifier_attr = 'classifier' # <<<--- VERIFY THIS ATTRIBUTE NAME ---<<<\n",
    "\n",
    "    if hasattr(model, classifier_attr):\n",
    "        original_classifier = getattr(model, classifier_attr)\n",
    "        logging.info(f\"Found classifier attribute '{classifier_attr}' of type {type(original_classifier)}\")\n",
    "\n",
    "        # Check if it's a simple Linear layer or potentially a sequence/projection\n",
    "        if isinstance(original_classifier, nn.Linear):\n",
    "            in_features = original_classifier.in_features\n",
    "            logging.info(f\"Replacing classifier head '{classifier_attr}'. Original out: {original_classifier.out_features}, New out: {num_labels}\")\n",
    "            setattr(model, classifier_attr, nn.Linear(in_features, num_labels))\n",
    "            print(f\"Successfully replaced classifier head '{classifier_attr}'.\")\n",
    "        # Add checks here if Wav2Vec2-BERT uses a different common head structure\n",
    "        # elif isinstance(original_classifier, nn.Sequential): ... etc.\n",
    "        else:\n",
    "             logging.warning(f\"Classifier head '{classifier_attr}' is not nn.Linear ({type(original_classifier)}). Attempting replacement might fail or need adjustment.\")\n",
    "             # If you know the structure (e.g., model.projector + model.classifier), adjust accordingly.\n",
    "             # For now, we assume a direct replacement might work or the implicit loading handled it.\n",
    "\n",
    "    else:\n",
    "         logging.warning(f\"Could not automatically find classifier attribute '{classifier_attr}'. Ensure head size ({num_labels}) was correctly set via 'num_labels' argument during loading or modify manually.\")\n",
    "\n",
    "    model.to(device)\n",
    "    logging.info(\"Wav2Vec2-BERT Model loaded and moved to device.\")\n",
    "    # print(model) # Uncomment this line and run to inspect the model structure and find the classifier name\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to load model '{model_checkpoint}': {e}\", exc_info=True)\n",
    "    raise SystemExit # Stop if model loading fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the correct attribute name for the classifier head for Wav2Vec2-BERT.\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 15:10:26,763 - INFO - Saved model architecture details to /workspace/musicClaGen/models/model_info/model_architecture.json\n",
      "2025-05-04 15:10:26,765 - INFO - Model architecture saved to /workspace/musicClaGen/models/model_info\n"
     ]
    }
   ],
   "source": [
    "# Cell: Save Model Architecture Details\n",
    "\n",
    "# Create a function to capture model architecture details\n",
    "def save_model_architecture(model, base_dir):\n",
    "    \"\"\"Save model architecture details\"\"\"\n",
    "    architecture_info = {\n",
    "        \"model_type\": model.__class__.__name__,\n",
    "        \"parameter_count\": sum(p.numel() for p in model.parameters()),\n",
    "        \"trainable_parameter_count\": sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    }\n",
    "    \n",
    "    # Add config parameters if available\n",
    "    if hasattr(model, 'config'):\n",
    "        if hasattr(model.config, 'to_dict'):\n",
    "            architecture_info[\"config\"] = model.config.to_dict()\n",
    "        else:\n",
    "            # Try to convert config to dict\n",
    "            try:\n",
    "                architecture_info[\"config\"] = vars(model.config)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Save to file\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    with open(os.path.join(base_dir, \"model_architecture.json\"), 'w') as f:\n",
    "        json.dump(architecture_info, f, indent=2)\n",
    "    \n",
    "    logging.info(f\"Saved model architecture details to {os.path.join(base_dir, 'model_architecture.json')}\")\n",
    "    \n",
    "    return architecture_info\n",
    "\n",
    "# Save architecture info - safely handle whether logger exists or not\n",
    "model_info_dir = os.path.join(model_save_dir, \"model_info\")\n",
    "model_arch = save_model_architecture(model, model_info_dir)\n",
    "logging.info(f\"Model architecture saved to {model_info_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6: Define Optimizer, Loss Function, and Metrics Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 15:10:26,785 - INFO - Optimizer AdamW defined with LR=5e-05, Weight Decay=0.01\n",
      "2025-05-04 15:10:26,789 - INFO - Loss function BCEWithLogitsLoss defined.\n",
      "Optimizer, Loss, and compute_metrics function defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Define Optimizer, Loss Function, and Metrics Calculation\n",
    "\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import hamming_loss, jaccard_score, f1_score # Make sure these are imported\n",
    "\n",
    "# --- Optimizer ---\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "logging.info(f\"Optimizer AdamW defined with LR={learning_rate}, Weight Decay={weight_decay}\")\n",
    "\n",
    "# --- Loss Function ---\n",
    "# Use BCEWithLogitsLoss for multi-label classification (includes Sigmoid)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "logging.info(\"Loss function BCEWithLogitsLoss defined.\")\n",
    "\n",
    "# --- Metrics Function ---\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"Calculates multi-label metrics from logits and labels.\"\"\"\n",
    "    logits, labels = eval_preds\n",
    "    # Ensure inputs are numpy arrays on CPU\n",
    "    logits_np = logits.detach().cpu().numpy() if isinstance(logits, torch.Tensor) else logits\n",
    "    labels_np = labels.detach().cpu().numpy() if isinstance(labels, torch.Tensor) else labels\n",
    "\n",
    "    # Apply sigmoid and threshold\n",
    "    probs = 1 / (1 + np.exp(-logits_np)) # Manual sigmoid\n",
    "    preds = (probs > 0.5).astype(int)\n",
    "    labels_np = labels_np.astype(int) # Ensure labels are integers\n",
    "\n",
    "    if labels_np.shape != preds.shape:\n",
    "         logging.error(f\"Shape mismatch in compute_metrics! Labels: {labels_np.shape}, Preds: {preds.shape}\")\n",
    "         # Return default metrics indicating failure\n",
    "         return {'hamming_loss': 1.0, 'jaccard_samples': 0.0, 'f1_micro': 0.0, 'f1_macro': 0.0}\n",
    "\n",
    "    metrics = {}\n",
    "    try:\n",
    "        metrics['hamming_loss'] = hamming_loss(labels_np, preds)\n",
    "        # Use average='samples' for Jaccard in multi-label scenario\n",
    "        metrics['jaccard_samples'] = jaccard_score(labels_np, preds, average='samples', zero_division=0)\n",
    "        metrics['f1_micro'] = f1_score(labels_np, preds, average='micro', zero_division=0)\n",
    "        metrics['f1_macro'] = f1_score(labels_np, preds, average='macro', zero_division=0)\n",
    "        # Optional: Add Accuracy (subset accuracy)\n",
    "        # metrics['accuracy'] = accuracy_score(labels_np, preds) # This is exact match accuracy\n",
    "    except Exception as e:\n",
    "         logging.error(f\"Error calculating metrics: {e}\")\n",
    "         metrics = {'hamming_loss': 1.0, 'jaccard_samples': 0.0, 'f1_micro': 0.0, 'f1_macro': 0.0}\n",
    "\n",
    "    # Log inside the main evaluate function now for better context\n",
    "    # logging.info(f\"Metrics: Hamming={metrics['hamming_loss']:.4f}, Jaccard(samples)={metrics['jaccard_samples']:.4f}, F1 Micro={metrics['f1_micro']:.4f}, F1 Macro={metrics['f1_macro']:.4f}\")\n",
    "    return metrics\n",
    "\n",
    "print(\"Optimizer, Loss, and compute_metrics function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7: Define Training Function for One Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_epoch function updated to accept scheduler.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Define Training Function for One Epoch (with AMP and Scheduler)\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler # Or from torch.amp import ...\n",
    "\n",
    "# Ensure compute_metrics, torch, logging, tqdm etc. are imported\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, gradient_accumulation_steps, scaler, scheduler=None): # <<< Added scheduler=None\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_samples = 0\n",
    "    successful_steps = 0 # Counter for successful steps\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    num_batches = len(dataloader) # Get total batches for scheduler check\n",
    "\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        if batch is None or not batch: continue\n",
    "\n",
    "        try:\n",
    "            expected_model_input_key = \"input_features\" # VERIFY THIS KEY NAME\n",
    "            input_data_key = 'input_values' if 'input_values' in batch else 'input_features'\n",
    "            model_inputs = {expected_model_input_key: batch[input_data_key].to(device)}\n",
    "            if 'attention_mask' in batch and batch['attention_mask'] is not None: model_inputs['attention_mask'] = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            with autocast(device_type=device.type, enabled=(device.type=='cuda')): # Correct autocast usage\n",
    "                outputs = model(**model_inputs)\n",
    "                logits = outputs.logits\n",
    "                loss = criterion(logits, labels)\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                logging.warning(f\"NaN loss detected at step {step}. Skipping batch.\")\n",
    "                if (step + 1) % gradient_accumulation_steps != 0: optimizer.zero_grad()\n",
    "                continue\n",
    "\n",
    "            scaled_loss = loss / gradient_accumulation_steps\n",
    "            scaler.scale(scaled_loss).backward()\n",
    "\n",
    "            batch_size_actual = labels.size(0)\n",
    "            total_loss += loss.item() * batch_size_actual\n",
    "            num_samples += batch_size_actual\n",
    "\n",
    "            if (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == num_batches:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                # --- Step the scheduler AFTER the optimizer step ---\n",
    "                if scheduler:\n",
    "                    scheduler.step() # <<<--- ADDED SCHEDULER STEP HERE\n",
    "                # -------------------------------------------------\n",
    "                optimizer.zero_grad()\n",
    "                successful_steps +=1 # Count successful optimizer steps\n",
    "\n",
    "\n",
    "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}', 'lr': f'{optimizer.param_groups[0][\"lr\"]:.2e}'}) # Optionally show LR\n",
    "\n",
    "        except Exception as e:\n",
    "             logging.error(f\"Error during training step {step}: {e}\", exc_info=True)\n",
    "             optimizer.zero_grad() # Zero grad on error too\n",
    "             continue\n",
    "\n",
    "    # Final optimizer step might not be needed if scheduler steps correctly, depends on exact logic.\n",
    "    # Let's remove the extra step outside the loop for now.\n",
    "\n",
    "    avg_loss = total_loss / num_samples if num_samples > 0 else 0\n",
    "    print(f\"\\nCompleted training epoch. Successful optimizer steps: {successful_steps}\")\n",
    "    print(f\"Average Training Loss for Epoch: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "print(\"train_epoch function updated to accept scheduler.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Define Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 8: Define Evaluation Function (Corrected Model Input)\n",
    "\n",
    "# def evaluate(model, dataloader, criterion, device):\n",
    "#     model.eval()\n",
    "#     total_loss = 0\n",
    "#     all_logits = []\n",
    "#     all_labels = []\n",
    "#     num_samples = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for step, batch in enumerate(tqdm(dataloader, desc=\"Evaluating\", leave=False)):\n",
    "#             try:\n",
    "#                 # --- CORRECTED INPUT PREPARATION ---\n",
    "#                 expected_model_input_key = \"input_features\" # <<<--- VERIFY THIS KEY NAME\n",
    "\n",
    "#                 if 'input_values' not in batch:\n",
    "#                      raise KeyError(\"Batch dictionary missing 'input_values' from Dataset/Extractor.\")\n",
    "\n",
    "#                 model_inputs = {\n",
    "#                     expected_model_input_key: batch['input_values'].to(device)\n",
    "#                 }\n",
    "#                 if 'attention_mask' in batch and batch['attention_mask'] is not None:\n",
    "#                      model_inputs['attention_mask'] = batch['attention_mask'].to(device)\n",
    "#                 # --- END CORRECTION ---\n",
    "\n",
    "#                 labels = batch['labels'].to(device)\n",
    "\n",
    "#                 # Forward pass\n",
    "#                 outputs = model(**model_inputs) # Pass the correctly named arguments\n",
    "#                 logits = outputs.logits\n",
    "\n",
    "#                 # Calculate loss\n",
    "#                 loss = criterion(logits, labels)\n",
    "#                 total_loss += loss.item() * labels.size(0)\n",
    "#                 num_samples += labels.size(0)\n",
    "\n",
    "#                 all_logits.append(logits.cpu())\n",
    "#                 all_labels.append(labels.cpu())\n",
    "#             except Exception as e:\n",
    "#                  logging.error(f\"Error during evaluation step {step}, batch keys: {batch.keys()}. Error: {e}\", exc_info=True)\n",
    "#                  continue # Skip batch\n",
    "\n",
    "#     if not all_logits or not all_labels or num_samples == 0:\n",
    "#         logging.warning(\"Evaluation yielded no results (all batches failed or empty dataloader?).\")\n",
    "#         return {}\n",
    "\n",
    "#     avg_loss = total_loss / num_samples\n",
    "\n",
    "#     all_logits_cat = torch.cat(all_logits, dim=0)\n",
    "#     all_labels_cat = torch.cat(all_labels, dim=0)\n",
    "\n",
    "#     eval_preds = (all_logits_cat, all_labels_cat)\n",
    "#     metrics = compute_metrics(eval_preds)\n",
    "#     metrics['eval_loss'] = avg_loss\n",
    "\n",
    "#     print(f\"\\nValidation Loss: {avg_loss:.4f}\")\n",
    "#     for name, value in metrics.items():\n",
    "#          if name != 'eval_loss': print(f\"  Validation {name.replace('_', ' ').title()}: {value:.4f}\")\n",
    "\n",
    "#     return metrics\n",
    "\n",
    "# print(\"evaluate function updated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 8: Define Evaluation Function (with AMP)\n",
    "\n",
    "# # Ensure compute_metrics function is defined in a previous cell\n",
    "# # Ensure torch, logging, tqdm, np are imported\n",
    "\n",
    "# def evaluate(model, dataloader, criterion, device):\n",
    "#     model.eval() # Set model to evaluation mode\n",
    "#     total_loss = 0\n",
    "#     all_logits = []\n",
    "#     all_labels = []\n",
    "#     num_samples = 0\n",
    "\n",
    "#     with torch.no_grad(): # Disable gradient calculations\n",
    "#         for step, batch in enumerate(tqdm(dataloader, desc=\"Evaluating\", leave=False)):\n",
    "#             if batch is None or not batch: continue\n",
    "#             try:\n",
    "#                 # Prepare inputs\n",
    "#                 expected_model_input_key = \"input_features\" # VERIFY THIS KEY NAME\n",
    "#                 input_data_key = 'input_values' if 'input_values' in batch else 'input_features'\n",
    "\n",
    "#                 model_inputs = {}\n",
    "#                 if input_data_key in batch:\n",
    "#                     model_inputs[expected_model_input_key] = batch[input_data_key].to(device)\n",
    "#                 else:\n",
    "#                     raise KeyError(f\"Required input key not found in batch during evaluation.\")\n",
    "\n",
    "#                 if 'attention_mask' in batch and batch['attention_mask'] is not None:\n",
    "#                      model_inputs['attention_mask'] = batch['attention_mask'].to(device)\n",
    "\n",
    "#                 labels = batch['labels'].to(device)\n",
    "\n",
    "#                 # --- Use autocast for forward pass during evaluation ---\n",
    "#                 # Although not strictly needed for memory unless inputs are huge,\n",
    "#                 # it ensures consistency with training pass calculations.\n",
    "#                 with autocast(device_type=device.type):\n",
    "#                     outputs = model(**model_inputs)\n",
    "#                     logits = outputs.logits\n",
    "#                     loss = criterion(logits, labels)\n",
    "#                 # ----------------------------------------------------\n",
    "\n",
    "#                 total_loss += loss.item() * labels.size(0)\n",
    "#                 num_samples += labels.size(0)\n",
    "\n",
    "#                 all_logits.append(logits.cpu()) # Store logits on CPU\n",
    "#                 all_labels.append(labels.cpu()) # Store labels on CPU\n",
    "#             except Exception as e:\n",
    "#                  logging.error(f\"Error during evaluation step {step}: {e}\", exc_info=True)\n",
    "#                  continue # Skip batch on error\n",
    "\n",
    "#     if not all_logits or not all_labels or num_samples == 0:\n",
    "#         logging.warning(\"Evaluation yielded no results.\")\n",
    "#         return {}\n",
    "\n",
    "#     # Calculate average loss over processed samples\n",
    "#     avg_loss = total_loss / num_samples\n",
    "\n",
    "#     # Concatenate results from all batches\n",
    "#     all_logits_cat = torch.cat(all_logits, dim=0)\n",
    "#     all_labels_cat = torch.cat(all_labels, dim=0)\n",
    "\n",
    "#     # Calculate metrics using the helper function\n",
    "#     eval_preds = (all_logits_cat, all_labels_cat) # Pass tensors directly\n",
    "#     metrics = compute_metrics(eval_preds)\n",
    "#     metrics['eval_loss'] = avg_loss\n",
    "\n",
    "#     # Log metrics\n",
    "#     print(f\"\\nValidation Loss: {avg_loss:.4f}\")\n",
    "#     for name, value in metrics.items():\n",
    "#          if name != 'eval_loss': print(f\"  Validation {name.replace('_', ' ').title()}: {value:.4f}\")\n",
    "\n",
    "#     return metrics # Return dictionary of all metrics\n",
    "\n",
    "# print(\"evaluate function defined with AMP (autocast only).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Enhanced Evaluation Function with Threshold Testing\n",
    "from torch.cuda.amp import autocast  # Add this import for mixed precision\n",
    "\n",
    "def evaluate_with_thresholds(model, dataloader, criterion, device, thresholds=[0.05, 0.1, 0.2, 0.3, 0.4, 0.5]):\n",
    "    \"\"\"Evaluate model with multiple thresholds to find the optimal one\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculations\n",
    "        for step, batch in enumerate(tqdm(dataloader, desc=\"Evaluating\", leave=False)):\n",
    "            if batch is None or not batch: continue\n",
    "            try:\n",
    "                # Prepare inputs - handle the parameter name mapping\n",
    "                input_data = batch.get('input_values', batch.get('input_features')).to(device)\n",
    "                attention_mask = batch.get('attention_mask', None)\n",
    "                if attention_mask is not None:\n",
    "                    attention_mask = attention_mask.to(device)\n",
    "                labels = batch.get('labels').to(device)\n",
    "                \n",
    "                # Create model input dict with the correct parameter name\n",
    "                model_inputs = {\n",
    "                    # Use input_features instead of input_values for Wav2Vec2-BERT\n",
    "                    'input_features': input_data,\n",
    "                    'attention_mask': attention_mask if attention_mask is not None else None\n",
    "                }\n",
    "                \n",
    "                # Forward pass with autocast\n",
    "                with autocast(device_type=device.type):\n",
    "                    outputs = model(**model_inputs)\n",
    "                    logits = outputs.logits\n",
    "                    loss = criterion(logits, labels)\n",
    "                \n",
    "                total_loss += loss.item() * labels.size(0)\n",
    "                num_samples += labels.size(0)\n",
    "                \n",
    "                # Store logits and labels for metric calculation\n",
    "                all_logits.append(logits.cpu())\n",
    "                all_labels.append(labels.cpu())\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error during evaluation step {step}: {e}\", exc_info=True)\n",
    "                continue\n",
    "    \n",
    "    if not all_logits or not all_labels or num_samples == 0:\n",
    "        logging.warning(\"Evaluation yielded no results.\")\n",
    "        return {}\n",
    "    \n",
    "    # Calculate average loss\n",
    "    avg_loss = total_loss / num_samples\n",
    "    \n",
    "    # Concatenate results from all batches\n",
    "    all_logits_cat = torch.cat(all_logits, dim=0)\n",
    "    all_labels_cat = torch.cat(all_labels, dim=0)\n",
    "    \n",
    "    # Convert to numpy\n",
    "    logits_np = all_logits_cat.numpy()\n",
    "    labels_np = all_labels_cat.numpy()\n",
    "    \n",
    "    # Apply sigmoid to get probabilities\n",
    "    probs = 1 / (1 + np.exp(-logits_np))  # Manual sigmoid, same as in your compute_metrics\n",
    "    \n",
    "    # Add diagnostic information about predictions\n",
    "    print(f\"Prediction stats - Min: {probs.min():.4f}, Max: {probs.max():.4f}, Mean: {probs.mean():.4f}\")\n",
    "    print(f\"Prediction histogram: {np.histogram(probs.flatten(), bins=10, range=(0,1))[0]}\")\n",
    "    \n",
    "    # Display label distribution\n",
    "    label_counts = np.sum(labels_np, axis=0)\n",
    "    print(f\"Label distribution: min={label_counts.min()}, max={label_counts.max()}, mean={label_counts.mean():.1f}\")\n",
    "    \n",
    "    # Test multiple thresholds to find the optimal one\n",
    "    metrics = {'eval_loss': avg_loss}\n",
    "    threshold_metrics = {}\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0.5  # Default\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        # Apply threshold\n",
    "        preds = (probs > threshold).astype(int)\n",
    "        \n",
    "        try:\n",
    "            # Calculate metrics for this threshold\n",
    "            ham = hamming_loss(labels_np, preds)\n",
    "            jac = jaccard_score(labels_np, preds, average='samples', zero_division=0)\n",
    "            f1_mic = f1_score(labels_np, preds, average='micro', zero_division=0)\n",
    "            f1_mac = f1_score(labels_np, preds, average='macro', zero_division=0)\n",
    "            \n",
    "            threshold_metrics[threshold] = {\n",
    "                'hamming_loss': ham,\n",
    "                'jaccard_samples': jac,\n",
    "                'f1_micro': f1_mic,\n",
    "                'f1_macro': f1_mac\n",
    "            }\n",
    "            \n",
    "            print(f\"Threshold {threshold}: Hamming={ham:.4f}, F1-micro={f1_mic:.4f}, F1-macro={f1_mac:.4f}\")\n",
    "            \n",
    "            # Track best threshold based on micro F1\n",
    "            if f1_mic > best_f1:\n",
    "                best_f1 = f1_mic\n",
    "                best_threshold = threshold\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error calculating metrics with threshold {threshold}: {e}\")\n",
    "    \n",
    "    print(f\"\\nBest threshold: {best_threshold} (F1-micro: {best_f1:.4f})\")\n",
    "    \n",
    "    # Use best threshold for final metrics\n",
    "    best_preds = (probs > best_threshold).astype(int)\n",
    "    metrics['hamming_loss'] = hamming_loss(labels_np, best_preds)\n",
    "    metrics['jaccard_samples'] = jaccard_score(labels_np, best_preds, average='samples', zero_division=0)\n",
    "    metrics['f1_micro'] = f1_score(labels_np, best_preds, average='micro', zero_division=0)\n",
    "    metrics['f1_macro'] = f1_score(labels_np, best_preds, average='macro', zero_division=0)\n",
    "    metrics['best_threshold'] = best_threshold\n",
    "    \n",
    "    # Don't include the full threshold_metrics dictionary in the returned metrics\n",
    "    # This prevents formatting errors when printing the metrics\n",
    "    # If you need this data, access it separately\n",
    "    # metrics['threshold_metrics'] = threshold_metrics\n",
    "    \n",
    "    # Log metrics\n",
    "    print(f\"\\nValidation Loss: {avg_loss:.4f}\")\n",
    "    for name, value in metrics.items():\n",
    "        if name != 'eval_loss':  # Already printed eval_loss above\n",
    "            try:\n",
    "                print(f\"  Validation {name.replace('_', ' ').title()}: {value:.4f}\")\n",
    "            except (TypeError, ValueError):\n",
    "                print(f\"  Validation {name.replace('_', ' ').title()}: {value}\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. One Trial Epoch for Debugging Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory before dataloader setup: 7.00 GB\n",
      "\n",
      "--- Starting Debug Training Run for 1 epoch (with AMP) ---\n",
      "\n",
      "--- Debug Epoch 1/1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed training epoch. Successful optimizer steps: 2\n",
      "Average Training Loss for Epoch: 0.5593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction stats - Min: 0.4116, Max: 0.5376, Mean: 0.4470\n",
      "Prediction histogram: [  0   0   0   0 168   8   0   0   0   0]\n",
      "Label distribution: min=0.0, max=4.0, mean=0.5\n",
      "Threshold 0.05: Hamming=0.9432, F1-micro=0.1075, F1-macro=0.0869\n",
      "Threshold 0.1: Hamming=0.9432, F1-micro=0.1075, F1-macro=0.0869\n",
      "Threshold 0.2: Hamming=0.9432, F1-micro=0.1075, F1-macro=0.0869\n",
      "Threshold 0.3: Hamming=0.9432, F1-micro=0.1075, F1-macro=0.0869\n",
      "Threshold 0.4: Hamming=0.9432, F1-micro=0.1075, F1-macro=0.0869\n",
      "Threshold 0.5: Hamming=0.0795, F1-micro=0.2222, F1-macro=0.0182\n",
      "\n",
      "Best threshold: 0.5 (F1-micro: 0.2222)\n",
      "\n",
      "Validation Loss: 0.6024\n",
      "  Validation Hamming Loss: 0.0795\n",
      "  Validation Jaccard Samples: 0.2500\n",
      "  Validation F1 Micro: 0.2222\n",
      "  Validation F1 Macro: 0.0182\n",
      "\n",
      "Debug Epoch 1 finished.\n",
      "  Avg Train Loss: 0.5593\n",
      "  Validation Eval Loss: 0.6024\n",
      "  Validation Hamming Loss: 0.0795\n",
      "  Validation Jaccard Samples: 0.2500\n",
      "  Validation F1 Micro: 0.2222\n",
      "  Validation F1 Macro: 0.0182\n",
      "  Validation Best Threshold: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to dict.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 60\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eval_metrics:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m eval_metrics\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 60\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Validation \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtitle()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Validation failed to produce metrics.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to dict.__format__"
     ]
    }
   ],
   "source": [
    "# Cell 9: Debug Training Run with Improved Evaluation and AMP\n",
    "\n",
    "# Import torch amp components for mixed precision training\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, grad_accum_steps=1, scaler=None, scheduler=None):\n",
    "    \"\"\"Train model for one epoch using gradient accumulation for larger effective batches\"\"\"\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    num_batches_processed = 0\n",
    "    optimizer_steps = 0\n",
    "    \n",
    "    # Zero the gradients at the beginning of epoch\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Process each batch\n",
    "    for step, batch in enumerate(tqdm(dataloader, desc=\"Training\", leave=False)):\n",
    "        # Skip empty or malformed batches\n",
    "        if batch is None or not batch:\n",
    "            continue\n",
    "            \n",
    "        # Process batch with gradient accumulation for efficiency\n",
    "        # Only backward + optimize every grad_accum_steps or at the last batch\n",
    "        do_optimizer_step = ((step + 1) % grad_accum_steps == 0) or (step == len(dataloader) - 1)\n",
    "        \n",
    "        try:\n",
    "            # Prepare batch inputs and move to device\n",
    "            input_values = batch.get('input_values', batch.get('input_features')).to(device)\n",
    "            attention_mask = batch.get('attention_mask', None)\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = attention_mask.to(device)\n",
    "            labels = batch.get('labels').to(device)\n",
    "            \n",
    "            # Get model parameter name requirements (for different HF models)\n",
    "            model_inputs = {\n",
    "                'input_features': input_values,  # Use 'input_features' for Wav2Vec2-BERT\n",
    "                'attention_mask': attention_mask if attention_mask is not None else None\n",
    "            }\n",
    "            \n",
    "            # --- Forward pass with autocast for mixed precision ---\n",
    "            with autocast(device_type=device.type, enabled=(scaler is not None)):\n",
    "                outputs = model(**model_inputs)\n",
    "                loss = criterion(outputs.logits, labels)\n",
    "                \n",
    "                # Scale loss by gradient accumulation steps\n",
    "                loss = loss / grad_accum_steps\n",
    "            \n",
    "            # --- Backward pass with scaler ---\n",
    "            if scaler is not None:\n",
    "                scaler.scale(loss).backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                \n",
    "            # --- Optimizer step if needed ---\n",
    "            if do_optimizer_step:\n",
    "                if scaler is not None:\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    optimizer.step()\n",
    "                \n",
    "                # Apply scheduler step\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "                \n",
    "                # Zero the gradients\n",
    "                optimizer.zero_grad()  \n",
    "                optimizer_steps += 1\n",
    "                \n",
    "            # Accumulate loss statistics (use the pre-scaled loss for reporting)\n",
    "            epoch_loss += loss.item() * grad_accum_steps\n",
    "            num_batches_processed += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in training batch {step}: {e}\")\n",
    "            continue  # Skip problematic batch\n",
    "    \n",
    "    # Calculate average loss for the epoch\n",
    "    avg_loss = epoch_loss / num_batches_processed if num_batches_processed > 0 else float('inf')\n",
    "    \n",
    "    # Log results\n",
    "    print(f\"\\nCompleted training epoch. Successful optimizer steps: {optimizer_steps}\")\n",
    "    print(f\"Average Training Loss for Epoch: {avg_loss:.4f}\")\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "# Check memory before initializing dataloaders\n",
    "print(f\"GPU memory before dataloader setup: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "# --- Debug Training Run ---\n",
    "print(\"\\n--- Starting Debug Training Run for 1 epoch (with AMP) ---\")\n",
    "num_epochs = num_epochs_debug\n",
    "\n",
    "# Initialize GradScaler for AMP\n",
    "scaler = GradScaler(enabled=(device.type == 'cuda'))\n",
    "\n",
    "# Save model checkpoint after each debug epoch\n",
    "debug_checkpoint_path = os.path.join(model_save_dir, f\"{model_checkpoint.replace('/', '_')}_debug_AMP\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize any metrics tracking\n",
    "epoch_metrics = []\n",
    "\n",
    "# Run training loop for specified number of debug epochs\n",
    "for epoch in range(num_epochs):\n",
    "    logging.info(f\"DEBUG Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    # Run one training epoch (with gradient accumulation and AMP)\n",
    "    train_loss = train_epoch(\n",
    "        model,\n",
    "        debug_train_dataloader,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        device,\n",
    "        gradient_accumulation_steps,\n",
    "        scaler  # Pass the scaler object (created with new API)\n",
    "    )\n",
    "    \n",
    "    # Run evaluation step with the new evaluation function\n",
    "    eval_metrics = evaluate_with_thresholds(\n",
    "        model,\n",
    "        debug_val_dataloader,\n",
    "        criterion,\n",
    "        device\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nDebug Epoch {epoch+1} finished.\")\n",
    "    print(f\"  Avg Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # Modified printing to handle potential dictionaries in metrics\n",
    "    if eval_metrics:\n",
    "        for name, value in eval_metrics.items():\n",
    "            if name not in ['threshold_metrics']:  # Skip dictionary values \n",
    "                try:\n",
    "                    print(f\"  Validation {name.replace('_', ' ').title()}: {value:.4f}\")\n",
    "                except (TypeError, ValueError):\n",
    "                    print(f\"  Validation {name.replace('_', ' ').title()}: {value}\")\n",
    "    else:\n",
    "        print(\"  Validation failed to produce metrics.\")\n",
    "    \n",
    "    # Save checkpoint for this epoch\n",
    "    epoch_save_path = f\"{debug_checkpoint_path}_epoch_{epoch+1}.pth\"\n",
    "    torch.save(model.state_dict(), epoch_save_path)\n",
    "    logging.info(f\"Saved debug model checkpoint to {epoch_save_path}\")\n",
    "    \n",
    "    # Add metrics to tracking\n",
    "    epoch_metrics.append({\n",
    "        'epoch': epoch+1,\n",
    "        'train_loss': train_loss,\n",
    "        'eval_metrics': eval_metrics,\n",
    "    })\n",
    "\n",
    "# Report training metrics over all epochs\n",
    "debug_duration = time.time() - start_time\n",
    "print(f\"\\n--- Debug Run Finished in {debug_duration:.2f} seconds ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial debug training run worked! Now let's try the full training run.\n",
    "\n",
    "# 10. Set Up DataLoaders for FULL Splits & LR Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 04:27:37,443 - INFO - Loading feature extractor for: facebook/w2v-bert-2.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 04:27:37,534 - INFO - Feature extractor loaded successfully.\n",
      "Feature extractor expects sample rate: 16000\n",
      "2025-05-04 04:27:37,536 - INFO - Initializing FMARawAudioDataset from: /workspace/musicClaGen/data/processed/small_subset_multihot.csv\n",
      "2025-05-04 04:27:37,538 - INFO - Target sampling rate set from feature extractor: 16000 Hz\n",
      "2025-05-04 04:27:37,539 - INFO - Loading manifest from: /workspace/musicClaGen/data/processed/small_subset_multihot.csv\n",
      "2025-05-04 04:27:37,572 - INFO - Attempting to parse 'multi_hot_label' column using ast.literal_eval...\n",
      "2025-05-04 04:27:37,893 - INFO - Example parsed label verified (type <class 'list'>, length 22): [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "2025-05-04 04:27:37,895 - INFO - Loaded and parsed manifest with 8000 entries.\n",
      "2025-05-04 04:27:37,897 - INFO - Creating DataLoaders with FULL splits and custom collator...\n",
      "DataCollatorAudio instance created.\n",
      "2025-05-04 04:27:37,904 - INFO - Batch size: 2, Grad Accum Steps: 4, Effective BS: 8\n",
      "2025-05-04 04:27:37,905 - INFO - FULL Dataset sizes: Train=6400, Val=800, Test=800\n",
      "2025-05-04 04:27:37,907 - INFO - FULL DataLoaders with custom collator created.\n",
      "2025-05-04 04:27:37,908 - INFO - LR Scheduler created. Total optimization steps: 6400\n",
      "\n",
      "Setup for full training run complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Setup DataLoaders for FULL Splits & LR Scheduler\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup # Import scheduler\n",
    "\n",
    "# --- Ensure Feature Extractor is Loaded ---\n",
    "# (Code from previous Cell 4 - necessary if kernel restarted)\n",
    "logging.info(f\"Loading feature extractor for: {model_checkpoint}\")\n",
    "try:\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint)\n",
    "    logging.info(\"Feature extractor loaded successfully.\")\n",
    "    target_sr = feature_extractor.sampling_rate\n",
    "    print(f\"Feature extractor expects sample rate: {target_sr}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Could not load feature extractor. Error: {e}\", exc_info=True)\n",
    "    raise SystemExit\n",
    "\n",
    "# --- Create Full Dataset instance ---\n",
    "try:\n",
    "    full_dataset = FMARawAudioDataset(manifest_path, feature_extractor=feature_extractor)\n",
    "    manifest_df = full_dataset.manifest\n",
    "except Exception as e:\n",
    "     logging.error(\"Failed to instantiate FMARawAudioDataset.\", exc_info=True)\n",
    "     raise SystemExit\n",
    "\n",
    "# --- Create FULL Datasets for Train/Val/Test ---\n",
    "logging.info(\"Creating DataLoaders with FULL splits and custom collator...\")\n",
    "try:\n",
    "    # Get indices for the splits from the manifest\n",
    "    train_indices = manifest_df[manifest_df['split'] == 'training'].index.tolist()\n",
    "    val_indices = manifest_df[manifest_df['split'] == 'validation'].index.tolist()\n",
    "    test_indices = manifest_df[manifest_df['split'] == 'test'].index.tolist() # Get test indices too\n",
    "\n",
    "    # Create Subset instances using the FULL index lists\n",
    "    train_dataset = Subset(full_dataset, train_indices)\n",
    "    val_dataset = Subset(full_dataset, val_indices)\n",
    "    test_dataset = Subset(full_dataset, test_indices) # Create test dataset\n",
    "\n",
    "    # --- Create Data Collator Instance ---\n",
    "    data_collator = DataCollatorAudio()\n",
    "    print(\"DataCollatorAudio instance created.\")\n",
    "\n",
    "    # --- Create DataLoaders ---\n",
    "    # Use actual batch_size from config\n",
    "    effective_batch_size = config.MODEL_PARAMS[\"batch_size\"] * config.MODEL_PARAMS[\"gradient_accumulation_steps\"]\n",
    "    logging.info(f\"Batch size: {config.MODEL_PARAMS['batch_size']}, Grad Accum Steps: {config.MODEL_PARAMS['gradient_accumulation_steps']}, Effective BS: {effective_batch_size}\")\n",
    "\n",
    "    # Use num_workers for faster loading (adjust based on instance cores)\n",
    "    num_workers = 4 if os.name == 'posix' else 0\n",
    "    pin_memory = True if device.type == 'cuda' else False\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, batch_size=config.MODEL_PARAMS[\"batch_size\"], shuffle=True,\n",
    "        collate_fn=data_collator, num_workers=num_workers, pin_memory=pin_memory, persistent_workers=(num_workers>0)\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset, batch_size=config.MODEL_PARAMS[\"batch_size\"], shuffle=False,\n",
    "        collate_fn=data_collator, num_workers=num_workers, pin_memory=pin_memory, persistent_workers=(num_workers>0)\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset, batch_size=config.MODEL_PARAMS[\"batch_size\"], shuffle=False,\n",
    "        collate_fn=data_collator, num_workers=num_workers, pin_memory=pin_memory, persistent_workers=(num_workers>0)\n",
    "    )\n",
    "    logging.info(f\"FULL Dataset sizes: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n",
    "    logging.info(\"FULL DataLoaders with custom collator created.\")\n",
    "\n",
    "    # --- Setup LR Scheduler ---\n",
    "    num_epochs = config.MODEL_PARAMS[\"epochs\"]\n",
    "    num_training_steps = (len(train_dataloader) // config.MODEL_PARAMS[\"gradient_accumulation_steps\"]) * num_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "         optimizer, # Optimizer defined in Cell 6\n",
    "         num_warmup_steps=0, # You can add warmup steps if desired (e.g., 10% of total steps)\n",
    "         num_training_steps=num_training_steps\n",
    "    )\n",
    "    logging.info(f\"LR Scheduler created. Total optimization steps: {num_training_steps}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to create datasets/dataloaders: {e}\", exc_info=True)\n",
    "    raise SystemExit\n",
    "\n",
    "print(\"\\nSetup for full training run complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.Full training run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 11: Run Full Training Loop\n",
    "\n",
    "# # Clear CUDA cache and force garbage collection\n",
    "# import gc\n",
    "# import torch\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()\n",
    "\n",
    "# # Check memory usage before training\n",
    "# print(f\"GPU memory allocated before training: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "# print(f\"GPU memory reserved before training: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "\n",
    "\n",
    "# # Make sure model, criterion, optimizer, scheduler, dataloaders defined from previous cells\n",
    "# num_epochs = config.MODEL_PARAMS[\"epochs\"] # Get actual epochs from config\n",
    "# gradient_accumulation_steps = config.MODEL_PARAMS[\"gradient_accumulation_steps\"]\n",
    "# metric_to_monitor = 'hamming_loss' # Metric to decide best model (lower is better)\n",
    "# best_val_metric = float('inf')\n",
    "\n",
    "# # --- Initialize GradScaler for AMP ---\n",
    "# scaler = GradScaler(enabled=(device.type == 'cuda'))\n",
    "# # ------------------------------------\n",
    "\n",
    "# logging.info(f\"--- Starting FULL Training for {num_epochs} epochs ---\")\n",
    "# start_time = time.time()\n",
    "\n",
    "# # Make sure model and criterion are on the correct device\n",
    "# model.to(device)\n",
    "# criterion.to(device)\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     epoch_start_time = time.time()\n",
    "#     logging.info(f\"\\n--- Epoch {epoch+1}/{num_epochs} ---\")\n",
    "\n",
    "#     # Run training for one epoch\n",
    "#     train_loss = train_epoch(\n",
    "#         model, train_dataloader, criterion, optimizer, device,\n",
    "#         gradient_accumulation_steps, scaler, scheduler # Pass scaler and scheduler\n",
    "#     )\n",
    "\n",
    "#     # Run evaluation on validation set\n",
    "#     eval_metrics = evaluate(model, val_dataloader, criterion, device)\n",
    "\n",
    "#     print(f\"\\nEpoch {epoch+1} finished.\")\n",
    "#     print(f\"  Avg Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "#     if not eval_metrics:\n",
    "#         logging.warning(f\"Epoch {epoch+1}: Evaluation failed, skipping checkpoint.\")\n",
    "#         continue\n",
    "\n",
    "#     # Log all validation metrics\n",
    "#     for name, value in eval_metrics.items():\n",
    "#         print(f\"  Validation {name.replace('_', ' ').title()}: {value:.4f}\")\n",
    "\n",
    "#     # Save model checkpoint if validation metric improved\n",
    "#     current_val_metric = eval_metrics.get(metric_to_monitor, float('inf'))\n",
    "#     if current_val_metric < best_val_metric:\n",
    "#         best_val_metric = current_val_metric\n",
    "#         # Use a consistent name for the best model checkpoint\n",
    "#         save_path = os.path.join(model_save_dir, f\"{model_checkpoint.replace('/', '_')}_finetuned_best.pth\")\n",
    "#         try:\n",
    "#             torch.save(model.state_dict(), save_path)\n",
    "#             logging.info(f\"Validation metric improved ({metric_to_monitor}={current_val_metric:.4f}). Saved best model to {save_path}\")\n",
    "#         except Exception as e:\n",
    "#             logging.error(f\"Failed to save model checkpoint: {e}\", exc_info=True)\n",
    "#     else:\n",
    "#          logging.info(f\"Validation metric did not improve ({metric_to_monitor}={current_val_metric:.4f}). Best: {best_val_metric:.4f}\")\n",
    "\n",
    "#     epoch_duration = time.time() - epoch_start_time\n",
    "#     logging.info(f\"Epoch {epoch+1} finished in {epoch_duration / 60:.2f} minutes.\")\n",
    "\n",
    "# total_training_time = time.time() - start_time\n",
    "# logging.info(f\"--- Training Finished in {total_training_time / 60:.2f} minutes ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "• Time: Took ~47 minutes for 1 epoch on the full `fma_small` training set (~6400 samples). This is a realistic time given the model size, 30s inputs, data loading, and AMP.\n",
    "\n",
    "• Errors During Training: The log shows several errors during the training loop:\n",
    "\n",
    "  • `ERROR - Error loading/processing track ...`\n",
    "  \n",
    "  `audioread.exceptions.NoBackendError`: This error occurred multiple times (tracks 133297, 99134, 98569, 98567, 98565, 108925). It indicates `librosa.load` failed. It first tries `soundfile` (which fails often with MP3s, sometimes due to file existence/permissions or internal errors), then falls back to `audioread`, which then fails because no suitable backend (like `ffmpeg`) was found or successfully used by `audioread`. This is despite installing `ffmpeg` earlier. It suggests `librosa`'s fallback mechanism isn't working reliably in this environment.\n",
    "\n",
    "  • `[src/libmpg123/...]: warning: Cannot read next header...`, `error: dequantization failed!`, `error: part2_3_length ... too large...`, `error: Giving up resync...`: These are lower-level MP3 decoding errors from the `mpg123` library, likely called by `audioread` or another backend. They indicate corrupted or non-standard MP3 files.\n",
    "\n",
    "\n",
    "When checked the documentation on fma github(https://github.com/mdeff/fma/wiki), these track IDs are flawed indeed, so everything is fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Enhanced Training Loop with Logger and Selected Checkpoints\n",
    "\n",
    "# Clear CUDA cache and force garbage collection\n",
    "import gc\n",
    "import torch\n",
    "import shutil\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Check memory usage before training\n",
    "print(f\"GPU memory allocated before training: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"GPU memory reserved before training: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "\n",
    "# Create training configuration dictionary\n",
    "training_config = {\n",
    "    \"model_checkpoint\": model_checkpoint,\n",
    "    \"num_labels\": num_labels,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"num_epochs\": config.MODEL_PARAMS[\"epochs\"],\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
    "    \"device\": str(device),\n",
    "    \"dataset_info\": {\n",
    "        \"train_size\": len(train_dataset),\n",
    "        \"val_size\": len(val_dataset),\n",
    "        \"test_size\": len(test_dataset),\n",
    "        \"num_genres\": num_labels,\n",
    "        \"genre_list\": unified_genres\n",
    "    },\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"scheduler\": \"linear_warmup_decay\",\n",
    "    \"model_params\": {\n",
    "        \"hidden_size\": model.config.hidden_size if hasattr(model.config, \"hidden_size\") else \"unknown\",\n",
    "        \"num_hidden_layers\": model.config.num_hidden_layers if hasattr(model.config, \"num_hidden_layers\") else \"unknown\",\n",
    "        \"num_attention_heads\": model.config.num_attention_heads if hasattr(model.config, \"num_attention_heads\") else \"unknown\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Modified TrainingLogger class with checkpoint management\n",
    "class TrainingLoggerWithCleanup(TrainingLogger):\n",
    "    def __init__(self, output_dir, model_name, config=None, max_checkpoints=5, save_frequency=2):\n",
    "        super().__init__(output_dir, model_name, config)\n",
    "        self.max_checkpoints = max_checkpoints\n",
    "        self.save_frequency = save_frequency  # Save one checkpoint every N epochs\n",
    "        self.saved_checkpoints = []\n",
    "    \n",
    "    def save_model_checkpoint(self, model, epoch, step, optimizer=None, scheduler=None, is_best=False):\n",
    "        \"\"\"Save model checkpoint with cleanup\"\"\"\n",
    "        # Only save checkpoint if it's a multiple of save_frequency or it's the best model\n",
    "        if epoch % self.save_frequency != 0 and not is_best:\n",
    "            logging.info(f\"Skipping checkpoint at epoch {epoch} (saving every {self.save_frequency} epochs)\")\n",
    "            return None\n",
    "            \n",
    "        # Save checkpoint as normal\n",
    "        checkpoint_name = f\"checkpoint-{epoch}\"\n",
    "        checkpoint_path = os.path.join(self.checkpoint_dir, checkpoint_name)\n",
    "        os.makedirs(checkpoint_path, exist_ok=True)\n",
    "        \n",
    "        # Save model state dict\n",
    "        model_path = os.path.join(checkpoint_path, \"model.pth\")\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        \n",
    "        # Save optimizer and scheduler if provided\n",
    "        if optimizer:\n",
    "            optimizer_path = os.path.join(checkpoint_path, \"optimizer.pth\")\n",
    "            torch.save(optimizer.state_dict(), optimizer_path)\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler_path = os.path.join(checkpoint_path, \"scheduler.pth\")\n",
    "            torch.save(scheduler.state_dict(), scheduler_path)\n",
    "        \n",
    "        # Save model config\n",
    "        if hasattr(model, 'config'):\n",
    "            config_path = os.path.join(checkpoint_path, \"config.json\")\n",
    "            with open(config_path, 'w') as f:\n",
    "                json.dump(model.config.to_dict(), f, indent=2)\n",
    "        \n",
    "        # Create a symbolic link or copy for the best model\n",
    "        if is_best:\n",
    "            best_path = os.path.join(self.base_dir, \"best_model\")\n",
    "            # If we can use symlinks\n",
    "            try:\n",
    "                if os.path.exists(best_path):\n",
    "                    if os.path.islink(best_path):\n",
    "                        os.unlink(best_path)\n",
    "                    else:\n",
    "                        os.rmdir(best_path)\n",
    "                os.symlink(checkpoint_path, best_path)\n",
    "                logging.info(f\"Created symbolic link to best model at {best_path}\")\n",
    "            except (OSError, NotImplementedError):\n",
    "                # Fallback: copy the model file\n",
    "                best_model_path = os.path.join(self.base_dir, \"best_model.pth\")\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "                logging.info(f\"Saved copy of best model to {best_model_path}\")\n",
    "        \n",
    "        # Add to list of saved checkpoints\n",
    "        self.saved_checkpoints.append(checkpoint_path)\n",
    "        logging.info(f\"Saved model checkpoint to {checkpoint_path}\")\n",
    "        \n",
    "        # Clean up old checkpoints if we have too many\n",
    "        self._cleanup_old_checkpoints()\n",
    "        \n",
    "        return checkpoint_path\n",
    "    \n",
    "    def _cleanup_old_checkpoints(self):\n",
    "        \"\"\"Remove oldest checkpoints to maintain only the last max_checkpoints\"\"\"\n",
    "        # Always keep best checkpoint separate\n",
    "        best_checkpoint = os.path.join(self.base_dir, \"best_model\")\n",
    "        best_checkpoint_target = None\n",
    "        if os.path.islink(best_checkpoint):\n",
    "            best_checkpoint_target = os.path.realpath(best_checkpoint)\n",
    "        \n",
    "        # Skip cleanup if we don't have enough checkpoints yet\n",
    "        if len(self.saved_checkpoints) <= self.max_checkpoints:\n",
    "            return\n",
    "        \n",
    "        # Remove oldest checkpoints\n",
    "        while len(self.saved_checkpoints) > self.max_checkpoints:\n",
    "            oldest_checkpoint = self.saved_checkpoints.pop(0)\n",
    "            \n",
    "            # Don't delete if it's the best checkpoint\n",
    "            if best_checkpoint_target and oldest_checkpoint == best_checkpoint_target:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                if os.path.exists(oldest_checkpoint):\n",
    "                    shutil.rmtree(oldest_checkpoint)\n",
    "                    logging.info(f\"Removed old checkpoint: {oldest_checkpoint}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error removing checkpoint {oldest_checkpoint}: {e}\")\n",
    "\n",
    "# Initialize the logger with cleanup\n",
    "logger = TrainingLoggerWithCleanup(\n",
    "    output_dir=model_save_dir,\n",
    "    model_name=model_checkpoint.replace('/', '_'),\n",
    "    config=training_config,\n",
    "    max_checkpoints=5,     # Keep only the 5 most recent checkpoints\n",
    "    save_frequency=2       # Save checkpoint every 2 epochs\n",
    ")\n",
    "\n",
    "# Copy model architecture to logger's directory if we already saved it elsewhere\n",
    "if os.path.exists(os.path.join(model_save_dir, \"model_info\", \"model_architecture.json\")):\n",
    "    import shutil\n",
    "    os.makedirs(logger.base_dir, exist_ok=True)\n",
    "    shutil.copy(\n",
    "        os.path.join(model_save_dir, \"model_info\", \"model_architecture.json\"),\n",
    "        os.path.join(logger.base_dir, \"model_architecture.json\")\n",
    "    )\n",
    "    logging.info(f\"Copied model architecture to training run directory: {logger.base_dir}\")\n",
    "\n",
    "# Save processor for later use\n",
    "processor_save_path = os.path.join(logger.base_dir, \"processor\")\n",
    "os.makedirs(processor_save_path, exist_ok=True)\n",
    "processor.save_pretrained(processor_save_path)\n",
    "logging.info(f\"Saved processor to {processor_save_path}\")\n",
    "\n",
    "# Make sure model, criterion are on the correct device\n",
    "model.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "# Initialize variables for training loop\n",
    "num_epochs = config.MODEL_PARAMS[\"epochs\"]\n",
    "gradient_accumulation_steps = config.MODEL_PARAMS[\"gradient_accumulation_steps\"]\n",
    "metric_to_monitor = 'hamming_loss'  # Metric to decide best model (lower is better)\n",
    "best_val_metric = float('inf')\n",
    "global_step = 0\n",
    "\n",
    "# Initialize GradScaler for AMP\n",
    "scaler = GradScaler(enabled=(device.type == 'cuda'))\n",
    "\n",
    "logging.info(f\"--- Starting FULL Training for {num_epochs} epochs ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    logging.info(f\"\\n--- Epoch {epoch+1}/{num_epochs} ---\")\n",
    "\n",
    "    # Run training for one epoch\n",
    "    train_loss = train_epoch(\n",
    "        model, train_dataloader, criterion, optimizer, device,\n",
    "        gradient_accumulation_steps, scaler, scheduler  # Pass scaler and scheduler\n",
    "    )\n",
    "    \n",
    "    # Update global step (approximate)\n",
    "    steps_per_epoch = len(train_dataloader) // gradient_accumulation_steps\n",
    "    global_step += steps_per_epoch\n",
    "\n",
    "    # Run evaluation on validation set (using enhanced function)\n",
    "    eval_metrics = evaluate_with_thresholds(model, val_dataloader, criterion, device)\n",
    "\n",
    "    print(f\"\\nEpoch {epoch+1} finished.\")\n",
    "    print(f\"  Avg Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "    if not eval_metrics:\n",
    "        logging.warning(f\"Epoch {epoch+1}: Evaluation failed, skipping checkpoint.\")\n",
    "        continue\n",
    "\n",
    "    # Log epoch metrics to the logger\n",
    "    logger.log_epoch(\n",
    "        epoch=epoch+1,\n",
    "        train_loss=train_loss,\n",
    "        val_metrics=eval_metrics,\n",
    "        learning_rate=scheduler.get_last_lr()[0] if scheduler else learning_rate,\n",
    "        step=global_step\n",
    "    )\n",
    "\n",
    "    # Save checkpoint for this epoch (logger will handle save frequency)\n",
    "    checkpoint_path = logger.save_model_checkpoint(\n",
    "        model=model,\n",
    "        epoch=epoch+1,\n",
    "        step=global_step,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler\n",
    "    )\n",
    "\n",
    "    # Check if this is the best model\n",
    "    current_val_metric = eval_metrics.get(metric_to_monitor, float('inf'))\n",
    "    is_best = False\n",
    "    \n",
    "    if current_val_metric < best_val_metric:\n",
    "        is_best = True\n",
    "        best_val_metric = current_val_metric\n",
    "        logging.info(f\"Validation metric improved ({metric_to_monitor}={current_val_metric:.4f})\")\n",
    "        \n",
    "        # Update best model in logger\n",
    "        logger.update_best_metrics(\n",
    "            epoch=epoch+1,\n",
    "            step=global_step,\n",
    "            val_metrics=eval_metrics,\n",
    "            model_path=checkpoint_path\n",
    "        )\n",
    "        \n",
    "        # Save as best model (always save the best model regardless of save_frequency)\n",
    "        logger.save_model_checkpoint(\n",
    "            model=model,\n",
    "            epoch=epoch+1,\n",
    "            step=global_step,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            is_best=True\n",
    "        )\n",
    "    else:\n",
    "        logging.info(f\"Validation metric did not improve ({metric_to_monitor}={current_val_metric:.4f}). Best: {best_val_metric:.4f}\")\n",
    "\n",
    "    # Update trainer state after each epoch\n",
    "    logger.save_trainer_state(\n",
    "        epoch=epoch+1,\n",
    "        step=global_step,\n",
    "        optimizer_state=optimizer.state_dict(),\n",
    "        scheduler_state=scheduler.state_dict() if scheduler else None\n",
    "    )\n",
    "\n",
    "    epoch_duration = time.time() - epoch_start_time\n",
    "    logging.info(f\"Epoch {epoch+1} finished in {epoch_duration / 60:.2f} minutes.\")\n",
    "\n",
    "total_training_time = time.time() - start_time\n",
    "logger.finish_training(total_training_time)\n",
    "logging.info(f\"--- Training Finished in {total_training_time / 60:.2f} minutes ---\")\n",
    "\n",
    "# Print path to training logs and results\n",
    "print(f\"\\nTraining logs and results saved to: {logger.base_dir}\")\n",
    "print(f\"Best model checkpoint: {logger.metrics['best_metrics'].get('model_checkpoint', 'None')}\")\n",
    "\n",
    "# Show summary of training results\n",
    "print(\"\\nTraining Summary:\")\n",
    "print(f\"  Total epochs: {num_epochs}\")\n",
    "print(f\"  Best {metric_to_monitor}: {best_val_metric:.4f}\")\n",
    "print(f\"  Final train loss: {train_loss:.4f}\")\n",
    "print(f\"  Training time: {total_training_time / 60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Evaluate Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 10:46:34,932 - INFO - \n",
      "--- Evaluating on Test Set using Best Model ---\n",
      "2025-05-04 10:46:34,936 - INFO - Loading best model from /workspace/musicClaGen/models/facebook_w2v-bert-2.0_finetuned_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2BertForSequenceClassification were not initialized from the model checkpoint at facebook/w2v-bert-2.0 and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 10:46:41,758 - INFO - Model successfully loaded and moved to device\n",
      "2025-05-04 10:46:41,761 - INFO - Created safer test dataloader without worker processes\n",
      "2025-05-04 10:46:41,762 - INFO - Starting evaluation on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Loss: 0.1972\n",
      "  Validation Hamming Loss: 0.0574\n",
      "  Validation Jaccard Samples: 0.0000\n",
      "  Validation F1 Micro: 0.0000\n",
      "  Validation F1 Macro: 0.0000\n",
      "2025-05-04 10:55:42,077 - INFO - \n",
      "--- Final Test Set Results (completed in 540.31s) ---\n",
      "2025-05-04 10:55:42,078 - INFO - Test Hamming Loss: 0.0574\n",
      "2025-05-04 10:55:42,080 - INFO - Test Jaccard Samples: 0.0000\n",
      "2025-05-04 10:55:42,081 - INFO - Test F1 Micro: 0.0000\n",
      "2025-05-04 10:55:42,082 - INFO - Test F1 Macro: 0.0000\n",
      "2025-05-04 10:55:42,083 - INFO - Test Eval Loss: 0.1972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Cell 12: Evaluate Best Model on Test Set (Robust Version)\n",
    "\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from transformers import AutoModelForAudioClassification\n",
    "import logging\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "logging.info(\"\\n--- Evaluating on Test Set using Best Model ---\")\n",
    "\n",
    "# Construct path to the best saved model\n",
    "best_model_path = os.path.join(model_save_dir, f\"{model_checkpoint.replace('/', '_')}_finetuned_best.pth\")\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    try:\n",
    "        logging.info(f\"Loading best model from {best_model_path}\")\n",
    "        \n",
    "        # Re-initialize model with correct structure\n",
    "        model_reloaded = AutoModelForAudioClassification.from_pretrained(\n",
    "            model_checkpoint,\n",
    "            num_labels=num_labels,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        \n",
    "        # Load the saved state dict\n",
    "        model_reloaded.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "        model_reloaded.to(device)\n",
    "        model_reloaded.eval()\n",
    "        logging.info(\"Model successfully loaded and moved to device\")\n",
    "        \n",
    "        # Create a safer test dataloader with no workers (avoid multiprocessing issues)\n",
    "        safe_test_dataloader = DataLoader(\n",
    "            test_dataset, \n",
    "            batch_size=config.MODEL_PARAMS[\"batch_size\"],\n",
    "            shuffle=False,\n",
    "            collate_fn=data_collator,\n",
    "            num_workers=0,  # Use main process only - no worker processes\n",
    "            pin_memory=False  # Disable pinned memory to reduce memory usage\n",
    "        )\n",
    "        logging.info(\"Created safer test dataloader without worker processes\")\n",
    "        \n",
    "        # Run evaluation with extra error handling\n",
    "        logging.info(\"Starting evaluation on test set...\")\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            test_metrics = evaluate_with_thresholds(model_reloaded, safe_test_dataloader, criterion, device)\n",
    "            eval_time = time.time() - start_time\n",
    "            \n",
    "            # Log test results\n",
    "            logging.info(f\"\\n--- Final Test Set Results (completed in {eval_time:.2f}s) ---\")\n",
    "            if test_metrics:\n",
    "                for metric_name, metric_value in test_metrics.items():\n",
    "                    if metric_name != 'threshold_metrics':  # Skip nested dictionary\n",
    "                        try:\n",
    "                            logging.info(f\"Test {metric_name.replace('_', ' ').title()}: {metric_value:.4f}\")\n",
    "                        except (TypeError, ValueError):\n",
    "                            logging.info(f\"Test {metric_name.replace('_', ' ').title()}: {metric_value}\")\n",
    "            else:\n",
    "                logging.info(\"Test evaluation failed to produce metrics.\")\n",
    "\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            if \"CUDA out of memory\" in str(e):\n",
    "                logging.error(\"CUDA out of memory during evaluation. Try reducing batch size.\")\n",
    "            elif \"DataLoader worker\" in str(e):\n",
    "                logging.error(f\"DataLoader worker error (should not happen with num_workers=0): {e}\")\n",
    "            else:\n",
    "                logging.error(f\"Runtime error during evaluation: {e}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during evaluation: {e}\", exc_info=True)\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load model: {e}\", exc_info=True)\n",
    "else:\n",
    "    logging.warning(f\"Best model checkpoint not found at {best_model_path}. Skipping final test evaluation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicClaGen_env22",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
