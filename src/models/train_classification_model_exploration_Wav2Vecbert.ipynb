{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT detected as: /workspace/musicClaGen\n",
      "Adding /workspace/musicClaGen to sys.path\n",
      "/workspace/musicClaGen\n",
      "Imports and basic setup complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "# Use AutoProcessor for Wav2Vec2-BERT - it bundles feature_extractor and tokenizer (if needed)\n",
    "from transformers import AutoModelForAudioClassification, AutoProcessor\n",
    "\n",
    "from torch.optim import AdamW\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import ast # For parsing string representations of lists/arrays\n",
    "import logging\n",
    "import time\n",
    "from sklearn.metrics import hamming_loss, jaccard_score, f1_score # Add more as needed\n",
    "from tqdm.notebook import tqdm # Use notebook version of tqdm\n",
    "import librosa # Needed for loading raw audio now\n",
    "\n",
    "\n",
    "\n",
    "# --- Project Setup ---\n",
    "# Detect if running in notebook or script to adjust path\n",
    "\n",
    "cwd = os.getcwd()\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(cwd, '../../')) # NOTE: remember to change if change the directory structure\n",
    "\n",
    "\n",
    "\n",
    "print(f\"PROJECT_ROOT detected as: {PROJECT_ROOT}\")\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    print(f\"Adding {PROJECT_ROOT} to sys.path\")\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "# --- Config and Utils ---\n",
    "try:\n",
    "    import config # Import your configuration file\n",
    "    # Optionally import utils if needed, e.g., for get_audio_path if not defined here\n",
    "    # import src.utils as utils\n",
    "except ModuleNotFoundError:\n",
    "     print(\"ERROR: Cannot import config or utils. Make sure PROJECT_ROOT is correct and src is importable.\")\n",
    "     # Or add src to path: sys.path.insert(0, os.path.join(PROJECT_ROOT, 'src'))\n",
    "     # import config\n",
    "     # import utils\n",
    "\n",
    "\n",
    "# --- Setup Logging ---\n",
    "for handler in logging.root.handlers[:]: logging.root.removeHandler(handler) # Clear previous\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    handlers=[logging.StreamHandler(sys.stdout)])\n",
    "\n",
    "print(\"Imports and basic setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-03 17:34:10,095 - INFO - Loaded 22 unified genres from /workspace/musicClaGen/data/processed/unified_genres.txt\n",
      "2025-05-03 17:34:10,097 - INFO - Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Cell 2\n",
    "# --- Load Config ---\n",
    "# Ensure config.py has the correct paths in the PATHS dict\n",
    "manifest_path = config.PATHS.get('SMALL_MULTILABEL_PATH', os.path.join(config.PATHS['PROCESSED_DATA_DIR'], 'small_subset_multihot.csv')) # Use .get for safety\n",
    "genre_list_path = config.PATHS.get('GENRE_LIST_PATH', os.path.join(config.PATHS['PROCESSED_DATA_DIR'], 'unified_genres.txt'))\n",
    "model_save_dir = config.PATHS['MODELS_DIR']\n",
    "\n",
    "# Ensure config.py has MODEL_PARAMS dict with model_checkpoint\n",
    "model_checkpoint = config.MODEL_PARAMS['model_checkpoint'] # e.g., \"facebook/w2v-bert-2.0\" - VERIFY!\n",
    "learning_rate = config.MODEL_PARAMS['learning_rate']\n",
    "batch_size = config.MODEL_PARAMS['batch_size'] # Use the small BS for notebook test\n",
    "num_epochs_debug = 1 # <<<--- RUN ONLY 1 EPOCH FOR DEBUGGING ---<<<\n",
    "weight_decay = config.MODEL_PARAMS['weight_decay']\n",
    "gradient_accumulation_steps = config.MODEL_PARAMS['gradient_accumulation_steps']\n",
    "\n",
    "# --- Load unified genre list ---\n",
    "try:\n",
    "    with open(genre_list_path, 'r') as f:\n",
    "        unified_genres = [line.strip() for line in f if line.strip()]\n",
    "    num_labels = len(unified_genres) # should be the number of labels defined in the unified_genres.txt file, in this case it should be 22.\n",
    "    logging.info(f\"Loaded {num_labels} unified genres from {genre_list_path}\")\n",
    "    if num_labels == 0: raise ValueError(\"Genre list is empty!\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to load or process unified genre list: {e}\", exc_info=True)\n",
    "    raise SystemExit(\"Cannot proceed without genre list.\")\n",
    "\n",
    "# --- Setup Device ---\n",
    "device = torch.device(config.DEVICE if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(f\"Using device: {device}\")\n",
    "if not torch.cuda.is_available() and config.DEVICE==\"cuda\":\n",
    "     logging.warning(\"CUDA selected but not available, falling back to CPU.\")\n",
    "\n",
    "# --- Create Save Directory ---\n",
    "os.makedirs(model_save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 2: Load Config & Define Constants (Modified for CPU Debugging)\n",
    "\n",
    "# import torch # Ensure torch is imported\n",
    "# import os\n",
    "# import logging\n",
    "# import config # Your config file\n",
    "\n",
    "# # --- Load Config ---\n",
    "# # Ensure config.py has the correct paths in the PATHS dict\n",
    "# # Use the key pointing to your manifest with raw audio paths\n",
    "# manifest_path = config.PATHS.get('SMALL_MULTILABEL_PATH', os.path.join(config.PATHS['PROCESSED_DATA_DIR'], 'small_subset_multihot.csv'))\n",
    "# genre_list_path = config.PATHS.get('GENRE_LIST_PATH', os.path.join(config.PATHS['PROCESSED_DATA_DIR'], 'unified_genres.txt'))\n",
    "# model_save_dir = config.PATHS['MODELS_DIR']\n",
    "\n",
    "# # Ensure config.py has MODEL_PARAMS dict with model_checkpoint\n",
    "# model_checkpoint = config.MODEL_PARAMS['model_checkpoint'] # e.g., \"facebook/w2v-bert-2.0\"\n",
    "# learning_rate = config.MODEL_PARAMS['learning_rate']\n",
    "# batch_size = config.MODEL_PARAMS['batch_size']\n",
    "# num_epochs_debug = 1 # Keep as 1 for debug run\n",
    "# weight_decay = config.MODEL_PARAMS['weight_decay']\n",
    "# gradient_accumulation_steps = config.MODEL_PARAMS['gradient_accumulation_steps']\n",
    "\n",
    "# # --- Load unified genre list ---\n",
    "# try:\n",
    "#     with open(genre_list_path, 'r') as f:\n",
    "#         unified_genres = [line.strip() for line in f if line.strip()]\n",
    "#     num_labels = len(unified_genres)\n",
    "#     logging.info(f\"Loaded {num_labels} unified genres from {genre_list_path}\")\n",
    "#     if num_labels == 0: raise ValueError(\"Genre list is empty!\")\n",
    "# except Exception as e:\n",
    "#     logging.error(f\"Failed to load or process unified genre list: {e}\", exc_info=True)\n",
    "#     raise SystemExit(\"Cannot proceed without genre list.\")\n",
    "\n",
    "# # --- Setup Device (FORCED TO CPU FOR DEBUGGING) ---\n",
    "# # Original line: device = torch.device(config.DEVICE if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\") # <<<--- TEMPORARY CHANGE FOR DEBUGGING CUDA ERROR\n",
    "# logging.info(f\"Using device: {device} (Forced CPU for debugging)\")\n",
    "# # --- End Temporary Change ---\n",
    "\n",
    "# # --- Create Save Directory ---\n",
    "# os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# print(\"Cell 2 setup complete (Device forced to CPU for debugging).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/musicClaGen/data/processed/small_subset_multihot.csv\n"
     ]
    }
   ],
   "source": [
    "print(manifest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 3: Dataset Class Definition (Raw Audio Version) This cell uses the regex parser to parse the multi_hot_label string back into a list of integers.\n",
    "\n",
    "\n",
    "\n",
    "# # Define(recollect)the regex parser from preprocess.py if needed, \n",
    "# # otherwise use ast.literal_eval--- \n",
    "# # NOTE: After changing usage.ipynb 05/03/2025, should fall back to ast.literal_eval now. Clean code later\n",
    "\n",
    "# import re\n",
    "\n",
    "# def parse_numpy_array_string(array_str):\n",
    "#     \"\"\"\n",
    "#     Parse strings like '[np.float32(1.0), np.float32(0.0), ...]' into a list of integers.\n",
    "#     This is needed because ast.literal_eval cannot handle 'np.float32()' in the string.\n",
    "#     \"\"\"\n",
    "#     if not isinstance(array_str, str):\n",
    "#         return []\n",
    "    \n",
    "#     try:\n",
    "#         # Extract all the float values using regular expressions\n",
    "#         float_matches = re.findall(r'np\\.float32\\((\\d+\\.\\d+)\\)', array_str)\n",
    "        \n",
    "#         # Convert matches to integers (1.0 -> 1, 0.0 -> 0)\n",
    "#         values = []\n",
    "#         for match in float_matches:\n",
    "#             value = float(match)\n",
    "#             # Convert to integer if it's 0.0 or 1.0\n",
    "#             if value == 1.0:\n",
    "#                 values.append(1)\n",
    "#             elif value == 0.0:\n",
    "#                 values.append(0)\n",
    "#             else:\n",
    "#                 values.append(value)  # Keep as float if not 0 or 1\n",
    "                \n",
    "#         return values\n",
    "#     except Exception as e:\n",
    "#         logging.warning(f\"Error parsing array string: {e}\")\n",
    "#         return []\n",
    "\n",
    "# class FMARawAudioDataset(Dataset):\n",
    "#     \"\"\"\n",
    "#     Loads raw audio waveforms and labels from manifest, uses Hugging Face\n",
    "#     feature extractor (like ASTFeatureExtractor or Wav2Vec2Processor) on the fly.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, manifest_path, feature_extractor):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             manifest_path (str): Path to the final manifest CSV file.\n",
    "#             feature_extractor: Initialized Hugging Face AutoFeatureExtractor or AutoProcessor.\n",
    "#         \"\"\"\n",
    "#         logging.info(f\"Initializing FMARawAudioDataset from: {manifest_path}\")\n",
    "#         if feature_extractor is None:\n",
    "#              raise ValueError(\"FMARawAudioDataset requires a feature_extractor/processor instance.\")\n",
    "\n",
    "#         self.feature_extractor = feature_extractor\n",
    "#         # Get target sampling rate directly from the extractor/processor\n",
    "#         try:\n",
    "#              # Works for Wav2Vec2Processor, ASTFeatureExtractor, etc.\n",
    "#              self.target_sr = self.feature_extractor.sampling_rate\n",
    "#              logging.info(f\"Target sampling rate set from feature extractor: {self.target_sr} Hz\")\n",
    "#         except AttributeError:\n",
    "#              logging.warning(\"Could not get sampling_rate from feature_extractor, using config.\")\n",
    "#              # Fallback to config if needed, but ensuring match is crucial\n",
    "#              self.target_sr = config.PREPROCESSING_PARAMS['sample_rate']\n",
    "\n",
    "\n",
    "#         logging.info(f\"Loading manifest from: {manifest_path}\")\n",
    "#         try:\n",
    "#             self.manifest = pd.read_csv(manifest_path)\n",
    "#             # Ensure index is set if needed elsewhere, or use default range index\n",
    "#             if 'track_id' in self.manifest.columns:\n",
    "#                  self.manifest = self.manifest.set_index('track_id', drop=False)\n",
    "\n",
    "#             # --- Parse the 'multi_hot_label' string back into a list ---\n",
    "#             # Here: if we decide to use raw audio, we use regex parser; \n",
    "#             #       if we decide to use mel spectrogram, we use ast.literal_eval\n",
    "\n",
    "#             # Choose the correct parser based on how labels were saved in the CSV\n",
    "#             # If saved as '[1.0, 0.0,...]' use ast.literal_eval\n",
    "#             # label_parser = ast.literal_eval\n",
    "#             # If saved as '[np.float32(1.0)...]' uncomment and use regex parser\n",
    "#             label_parser = parse_numpy_array_string\n",
    "\n",
    "#             self.manifest['multi_hot_label'] = self.manifest['multi_hot_label'].apply(label_parser)\n",
    "#             logging.info(f\"Loaded and parsed manifest with {len(self.manifest)} entries.\")\n",
    "#             # Check the first parsed label\n",
    "#             logging.info(f\"Example parsed label (first entry): {self.manifest['multi_hot_label'].iloc[0]}\")\n",
    "\n",
    "#         except Exception as e:\n",
    "#             logging.error(f\"Error loading or parsing manifest {manifest_path}: {e}\", exc_info=True)\n",
    "#             raise\n",
    "\n",
    "#     def __len__(self):\n",
    "#         \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "#         return len(self.manifest)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         \"\"\"\n",
    "#         Loads raw audio for index idx, processes it with the feature extractor,\n",
    "#         and returns the processed inputs and labels.\n",
    "#         \"\"\"\n",
    "#         if torch.is_tensor(idx): idx = idx.tolist() # Handle tensor indices\n",
    "\n",
    "#         # Get the row data from the manifest\n",
    "#         row = self.manifest.iloc[idx]\n",
    "#         track_id = row.get('track_id', self.manifest.index[idx]) # Get track_id safely\n",
    "#         label_vector = row['multi_hot_label'] # Already parsed list/array\n",
    "\n",
    "#         # Construct absolute audio path if necessary\n",
    "#         audio_path = row['audio_path']\n",
    "\n",
    "#         #NOTE: originally, the mel-spectrogram's path is relative  but the raw audio's path is absolute, so we need to make sure the audio_path is absolute\n",
    "#         # So we are check if the audio_path is absolute or relative in case we load the wrong data, if it's relative, we need to join it with the PROJECT_ROOT\n",
    "#         if not os.path.isabs(audio_path):\n",
    "#              # Assumes path in manifest is relative to PROJECT_ROOT\n",
    "#              audio_path = os.path.join(config.PROJECT_ROOT, audio_path)\n",
    "\n",
    "#         try:\n",
    "#             # --- 1. Load RAW Audio Waveform ---\n",
    "#             # Load full 30s clip at the TARGET sample rate required by the processor\n",
    "#             waveform, loaded_sr = librosa.load(\n",
    "#                 audio_path,\n",
    "#                 sr=self.target_sr, # Use processor's sampling rate\n",
    "#                 duration=30.0     # Load the full 30 seconds\n",
    "#             )\n",
    "#             # Ensure minimum length if needed (though duration should handle it)\n",
    "#             min_samples = int(0.1 * self.target_sr) # Example: require at least 0.1s\n",
    "#             if len(waveform) < min_samples:\n",
    "#                  raise ValueError(f\"Audio signal for track {track_id} too short after loading.\")\n",
    "\n",
    "#             # --- 2. Apply Feature Extractor ---\n",
    "#             # Pass the raw waveform numpy array\n",
    "#             # The extractor handles normalization, padding/truncation, tensor conversion\n",
    "            \n",
    "#             max_length = 5000\n",
    "\n",
    "#             inputs = self.feature_extractor(\n",
    "#                 waveform,\n",
    "#                 sampling_rate=self.target_sr,\n",
    "#                 return_tensors=\"pt\",\n",
    "#                 return_attention_mask=True # Request attention mask\n",
    "#             )\n",
    "\n",
    "#             # --- 3. Prepare Outputs ---\n",
    "#             # Squeeze unnecessary batch dimension added by the extractor\n",
    "#             # Key name ('input_values', 'input_features') depends on the specific extractor\n",
    "#             feature_tensor = inputs.get('input_values', inputs.get('input_features'))\n",
    "#             if feature_tensor is None:\n",
    "#                 raise KeyError(\"Expected 'input_values' or 'input_features' key from feature_extractor output.\")\n",
    "#             feature_tensor = feature_tensor.squeeze(0) # Remove batch dim -> [Channels?, Freq?, Time] or [SeqLen, Dim]\n",
    "\n",
    "#             attention_mask = inputs.get('attention_mask', None)\n",
    "#             if attention_mask is not None:\n",
    "#                  attention_mask = attention_mask.squeeze(0)\n",
    "\n",
    "#             # Convert label list/array to float tensor for BCE loss\n",
    "#             label_tensor = torch.tensor(label_vector, dtype=torch.float32)\n",
    "\n",
    "#             # Return dictionary matching model's expected input names\n",
    "#             model_input_dict = {\"labels\": label_tensor}\n",
    "#             # Use the key the feature extractor provided\n",
    "#             if 'input_values' in inputs:\n",
    "#                  model_input_dict['input_values'] = feature_tensor\n",
    "#             elif 'input_features' in inputs:\n",
    "#                  model_input_dict['input_features'] = feature_tensor\n",
    "\n",
    "#             if attention_mask is not None:\n",
    "#                  model_input_dict['attention_mask'] = attention_mask\n",
    "\n",
    "#             return model_input_dict\n",
    "\n",
    "#         except FileNotFoundError:\n",
    "#              logging.error(f\"Audio file not found for track {track_id} at {audio_path}\")\n",
    "#              raise # Or implement skipping logic with collate_fn\n",
    "#         except Exception as e:\n",
    "#             logging.error(f\"Error loading/processing track {track_id} at {audio_path}: {e}\", exc_info=True)\n",
    "#             raise # Or implement skipping logic\n",
    "\n",
    "\n",
    "# print(\"FMARawAudioDataset class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Cell 3: Dataset Class Definition (Raw Audio Version) This cell uses the ast.literal_eval parser to parse the multi_hot_label string back into a list of integers.\n",
    "\n",
    "\n",
    "# import re\n",
    "# import ast \n",
    "\n",
    "\n",
    "\n",
    "# # # Define(recollect)the regex parser from preprocess.py if needed,\n",
    "# # # otherwise use ast.literal_eval---\n",
    "# # # NOTE: After changing usage.ipynb 05/03/2025, should fall back to ast.literal_eval now. Clean code later\n",
    "# # def parse_numpy_array_string(array_str):\n",
    "# #     \"\"\"\n",
    "# #     Parse strings like '[np.float32(1.0), np.float32(0.0), ...]' into a list of integers.\n",
    "# #     This is needed because ast.literal_eval cannot handle 'np.float32()' in the string.\n",
    "# #     \"\"\"\n",
    "# #     if not isinstance(array_str, str):\n",
    "# #         return []\n",
    "# #     try:\n",
    "# #         # Extract all the float values using regular expressions\n",
    "# #         # Refined regex to handle numbers with or without decimal points\n",
    "# #         float_matches = re.findall(r'np\\.float32\\(([\\d\\.]+)\\)', array_str)\n",
    "# #         # Convert matches to floats then maybe int (use float for BCE loss)\n",
    "# #         values = []\n",
    "# #         for match in float_matches:\n",
    "# #             value = float(match)\n",
    "# #             values.append(1.0 if value == 1.0 else 0.0) # Store as float 0.0 or 1.0\n",
    "# #         return values\n",
    "# #     except Exception as e:\n",
    "# #         logging.warning(f\"Error parsing array string: {e}\")\n",
    "# #         return []\n",
    "# # --- End commented out parser ---\n",
    "\n",
    "\n",
    "# class FMARawAudioDataset(Dataset):\n",
    "#     \"\"\"\n",
    "#     Loads raw audio waveforms and labels from manifest, uses Hugging Face\n",
    "#     feature extractor (like ASTFeatureExtractor or Wav2Vec2Processor/AutoFeatureExtractor) on the fly.\n",
    "#     Assumes padding/truncation will be handled by a collate function.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, manifest_path, feature_extractor):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             manifest_path (str): Path to the final manifest CSV file (e.g., small_subset_multihot.csv).\n",
    "#             feature_extractor: Initialized Hugging Face AutoFeatureExtractor instance.\n",
    "#         \"\"\"\n",
    "#         logging.info(f\"Initializing FMARawAudioDataset from: {manifest_path}\")\n",
    "#         if feature_extractor is None:\n",
    "#              raise ValueError(\"FMARawAudioDataset requires a feature_extractor instance.\")\n",
    "\n",
    "#         self.feature_extractor = feature_extractor\n",
    "#         try:\n",
    "#              self.target_sr = self.feature_extractor.sampling_rate\n",
    "#              logging.info(f\"Target sampling rate set from feature extractor: {self.target_sr} Hz\")\n",
    "#         except AttributeError:\n",
    "#              logging.error(\"Could not get sampling_rate from feature_extractor.\", exc_info=True)\n",
    "#              raise\n",
    "\n",
    "#         logging.info(f\"Loading manifest from: {manifest_path}\")\n",
    "#         try:\n",
    "#             self.manifest = pd.read_csv(manifest_path)\n",
    "#             if 'track_id' in self.manifest.columns:\n",
    "#                  self.manifest = self.manifest.set_index('track_id', drop=False)\n",
    "\n",
    "#             # --- Parse the 'multi_hot_label' string back into a list ---\n",
    "#             # Use ast.literal_eval assuming labels were saved as standard list strings '[1.0, 0.0,...]'\n",
    "#             logging.info(\"Attempting to parse 'multi_hot_label' column using ast.literal_eval...\")\n",
    "#             label_parser = ast.literal_eval # <<<--- Using ast.literal_eval\n",
    "#             # label_parser = parse_numpy_array_string # Keep commented out\n",
    "\n",
    "#             # Ensure the column name matches your CSV ('multi_hot_label' based on your previous code)\n",
    "#             label_col_name = 'multi_hot_label'\n",
    "#             if label_col_name not in self.manifest.columns:\n",
    "#                  raise KeyError(f\"Column '{label_col_name}' not found in manifest CSV at {manifest_path}\")\n",
    "\n",
    "#             self.manifest[label_col_name] = self.manifest[label_col_name].apply(label_parser)\n",
    "\n",
    "#             # Verification step\n",
    "#             first_label = self.manifest[label_col_name].iloc[0]\n",
    "#             if not isinstance(first_label, list):\n",
    "#                  raise TypeError(f\"Parsed label is not a list, check parser/CSV format. Got type: {type(first_label)}\")\n",
    "#             global num_labels # Make sure num_labels is defined/loaded in Cell 2\n",
    "#             if len(first_label) != num_labels:\n",
    "#                 logging.warning(f\"Parsed label length ({len(first_label)}) does not match expected num_labels ({num_labels}). Check parsing or unified_genres.txt.\")\n",
    "#             logging.info(f\"Example parsed label (type {type(first_label)}, length {len(first_label)}): {str(first_label)[:100]}...\")\n",
    "\n",
    "#             logging.info(f\"Loaded and parsed manifest with {len(self.manifest)} entries.\")\n",
    "\n",
    "#         except FileNotFoundError:\n",
    "#              logging.error(f\"Manifest file not found: {manifest_path}\", exc_info=True)\n",
    "#              raise\n",
    "#         except Exception as e:\n",
    "#             logging.error(f\"Error loading or parsing manifest {manifest_path}: {e}\", exc_info=True)\n",
    "#             raise\n",
    "\n",
    "#     def __len__(self):\n",
    "#         \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "#         return len(self.manifest)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         \"\"\"\n",
    "#         Loads raw audio for index idx, processes it with the feature extractor,\n",
    "#         and returns the processed inputs and labels.\n",
    "#         \"\"\"\n",
    "#         if torch.is_tensor(idx): idx = idx.tolist() # Handle tensor indices\n",
    "\n",
    "#         try:\n",
    "#             # Get the row data from the manifest\n",
    "#             row = self.manifest.iloc[idx]\n",
    "#             track_id = row.get('track_id', self.manifest.index[idx]) # Get track_id safely\n",
    "#             multi_hot_label = row['multi_hot_label'] # Use the correct column name\n",
    "#             audio_path = row['audio_path']\n",
    "\n",
    "#             # Construct absolute audio path if necessary\n",
    "#             # NOTE: originally, the mel-spectrogram's path is relative  but the raw audio's path is absolute, so we need to make sure the audio_path is absolute\n",
    "#             # So we are check if the audio_path is absolute or relative in case we load the wrong data, if it's relative, we need to join it with the PROJECT_ROOT\n",
    "#             if not os.path.isabs(audio_path):\n",
    "#                 # Assumes path in manifest is relative to PROJECT_ROOT defined in config\n",
    "#                 audio_path = os.path.join(config.PROJECT_ROOT, audio_path)\n",
    "\n",
    "#             # --- 1. Load RAW Audio Waveform ---\n",
    "#             waveform, loaded_sr = librosa.load(\n",
    "#                 audio_path,\n",
    "#                 sr=self.target_sr, # Use extractor's sampling rate\n",
    "#                 duration=30.0      # Load the full 30 seconds\n",
    "#             )\n",
    "#             min_samples = int(0.1 * self.target_sr)\n",
    "#             if len(waveform) < min_samples:\n",
    "#                  raise ValueError(f\"Audio signal for track {track_id} too short after loading.\")\n",
    "\n",
    "#             # --- 2. Apply Feature Extractor ---\n",
    "#             # Let the Data Collator handle padding/truncation later\n",
    "#             inputs = self.feature_extractor(\n",
    "#                 waveform,\n",
    "#                 sampling_rate=self.target_sr,\n",
    "#                 return_tensors=\"pt\",\n",
    "#                 # REMOVED padding/truncation args\n",
    "#                 return_attention_mask=True # Keep requesting mask\n",
    "#             )\n",
    "\n",
    "#             # --- 3. Prepare Outputs ---\n",
    "#             feature_tensor = inputs.get('input_values', inputs.get('input_features'))\n",
    "#             if feature_tensor is None:\n",
    "#                 raise KeyError(f\"Expected 'input_values' or 'input_features' key from feature_extractor output for track {track_id}. Got keys: {inputs.keys()}\")\n",
    "#             feature_tensor = feature_tensor.squeeze(0)\n",
    "\n",
    "#             attention_mask = inputs.get('attention_mask', None)\n",
    "#             if attention_mask is not None:\n",
    "#                 attention_mask = attention_mask.squeeze(0)\n",
    "\n",
    "#             # Convert label list/array to float tensor\n",
    "#             label_tensor = torch.tensor(multi_hot_label, dtype=torch.float32)\n",
    "\n",
    "#             # Return dictionary\n",
    "#             model_input_dict = {\"labels\": label_tensor}\n",
    "#             input_key = 'input_values' if 'input_values' in inputs else 'input_features'\n",
    "#             model_input_dict[input_key] = feature_tensor\n",
    "#             if attention_mask is not None:\n",
    "#                 model_input_dict['attention_mask'] = attention_mask\n",
    "\n",
    "#             return model_input_dict\n",
    "\n",
    "#         except FileNotFoundError:\n",
    "#             logging.error(f\"Audio file not found for track {track_id} at {audio_path}\")\n",
    "#             # Return None ONLY IF collate_fn handles it, otherwise raise\n",
    "#             raise\n",
    "#         except Exception as e:\n",
    "#             logging.error(f\"Error in __getitem__ for track {track_id}: {e}\", exc_info=True)\n",
    "#             # Return None ONLY IF collate_fn handles it, otherwise raise\n",
    "#             raise\n",
    "\n",
    "# print(\"FMARawAudioDataset class defined (using raw audio, feature extractor, ast.literal_eval for labels).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Cell 3: Dataset Class Definition (Raw Audio Version) This cell uses the ast.literal_eval parser to parse the multi_hot_label string back into a list of integers.\n",
    "\n",
    "\n",
    "# import re\n",
    "# import ast \n",
    "\n",
    "\n",
    "\n",
    "# # # Define(recollect)the regex parser from preprocess.py if needed,\n",
    "# # # otherwise use ast.literal_eval---\n",
    "# # # NOTE: After changing usage.ipynb 05/03/2025, should fall back to ast.literal_eval now. Clean code later\n",
    "# # def parse_numpy_array_string(array_str):\n",
    "# #     \"\"\"\n",
    "# #     Parse strings like '[np.float32(1.0), np.float32(0.0), ...]' into a list of integers.\n",
    "# #     This is needed because ast.literal_eval cannot handle 'np.float32()' in the string.\n",
    "# #     \"\"\"\n",
    "# #     if not isinstance(array_str, str):\n",
    "# #         return []\n",
    "# #     try:\n",
    "# #         # Extract all the float values using regular expressions\n",
    "# #         # Refined regex to handle numbers with or without decimal points\n",
    "# #         float_matches = re.findall(r'np\\.float32\\(([\\d\\.]+)\\)', array_str)\n",
    "# #         # Convert matches to floats then maybe int (use float for BCE loss)\n",
    "# #         values = []\n",
    "# #         for match in float_matches:\n",
    "# #             value = float(match)\n",
    "# #             values.append(1.0 if value == 1.0 else 0.0) # Store as float 0.0 or 1.0\n",
    "# #         return values\n",
    "# #     except Exception as e:\n",
    "# #         logging.warning(f\"Error parsing array string: {e}\")\n",
    "# #         return []\n",
    "# # --- End commented out parser ---\n",
    "\n",
    "\n",
    "# class FMARawAudioDataset(Dataset):\n",
    "#     \"\"\"\n",
    "#     Loads raw audio waveforms and labels from manifest, uses Hugging Face\n",
    "#     feature extractor (like ASTFeatureExtractor or Wav2Vec2Processor/AutoFeatureExtractor) on the fly.\n",
    "#     Assumes padding/truncation will be handled by a collate function.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, manifest_path, feature_extractor):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             manifest_path (str): Path to the final manifest CSV file (e.g., small_subset_multihot.csv).\n",
    "#             feature_extractor: Initialized Hugging Face AutoFeatureExtractor instance.\n",
    "#         \"\"\"\n",
    "#         logging.info(f\"Initializing FMARawAudioDataset from: {manifest_path}\")\n",
    "#         if feature_extractor is None:\n",
    "#              raise ValueError(\"FMARawAudioDataset requires a feature_extractor instance.\")\n",
    "\n",
    "#         self.feature_extractor = feature_extractor\n",
    "#         try:\n",
    "#              self.target_sr = self.feature_extractor.sampling_rate\n",
    "#              logging.info(f\"Target sampling rate set from feature extractor: {self.target_sr} Hz\")\n",
    "#         except AttributeError:\n",
    "#              logging.error(\"Could not get sampling_rate from feature_extractor.\", exc_info=True)\n",
    "#              raise\n",
    "\n",
    "#         logging.info(f\"Loading manifest from: {manifest_path}\")\n",
    "#         try:\n",
    "#             self.manifest = pd.read_csv(manifest_path)\n",
    "#             if 'track_id' in self.manifest.columns:\n",
    "#                  self.manifest = self.manifest.set_index('track_id', drop=False)\n",
    "\n",
    "#             # --- Parse the 'multi_hot_label' string back into a list ---\n",
    "#             # Use ast.literal_eval assuming labels were saved as standard list strings '[1.0, 0.0,...]'\n",
    "#             logging.info(\"Attempting to parse 'multi_hot_label' column using ast.literal_eval...\")\n",
    "#             label_parser = ast.literal_eval # <<<--- Using ast.literal_eval\n",
    "#             # label_parser = parse_numpy_array_string # Keep commented out\n",
    "\n",
    "#             # Ensure the column name matches your CSV ('multi_hot_label' based on your previous code)\n",
    "#             label_col_name = 'multi_hot_label'\n",
    "#             if label_col_name not in self.manifest.columns:\n",
    "#                  raise KeyError(f\"Column '{label_col_name}' not found in manifest CSV at {manifest_path}\")\n",
    "\n",
    "#             self.manifest[label_col_name] = self.manifest[label_col_name].apply(label_parser)\n",
    "\n",
    "#             # Verification step\n",
    "#             first_label = self.manifest[label_col_name].iloc[0]\n",
    "#             if not isinstance(first_label, list):\n",
    "#                  raise TypeError(f\"Parsed label is not a list, check parser/CSV format. Got type: {type(first_label)}\")\n",
    "#             global num_labels # Make sure num_labels is defined/loaded in Cell 2\n",
    "#             if len(first_label) != num_labels:\n",
    "#                 logging.warning(f\"Parsed label length ({len(first_label)}) does not match expected num_labels ({num_labels}). Check parsing or unified_genres.txt.\")\n",
    "#             logging.info(f\"Example parsed label (type {type(first_label)}, length {len(first_label)}): {str(first_label)[:100]}...\")\n",
    "\n",
    "#             logging.info(f\"Loaded and parsed manifest with {len(self.manifest)} entries.\")\n",
    "\n",
    "#         except FileNotFoundError:\n",
    "#              logging.error(f\"Manifest file not found: {manifest_path}\", exc_info=True)\n",
    "#              raise\n",
    "#         except Exception as e:\n",
    "#             logging.error(f\"Error loading or parsing manifest {manifest_path}: {e}\", exc_info=True)\n",
    "#             raise\n",
    "\n",
    "#     def __len__(self):\n",
    "#         \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "#         return len(self.manifest)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         \"\"\"\n",
    "#         Loads raw audio for index idx, processes it with the feature extractor,\n",
    "#         and returns the processed inputs and labels.\n",
    "#         \"\"\"\n",
    "#         if torch.is_tensor(idx): idx = idx.tolist() # Handle tensor indices\n",
    "\n",
    "#         try:\n",
    "#             # Get the row data from the manifest\n",
    "#             row = self.manifest.loc[idx]\n",
    "#             track_id = row.get('track_id', self.manifest.index[idx]) # Get track_id safely\n",
    "#             multi_hot_label = row['multi_hot_label'] # Use the correct column name\n",
    "#             audio_path = row['audio_path']\n",
    "\n",
    "#             # Construct absolute audio path if necessary\n",
    "#             # NOTE: originally, the mel-spectrogram's path is relative  but the raw audio's path is absolute, so we need to make sure the audio_path is absolute\n",
    "#             # So we are check if the audio_path is absolute or relative in case we load the wrong data, if it's relative, we need to join it with the PROJECT_ROOT\n",
    "#             if not os.path.isabs(audio_path):\n",
    "#                 # Assumes path in manifest is relative to PROJECT_ROOT defined in config\n",
    "#                 audio_path = os.path.join(config.PROJECT_ROOT, audio_path)\n",
    "\n",
    "#             # --- 1. Load RAW Audio Waveform ---\n",
    "#             waveform, loaded_sr = librosa.load(\n",
    "#                 audio_path,\n",
    "#                 sr=self.target_sr, # Use extractor's sampling rate\n",
    "#                 duration=30.0      # Load the full 30 seconds\n",
    "#             )\n",
    "#             min_samples = int(0.1 * self.target_sr)\n",
    "#             if len(waveform) < min_samples:\n",
    "#                  raise ValueError(f\"Audio signal for track {track_id} too short after loading.\")\n",
    "\n",
    "#             # --- 2. Apply Feature Extractor ---\n",
    "#             # Let the Data Collator handle padding/truncation later\n",
    "#             inputs = self.feature_extractor(\n",
    "#                 waveform,\n",
    "#                 sampling_rate=self.target_sr,\n",
    "#                 return_tensors=\"pt\",\n",
    "#                 # REMOVED padding/truncation args\n",
    "#                 return_attention_mask=True # Keep requesting mask\n",
    "#             )\n",
    "\n",
    "#             # --- 3. Prepare Outputs ---\n",
    "#             feature_tensor = inputs.get('input_values', inputs.get('input_features'))\n",
    "#             if feature_tensor is None:\n",
    "#                 raise KeyError(f\"Expected 'input_values' or 'input_features' key from feature_extractor output for track {track_id}. Got keys: {inputs.keys()}\")\n",
    "#             feature_tensor = feature_tensor.squeeze(0)\n",
    "\n",
    "#             attention_mask = inputs.get('attention_mask', None)\n",
    "#             if attention_mask is not None:\n",
    "#                 attention_mask = attention_mask.squeeze(0)\n",
    "\n",
    "#             # Convert label list/array to float tensor\n",
    "#             label_tensor = torch.tensor(multi_hot_label, dtype=torch.float32)\n",
    "\n",
    "#             # Return dictionary\n",
    "#             model_input_dict = {\"labels\": label_tensor}\n",
    "#             input_key = 'input_values' if 'input_values' in inputs else 'input_features'\n",
    "#             model_input_dict[input_key] = feature_tensor\n",
    "#             if attention_mask is not None:\n",
    "#                 model_input_dict['attention_mask'] = attention_mask\n",
    "\n",
    "#             return model_input_dict\n",
    "\n",
    "#         except FileNotFoundError:\n",
    "#             logging.error(f\"Audio file not found for track {track_id} at {audio_path}\")\n",
    "#             # Return None ONLY IF collate_fn handles it, otherwise raise\n",
    "#             raise\n",
    "#         except Exception as e:\n",
    "#             logging.error(f\"Error in __getitem__ for track {track_id}: {e}\", exc_info=True)\n",
    "#             # Return None ONLY IF collate_fn handles it, otherwise raise\n",
    "#             raise\n",
    "\n",
    "# print(\"FMARawAudioDataset class defined (using raw audio, feature extractor, ast.literal_eval for labels).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FMARawAudioDataset class defined (using raw audio, feature extractor, ast.literal_eval for labels, .loc access).\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Dataset Class Definition (Raw Audio Version - Corrected .loc access)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import ast # For parsing label string '[1.0, 0.0,...]'\n",
    "import re  # Keep import for the commented out function below\n",
    "import logging\n",
    "import librosa\n",
    "# Ensure config is imported from a previous cell or uncomment:\n",
    "# import config\n",
    "\n",
    "# --- Optional: Keep custom parser commented out for reference ---\n",
    "# # Define(recollect)the regex parser from preprocess.py if needed,\n",
    "# # otherwise use ast.literal_eval---\n",
    "# # NOTE: After changing usage.ipynb 05/03/2025, should fall back to ast.literal_eval now. Clean code later\n",
    "# def parse_numpy_array_string(array_str):\n",
    "#     \"\"\"\n",
    "#     Parse strings like '[np.float32(1.0), np.float32(0.0), ...]' into a list of integers.\n",
    "#     This is needed because ast.literal_eval cannot handle 'np.float32()' in the string.\n",
    "#     \"\"\"\n",
    "#     if not isinstance(array_str, str): return []\n",
    "#     try:\n",
    "#         # Match digits, optionally followed by a decimal and more digits\n",
    "#         float_matches = re.findall(r'np\\.float32\\(([\\d\\.]+)\\)', array_str)\n",
    "#         values = []\n",
    "#         for match_str in float_matches:\n",
    "#             value = float(match_str) # Convert string match to float\n",
    "#             values.append(1.0 if value == 1.0 else 0.0) # Store as float 0.0 or 1.0\n",
    "#         return values\n",
    "#     except Exception as e:\n",
    "#         logging.warning(f\"Error parsing array string: {e}\")\n",
    "#         return []\n",
    "# --- End commented out parser ---\n",
    "\n",
    "\n",
    "class FMARawAudioDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads raw audio waveforms and labels from manifest, uses Hugging Face\n",
    "    feature extractor (like ASTFeatureExtractor or Wav2Vec2Processor/AutoFeatureExtractor) on the fly.\n",
    "    Assumes padding/truncation will be handled by a collate function.\n",
    "    \"\"\"\n",
    "    def __init__(self, manifest_path, feature_extractor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            manifest_path (str): Path to the final manifest CSV file (e.g., small_subset_multihot.csv).\n",
    "            feature_extractor: Initialized Hugging Face AutoFeatureExtractor instance.\n",
    "        \"\"\"\n",
    "        # Ensure num_labels is available globally or passed if needed for verification\n",
    "        global num_labels\n",
    "        if 'num_labels' not in globals():\n",
    "             logging.error(\"Global variable 'num_labels' not found. Load it first (e.g., from Cell 2).\")\n",
    "             # Alternative: pass num_labels as an argument to __init__\n",
    "\n",
    "        logging.info(f\"Initializing FMARawAudioDataset from: {manifest_path}\")\n",
    "        if feature_extractor is None:\n",
    "             raise ValueError(\"FMARawAudioDataset requires a feature_extractor instance.\")\n",
    "\n",
    "        self.feature_extractor = feature_extractor\n",
    "        try:\n",
    "             self.target_sr = self.feature_extractor.sampling_rate\n",
    "             logging.info(f\"Target sampling rate set from feature extractor: {self.target_sr} Hz\")\n",
    "        except AttributeError:\n",
    "             logging.error(\"Could not get sampling_rate from feature_extractor.\", exc_info=True)\n",
    "             raise\n",
    "\n",
    "        logging.info(f\"Loading manifest from: {manifest_path}\")\n",
    "        try:\n",
    "            self.manifest = pd.read_csv(manifest_path)\n",
    "            # Set index to track_id AFTER loading, keep column too if needed elsewhere\n",
    "            if 'track_id' in self.manifest.columns:\n",
    "                 self.manifest = self.manifest.set_index('track_id', drop=False) # Keep column if row.get('track_id'...) is used\n",
    "            else:\n",
    "                 logging.warning(\"Manifest CSV does not contain 'track_id' column. Using DataFrame index.\")\n",
    "                 # Make sure index IS the track_id\n",
    "                 if not pd.api.types.is_integer_dtype(self.manifest.index):\n",
    "                      logging.warning(\"Manifest index is not integer type. Ensure it matches track IDs.\")\n",
    "\n",
    "\n",
    "            # --- Parse the 'multi_hot_label' string back into a list ---\n",
    "            # NOTE: After changing usage.ipynb 05/03/2025, should fall back to ast.literal_eval now. Clean code later\n",
    "            # Use ast.literal_eval assuming labels were saved as standard list strings '[1.0, 0.0,...]'\n",
    "            logging.info(\"Attempting to parse 'multi_hot_label' column using ast.literal_eval...\")\n",
    "            label_parser = ast.literal_eval # <<<--- Using ast.literal_eval\n",
    "            # label_parser = parse_numpy_array_string # Keep commented out as requested\n",
    "\n",
    "            label_col_name = 'multi_hot_label'\n",
    "            if label_col_name not in self.manifest.columns:\n",
    "                 raise KeyError(f\"Column '{label_col_name}' not found in manifest CSV at {manifest_path}\")\n",
    "\n",
    "            self.manifest[label_col_name] = self.manifest[label_col_name].apply(label_parser)\n",
    "\n",
    "            # --- Verification step ---\n",
    "            first_label = self.manifest[label_col_name].iloc[0] # Use iloc[0] here to get FIRST row for checking\n",
    "            if not isinstance(first_label, list):\n",
    "                 raise TypeError(f\"Parsed label is not a list, check parser/CSV format. Got type: {type(first_label)}\")\n",
    "            # Check length against num_labels loaded in Cell 2\n",
    "            if len(first_label) != num_labels:\n",
    "                 logging.error(f\"FATAL: Parsed label length ({len(first_label)}) does not match expected num_labels ({num_labels}). Check parsing or unified_genres.txt.\")\n",
    "                 raise ValueError(\"Parsed label length mismatch.\")\n",
    "            logging.info(f\"Example parsed label verified (type {type(first_label)}, length {len(first_label)}): {str(first_label)[:100]}...\")\n",
    "            # --- End Verification ---\n",
    "\n",
    "            logging.info(f\"Loaded and parsed manifest with {len(self.manifest)} entries.\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "             logging.error(f\"Manifest file not found: {manifest_path}\", exc_info=True)\n",
    "             raise\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading or parsing manifest {manifest_path}: {e}\", exc_info=True)\n",
    "            raise\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.manifest)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Loads raw audio for index 'idx' (which is the track_id/index label),\n",
    "        processes it with the feature extractor,\n",
    "        and returns the processed inputs and labels.\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx): idx = idx.tolist() # Handle tensor indices\n",
    "\n",
    "        # --- Use idx directly as track_id BEFORE main try block ---\n",
    "        track_id = idx\n",
    "        # ---------------------------------------------------------\n",
    "\n",
    "        try:\n",
    "            # --- Get the row data using .loc with the track_id ---\n",
    "            row = self.manifest.loc[track_id] # Use .loc with the index label (track_id)\n",
    "            # ------------------------------------------------------\n",
    "\n",
    "            # --- Get required data from the row ---\n",
    "            multi_hot_label = row['multi_hot_label']\n",
    "            audio_path = row['audio_path']\n",
    "            # ---------------------------------------\n",
    "\n",
    "            # Construct absolute audio path if necessary (keep your NOTE)\n",
    "            # NOTE: originally, the mel-spectrogram's path is relative  but the raw audio's path is absolute, so we need to make sure the audio_path is absolute\n",
    "            # So we are check if the audio_path is absolute or relative in case we load the wrong data, if it's relative, we need to join it with the PROJECT_ROOT\n",
    "            if not os.path.isabs(audio_path):\n",
    "                audio_path = os.path.join(config.PROJECT_ROOT, audio_path)\n",
    "\n",
    "            # --- 1. Load RAW Audio Waveform ---\n",
    "            waveform, loaded_sr = librosa.load(\n",
    "                audio_path,\n",
    "                sr=self.target_sr, # Use extractor's sampling rate\n",
    "                duration=30.0      # Load the full 30 seconds\n",
    "            )\n",
    "            min_samples = int(0.1 * self.target_sr)\n",
    "            if len(waveform) < min_samples:\n",
    "                 logging.warning(f\"Audio signal for track {track_id} too short, returning None.\")\n",
    "                 return None # Requires collate_fn to handle None\n",
    "\n",
    "            # --- 2. Apply Feature Extractor ---\n",
    "            # Let the Data Collator handle padding/truncation later\n",
    "            inputs = self.feature_extractor(\n",
    "                waveform,\n",
    "                sampling_rate=self.target_sr,\n",
    "                return_tensors=\"pt\",\n",
    "                # REMOVED padding/truncation/max_length args\n",
    "                return_attention_mask=True # Keep requesting mask\n",
    "            )\n",
    "\n",
    "            # --- 3. Prepare Outputs ---\n",
    "            feature_tensor = inputs.get('input_values', inputs.get('input_features'))\n",
    "            if feature_tensor is None:\n",
    "                raise KeyError(f\"Expected 'input_values' or 'input_features' key from feature_extractor output. Got keys: {inputs.keys()}\")\n",
    "            feature_tensor = feature_tensor.squeeze(0)\n",
    "\n",
    "            attention_mask = inputs.get('attention_mask', None)\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = attention_mask.squeeze(0)\n",
    "\n",
    "            # Convert label list to float tensor\n",
    "            label_tensor = torch.tensor(multi_hot_label, dtype=torch.float32)\n",
    "\n",
    "            # Return dictionary\n",
    "            model_input_dict = {\"labels\": label_tensor}\n",
    "            input_key = 'input_values' if 'input_values' in inputs else 'input_features'\n",
    "            model_input_dict[input_key] = feature_tensor\n",
    "            if attention_mask is not None:\n",
    "                model_input_dict['attention_mask'] = attention_mask\n",
    "\n",
    "            return model_input_dict\n",
    "\n",
    "        except KeyError:\n",
    "             # This might catch if track_id wasn't found by .loc (handled above),\n",
    "             # or if column names like 'multi_hot_label', 'audio_path' are wrong in CSV\n",
    "             logging.error(f\"KeyError accessing data for track {track_id}. Check manifest columns.\", exc_info=True)\n",
    "             return None\n",
    "        except FileNotFoundError:\n",
    "             logging.error(f\"Audio file not found for track {track_id} at {audio_path}\")\n",
    "             return None\n",
    "        except Exception as e:\n",
    "            # Use the track_id obtained safely before the try block\n",
    "            logging.error(f\"Error loading/processing track {track_id}: {e}\", exc_info=True)\n",
    "            return None # Return None on generic error\n",
    "\n",
    "print(\"FMARawAudioDataset class defined (using raw audio, feature extractor, ast.literal_eval for labels, .loc access).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "facebook/w2v-bert-2.0\n"
     ]
    }
   ],
   "source": [
    "print(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 3.5: Define Data Collator for Padding (Corrected Padding Logic)\n",
    "\n",
    "# import torch\n",
    "# from dataclasses import dataclass\n",
    "# from typing import Dict, List, Optional, Union\n",
    "# # from transformers.feature_extraction_utils import BatchFeature # Not strictly needed here\n",
    "\n",
    "# @dataclass\n",
    "# class DataCollatorAudio:\n",
    "#     \"\"\"\n",
    "#     Data collator that dynamically pads the inputs received Feature Extractor.\n",
    "#     Correctly handles padding for [SequenceLength, FeatureDim] tensors.\n",
    "#     \"\"\"\n",
    "#     padding_value: float = 0.0 # Standard padding for features/audio\n",
    "\n",
    "#     def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "#         # features is a list of dicts like [{'input_values': tensor1, 'labels': label1, 'attention_mask': mask1}, ...]\n",
    "\n",
    "#         # --- Pad 'input_values' (or 'input_features') ---\n",
    "#         input_key = 'input_values' if 'input_values' in features[0] else 'input_features'\n",
    "#         input_features = [d[input_key] for d in features]\n",
    "\n",
    "#         # Determine max sequence length *in this batch* (assuming shape [SeqLen, FeatureDim])\n",
    "#         # Add check for empty list\n",
    "#         if not input_features:\n",
    "#              return {}\n",
    "#         max_len = max(feat.shape[0] for feat in input_features) # <<<--- Get length of FIRST dimension\n",
    "\n",
    "#         # Pad each feature tensor to max_len along the sequence dimension (first dim)\n",
    "#         padded_features = []\n",
    "#         for feat in input_features:\n",
    "#             # feat shape is [SeqLen, FeatureDim]\n",
    "#             num_frames = feat.shape[0]\n",
    "#             num_features = feat.shape[1] # Should be consistent (e.g., 160)\n",
    "#             pad_width = max_len - num_frames\n",
    "\n",
    "#             # Pad argument format for 2D tensor: (pad_left_dim1, pad_right_dim1, pad_left_dim0, pad_right_dim0)\n",
    "#             # We only want to pad the end of the sequence dimension (dim 0)\n",
    "#             # (0, 0) means no padding on left/right of feature dim (dim 1)\n",
    "#             # (0, pad_width) means 0 padding before seq dim (dim 0), pad_width padding after\n",
    "#             padded_feat = torch.nn.functional.pad(feat, (0, 0, 0, pad_width), mode='constant', value=self.padding_value)\n",
    "#             # Verify shape after padding\n",
    "#             # print(f\"Original shape: {feat.shape}, Padded shape: {padded_feat.shape}, Target max_len: {max_len}\")\n",
    "#             padded_features.append(padded_feat)\n",
    "\n",
    "#         # Stack the padded features into a batch tensor\n",
    "#         # Now all tensors in padded_features should have shape [max_len, FeatureDim]\n",
    "#         try:\n",
    "#              batch_input_features = torch.stack(padded_features) # Shape: [BatchSize, max_len, FeatureDim]\n",
    "#         except RuntimeError as e:\n",
    "#              logging.error(f\"RuntimeError during torch.stack. Shapes in batch might still differ or be incompatible.\")\n",
    "#              # Print shapes for debugging\n",
    "#              for i, p_feat in enumerate(padded_features): logging.error(f\" Padded shape {i}: {p_feat.shape}\")\n",
    "#              raise e\n",
    "\n",
    "\n",
    "#         # --- Prepare Batch Dictionary ---\n",
    "#         batch = {\"input_values\": batch_input_features}\n",
    "\n",
    "#         # --- Pad 'attention_mask' if present ---\n",
    "#         # Attention mask usually has shape [SeqLen]\n",
    "#         if \"attention_mask\" in features[0] and features[0][\"attention_mask\"] is not None:\n",
    "#             attention_masks = [d[\"attention_mask\"] for d in features]\n",
    "#             padded_masks = []\n",
    "#             for mask in attention_masks:\n",
    "#                  pad_width = max_len - mask.shape[-1] # Pad last dimension (the sequence length)\n",
    "#                  # Pad argument format for 1D tensor: (pad_left, pad_right)\n",
    "#                  padded_mask = torch.nn.functional.pad(mask, (0, pad_width), mode='constant', value=0) # Pad attention mask with 0\n",
    "#                  padded_masks.append(padded_mask)\n",
    "#             batch[\"attention_mask\"] = torch.stack(padded_masks) # Shape: [BatchSize, max_len]\n",
    "\n",
    "#         # --- Stack Labels ---\n",
    "#         labels = [d[\"labels\"] for d in features]\n",
    "#         batch[\"labels\"] = torch.stack(labels) # Shape: [BatchSize, num_labels]\n",
    "\n",
    "#         return batch\n",
    "\n",
    "# # Create an instance of the collator (do this in Cell 4)\n",
    "# # data_collator = DataCollatorAudio()\n",
    "# # print(\"DataCollatorAudio defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3.5: Define Data Collator for Padding (Handles None values)\n",
    "\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Union\n",
    "import logging # Add logging\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorAudio:\n",
    "    \"\"\"\n",
    "    Data collator that dynamically pads the inputs received Feature Extractor.\n",
    "    Handles None values returned by the Dataset on error.\n",
    "    \"\"\"\n",
    "    padding_value: float = 0.0 # Standard padding for features/audio\n",
    "\n",
    "    def __call__(self, features: List[Optional[Dict[str, Union[List[int], torch.Tensor]]]]) -> Dict[str, torch.Tensor]:\n",
    "        # features is a list of dicts OR None values from __getitem__\n",
    "\n",
    "        # --- Filter out None entries ---\n",
    "        valid_features = [f for f in features if f is not None]\n",
    "        if not valid_features:\n",
    "             # If all samples in the batch failed, return an empty dictionary\n",
    "             # The training loop should ideally handle this (e.g., skip batch)\n",
    "             logging.warning(\"Collate function received empty batch after filtering Nones.\")\n",
    "             return {}\n",
    "        # -----------------------------\n",
    "\n",
    "        # --- Determine keys and pad based on valid features ---\n",
    "        input_key = 'input_values' if 'input_values' in valid_features[0] else 'input_features'\n",
    "        input_features = [d[input_key] for d in valid_features]\n",
    "\n",
    "        # Determine sequence length dimension based on the FIRST valid tensor\n",
    "        seq_len_dim = -1\n",
    "        if len(input_features[0].shape) == 2:\n",
    "            seq_len_dim = 0 if input_features[0].shape[0] > input_features[0].shape[1] else -1\n",
    "        elif len(input_features[0].shape) == 1:\n",
    "             seq_len_dim = 0\n",
    "        else:\n",
    "             logging.warning(f\"Unexpected tensor shape {input_features[0].shape}, assuming seq len is last dim.\")\n",
    "\n",
    "        max_len = max(feat.shape[seq_len_dim] for feat in input_features)\n",
    "\n",
    "        # Pad each feature tensor to max_len\n",
    "        padded_features = []\n",
    "        for feat in input_features:\n",
    "            pad_width = max_len - feat.shape[seq_len_dim]\n",
    "            if seq_len_dim == 0 and len(feat.shape)==2: padding = (0, 0, 0, pad_width) # Pad SeqLen dim (dim 0)\n",
    "            else: padding = (0, pad_width) # Pad last dim (SeqLen)\n",
    "\n",
    "            padded_feat = torch.nn.functional.pad(feat, padding, mode='constant', value=self.padding_value)\n",
    "            padded_features.append(padded_feat)\n",
    "\n",
    "        # Stack the padded features\n",
    "        batch_input_features = torch.stack(padded_features)\n",
    "        batch = {input_key: batch_input_features} # Use the correct key\n",
    "\n",
    "        # Pad 'attention_mask' if present\n",
    "        if \"attention_mask\" in valid_features[0] and valid_features[0][\"attention_mask\"] is not None:\n",
    "            attention_masks = [d[\"attention_mask\"] for d in valid_features]\n",
    "            # Assuming mask is 1D [SeqLen] or 2D [1, SeqLen] etc. - pad last dim\n",
    "            max_mask_len = max(m.shape[-1] for m in attention_masks)\n",
    "            padded_masks = []\n",
    "            for mask in attention_masks:\n",
    "                 pad_width = max_mask_len - mask.shape[-1]\n",
    "                 padded_mask = torch.nn.functional.pad(mask, (0, pad_width), mode='constant', value=0)\n",
    "                 padded_masks.append(padded_mask)\n",
    "            batch[\"attention_mask\"] = torch.stack(padded_masks)\n",
    "\n",
    "        # Stack Labels\n",
    "        labels = [d[\"labels\"] for d in valid_features]\n",
    "        batch[\"labels\"] = torch.stack(labels)\n",
    "\n",
    "        return batch\n",
    "\n",
    "# Create an instance of the collator (do this in Cell 4)\n",
    "# data_collator = DataCollatorAudio()\n",
    "# print(\"DataCollatorAudio defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-03 17:34:10,489 - INFO - Loading feature extractor for: facebook/w2v-bert-2.0\n",
      "2025-05-03 17:34:10,622 - INFO - Feature extractor loaded successfully.\n",
      "Feature extractor expects sample rate: 16000\n",
      "2025-05-03 17:34:10,624 - INFO - Initializing FMARawAudioDataset from: /workspace/musicClaGen/data/processed/small_subset_multihot.csv\n",
      "2025-05-03 17:34:10,625 - INFO - Target sampling rate set from feature extractor: 16000 Hz\n",
      "2025-05-03 17:34:10,628 - INFO - Loading manifest from: /workspace/musicClaGen/data/processed/small_subset_multihot.csv\n",
      "2025-05-03 17:34:10,676 - INFO - Attempting to parse 'multi_hot_label' column using ast.literal_eval...\n",
      "2025-05-03 17:34:10,999 - INFO - Example parsed label verified (type <class 'list'>, length 22): [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "2025-05-03 17:34:11,000 - INFO - Loaded and parsed manifest with 8000 entries.\n",
      "2025-05-03 17:34:11,001 - INFO - Creating DEBUG DataLoaders with small subsets and custom collator...\n",
      "DataCollatorAudio instance created.\n",
      "2025-05-03 17:34:11,008 - INFO - DEBUG Dataset sizes: Train=16, Val=8\n",
      "2025-05-03 17:34:11,009 - INFO - DEBUG DataLoaders with custom collator created.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load Feature Extractor, Create DataLoaders with Custom Collator\n",
    "\n",
    "from transformers import AutoFeatureExtractor # Use the correct class\n",
    "\n",
    "# Ensure FMARawAudioDataset and DataCollatorAudio are defined in previous cells\n",
    "\n",
    "# --- Load Feature Extractor ---\n",
    "# (Using model_checkpoint defined in Cell 2)\n",
    "logging.info(f\"Loading feature extractor for: {model_checkpoint}\")\n",
    "try:\n",
    "    # Load the feature extractor associated with Wav2Vec2-BERT\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint)\n",
    "    logging.info(\"Feature extractor loaded successfully.\")\n",
    "    # Log the expected sample rate\n",
    "    processor_sr = feature_extractor.sampling_rate\n",
    "    print(f\"Feature extractor expects sample rate: {processor_sr}\")\n",
    "    # Ensure config matches extractor's expected rate\n",
    "    if config.PREPROCESSING_PARAMS['sample_rate'] != processor_sr:\n",
    "         logging.warning(f\"Config sample rate ({config.PREPROCESSING_PARAMS['sample_rate']}) differs from feature extractor ({processor_sr}). Ensure audio loading uses {processor_sr} Hz.\")\n",
    "         # Update config value if necessary, or ensure Dataset uses processor_sr\n",
    "         # config.PREPROCESSING_PARAMS['sample_rate'] = processor_sr # Be careful modifying config dynamically\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Could not load feature extractor for {model_checkpoint}. Cannot proceed. Error: {e}\", exc_info=True)\n",
    "    raise SystemExit # Stop execution if extractor fails\n",
    "\n",
    "# --- Create Full Dataset ---\n",
    "# Ensure FMARawAudioDataset __init__ accepts feature_extractor\n",
    "try:\n",
    "    # Pass the loaded feature_extractor instance\n",
    "    full_dataset = FMARawAudioDataset(manifest_path, feature_extractor=feature_extractor)\n",
    "    manifest_df = full_dataset.manifest\n",
    "except Exception as e:\n",
    "     logging.error(\"Failed to instantiate FMARawAudioDataset.\", exc_info=True)\n",
    "     raise SystemExit\n",
    "\n",
    "# --- Create SMALLER DEBUG Datasets ---\n",
    "logging.info(\"Creating DEBUG DataLoaders with small subsets and custom collator...\")\n",
    "try:\n",
    "    # Get indices for the splits from the manifest\n",
    "    train_indices = manifest_df[manifest_df['split'] == 'training'].index[:16].tolist() # Small subset for debug\n",
    "    val_indices = manifest_df[manifest_df['split'] == 'validation'].index[:8].tolist()  # Small subset for debug\n",
    "\n",
    "    # Create Subset instances\n",
    "    debug_train_dataset = Subset(full_dataset, train_indices)\n",
    "    debug_val_dataset = Subset(full_dataset, val_indices)\n",
    "\n",
    "    # --- Create Data Collator Instance ---\n",
    "    # (Assumes DataCollatorAudio class is defined in Cell 3.5)\n",
    "    data_collator = DataCollatorAudio()\n",
    "    print(\"DataCollatorAudio instance created.\")\n",
    "\n",
    "    # --- Create DataLoaders using the custom collate_fn ---\n",
    "    debug_train_dataloader = DataLoader(\n",
    "        debug_train_dataset,\n",
    "        batch_size=batch_size, # Use small batch_size from config\n",
    "        shuffle=True,\n",
    "        collate_fn=data_collator # Apply custom padding at batch level\n",
    "        # num_workers=4, # Optional: Add workers later for performance\n",
    "        # pin_memory=True # Optional: Add if using GPU\n",
    "    )\n",
    "    debug_val_dataloader = DataLoader(\n",
    "        debug_val_dataset,\n",
    "        batch_size=batch_size, # Use small batch_size from config\n",
    "        shuffle=False, # No need to shuffle validation data\n",
    "        collate_fn=data_collator # Apply custom padding at batch level\n",
    "        # num_workers=4,\n",
    "        # pin_memory=True\n",
    "    )\n",
    "    logging.info(f\"DEBUG Dataset sizes: Train={len(debug_train_dataset)}, Val={len(debug_val_dataset)}\")\n",
    "    logging.info(\"DEBUG DataLoaders with custom collator created.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to create DEBUG datasets/dataloaders: {e}\", exc_info=True)\n",
    "    raise SystemExit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-03 17:34:11,025 - INFO - Loading pre-trained Wav2Vec2-BERT model: facebook/w2v-bert-2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2BertForSequenceClassification were not initialized from the model checkpoint at facebook/w2v-bert-2.0 and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-03 17:34:13,482 - INFO - Model loaded initially.\n",
      "2025-05-03 17:34:13,484 - INFO - Found classifier attribute 'classifier' of type <class 'torch.nn.modules.linear.Linear'>\n",
      "2025-05-03 17:34:13,485 - INFO - Replacing classifier head 'classifier'. Original out: 22, New out: 22\n",
      "Successfully replaced classifier head 'classifier'.\n",
      "2025-05-03 17:34:14,626 - INFO - Wav2Vec2-BERT Model loaded and moved to device.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Load Wav2Vec2-BERT Model and Modify Head\n",
    "\n",
    "import torch.nn as nn # Ensure nn is imported\n",
    "from transformers import AutoModelForAudioClassification\n",
    "\n",
    "logging.info(f\"Loading pre-trained Wav2Vec2-BERT model: {model_checkpoint}\")\n",
    "try:\n",
    "    # Load the model configured for audio classification\n",
    "    model = AutoModelForAudioClassification.from_pretrained(\n",
    "        model_checkpoint,\n",
    "        num_labels=num_labels,\n",
    "        ignore_mismatched_sizes=True # Essential for replacing the head\n",
    "    )\n",
    "    logging.info(\"Model loaded initially.\")\n",
    "\n",
    "    # --- Explicit Head Replacement (Recommended) ---\n",
    "    # Though I have defined num_labels = num_labels on previous step, I want to explicitly replace it again to ensure the head is correct.\n",
    "    # If the above code is correct, the explicitly approach below might seem redundant but.\n",
    "    \n",
    "    # I MUST verify the correct attribute name for the classifier head for Wav2Vec2-BERT. \n",
    "    # Common names include 'classifier', 'projector','classification_head'. Use print(model) after loading to check.\n",
    "    classifier_attr = 'classifier' # <<<--- VERIFY THIS ATTRIBUTE NAME ---<<<\n",
    "\n",
    "    if hasattr(model, classifier_attr):\n",
    "        original_classifier = getattr(model, classifier_attr)\n",
    "        logging.info(f\"Found classifier attribute '{classifier_attr}' of type {type(original_classifier)}\")\n",
    "\n",
    "        # Check if it's a simple Linear layer or potentially a sequence/projection\n",
    "        if isinstance(original_classifier, nn.Linear):\n",
    "            in_features = original_classifier.in_features\n",
    "            logging.info(f\"Replacing classifier head '{classifier_attr}'. Original out: {original_classifier.out_features}, New out: {num_labels}\")\n",
    "            setattr(model, classifier_attr, nn.Linear(in_features, num_labels))\n",
    "            print(f\"Successfully replaced classifier head '{classifier_attr}'.\")\n",
    "        # Add checks here if Wav2Vec2-BERT uses a different common head structure\n",
    "        # elif isinstance(original_classifier, nn.Sequential): ... etc.\n",
    "        else:\n",
    "             logging.warning(f\"Classifier head '{classifier_attr}' is not nn.Linear ({type(original_classifier)}). Attempting replacement might fail or need adjustment.\")\n",
    "             # If you know the structure (e.g., model.projector + model.classifier), adjust accordingly.\n",
    "             # For now, we assume a direct replacement might work or the implicit loading handled it.\n",
    "\n",
    "    else:\n",
    "         logging.warning(f\"Could not automatically find classifier attribute '{classifier_attr}'. Ensure head size ({num_labels}) was correctly set via 'num_labels' argument during loading or modify manually.\")\n",
    "\n",
    "    model.to(device)\n",
    "    logging.info(\"Wav2Vec2-BERT Model loaded and moved to device.\")\n",
    "    # print(model) # Uncomment this line and run to inspect the model structure and find the classifier name\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to load model '{model_checkpoint}': {e}\", exc_info=True)\n",
    "    raise SystemExit # Stop if model loading fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the correct attribute name for the classifier head for Wav2Vec2-BERT.\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer, Loss, Metrics Functoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-03 17:34:14,683 - INFO - Optimizer AdamW defined with LR=5e-05, Weight Decay=0.01\n",
      "2025-05-03 17:34:14,686 - INFO - Loss function BCEWithLogitsLoss defined.\n",
      "Optimizer, Loss, and compute_metrics function defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Define Optimizer, Loss Function, and Metrics Calculation\n",
    "\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import hamming_loss, jaccard_score, f1_score # Make sure these are imported\n",
    "\n",
    "# --- Optimizer ---\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "logging.info(f\"Optimizer AdamW defined with LR={learning_rate}, Weight Decay={weight_decay}\")\n",
    "\n",
    "# --- Loss Function ---\n",
    "# Use BCEWithLogitsLoss for multi-label classification (includes Sigmoid)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "logging.info(\"Loss function BCEWithLogitsLoss defined.\")\n",
    "\n",
    "# --- Metrics Function ---\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"Calculates multi-label metrics from logits and labels.\"\"\"\n",
    "    logits, labels = eval_preds\n",
    "    # Ensure inputs are numpy arrays on CPU\n",
    "    logits_np = logits.detach().cpu().numpy() if isinstance(logits, torch.Tensor) else logits\n",
    "    labels_np = labels.detach().cpu().numpy() if isinstance(labels, torch.Tensor) else labels\n",
    "\n",
    "    # Apply sigmoid and threshold\n",
    "    probs = 1 / (1 + np.exp(-logits_np)) # Manual sigmoid\n",
    "    preds = (probs > 0.5).astype(int)\n",
    "    labels_np = labels_np.astype(int) # Ensure labels are integers\n",
    "\n",
    "    if labels_np.shape != preds.shape:\n",
    "         logging.error(f\"Shape mismatch in compute_metrics! Labels: {labels_np.shape}, Preds: {preds.shape}\")\n",
    "         # Return default metrics indicating failure\n",
    "         return {'hamming_loss': 1.0, 'jaccard_samples': 0.0, 'f1_micro': 0.0, 'f1_macro': 0.0}\n",
    "\n",
    "    metrics = {}\n",
    "    try:\n",
    "        metrics['hamming_loss'] = hamming_loss(labels_np, preds)\n",
    "        # Use average='samples' for Jaccard in multi-label scenario\n",
    "        metrics['jaccard_samples'] = jaccard_score(labels_np, preds, average='samples', zero_division=0)\n",
    "        metrics['f1_micro'] = f1_score(labels_np, preds, average='micro', zero_division=0)\n",
    "        metrics['f1_macro'] = f1_score(labels_np, preds, average='macro', zero_division=0)\n",
    "        # Optional: Add Accuracy (subset accuracy)\n",
    "        # metrics['accuracy'] = accuracy_score(labels_np, preds) # This is exact match accuracy\n",
    "    except Exception as e:\n",
    "         logging.error(f\"Error calculating metrics: {e}\")\n",
    "         metrics = {'hamming_loss': 1.0, 'jaccard_samples': 0.0, 'f1_micro': 0.0, 'f1_macro': 0.0}\n",
    "\n",
    "    # Log inside the main evaluate function now for better context\n",
    "    # logging.info(f\"Metrics: Hamming={metrics['hamming_loss']:.4f}, Jaccard(samples)={metrics['jaccard_samples']:.4f}, F1 Micro={metrics['f1_micro']:.4f}, F1 Macro={metrics['f1_macro']:.4f}\")\n",
    "    return metrics\n",
    "\n",
    "print(\"Optimizer, Loss, and compute_metrics function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 7: Define Training Function for One Epoch \n",
    "\n",
    "# def train_epoch(model, dataloader, criterion, optimizer, device, gradient_accumulation_steps):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     num_samples = 0\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "#     for step, batch in enumerate(progress_bar):\n",
    "#         try:\n",
    "#             # --- CORRECTED INPUT PREPARATION ---\n",
    "#             expected_model_input_key = \"input_features\"  \n",
    "\n",
    "#             if 'input_values' not in batch: # Check if extractor output key is different\n",
    "#                  raise KeyError(\"Batch dictionary missing 'input_values' from Dataset/Extractor.\")\n",
    "\n",
    "#             # Build the dictionary for the model's forward pass\n",
    "#             model_inputs = {\n",
    "#                 expected_model_input_key: batch['input_values'].to(device) # Map dataset output key to model input key\n",
    "#             }\n",
    "#             if 'attention_mask' in batch and batch['attention_mask'] is not None:\n",
    "#                  model_inputs['attention_mask'] = batch['attention_mask'].to(device)\n",
    "#             # --- END CORRECTION ---\n",
    "\n",
    "#             labels = batch['labels'].to(device)\n",
    "\n",
    "#             # Forward pass\n",
    "#             outputs = model(**model_inputs) # Pass the correctly named arguments\n",
    "#             logits = outputs.logits\n",
    "\n",
    "#             # Calculate loss\n",
    "#             loss = criterion(logits, labels)\n",
    "\n",
    "#             # ... (rest of loss scaling, backward, optimizer step remains the same) ...\n",
    "#             if torch.isnan(loss):\n",
    "#                 logging.warning(f\"NaN loss detected at step {step}. Skipping batch.\")\n",
    "#                 if (step + 1) % gradient_accumulation_steps != 0: model.zero_grad()\n",
    "#                 continue\n",
    "#             scaled_loss = loss / gradient_accumulation_steps\n",
    "#             scaled_loss.backward()\n",
    "#             batch_size_actual = labels.size(0)\n",
    "#             total_loss += loss.item() * batch_size_actual\n",
    "#             num_samples += batch_size_actual\n",
    "#             if (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == len(dataloader):\n",
    "#                 optimizer.step()\n",
    "#                 optimizer.zero_grad()\n",
    "#             progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "#         except Exception as e:\n",
    "#              logging.error(f\"Error during training step {step}, batch keys: {batch.keys()}. Error: {e}\", exc_info=True)\n",
    "#              continue\n",
    "\n",
    "#     if (step + 1) % gradient_accumulation_steps != 0 and num_samples > 0: # Ensure step was defined\n",
    "#          optimizer.step(); optimizer.zero_grad()\n",
    "\n",
    "#     avg_loss = total_loss / num_samples if num_samples > 0 else 0\n",
    "#     print(f\"\\nAverage Training Loss for Epoch: {avg_loss:.4f}\")\n",
    "#     return avg_loss\n",
    "\n",
    "# print(\"train_epoch function updated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 7: Define Training Function for One Epoch (with AMP)\n",
    "\n",
    "# from torch.cuda.amp import autocast, GradScaler # Import AMP utilities\n",
    "\n",
    "\n",
    "# # def train_epoch(model, dataloader, criterion, optimizer, device, gradient_accumulation_steps, scaler): # Add scaler argument\n",
    "# #     model.train() # Set model to training mode\n",
    "# #     total_loss = 0\n",
    "# #     num_samples = 0\n",
    "# #     optimizer.zero_grad() # Zero gradients once before the epoch loop\n",
    "\n",
    "# #     progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "# #     for step, batch in enumerate(progress_bar):\n",
    "# #         if batch is None or not batch: continue # Skip potentially None batches if Dataset has errors\n",
    "\n",
    "# #         try:\n",
    "# #             # Uses 'input_features' as the model's expected key based on previous debugging\n",
    "# #             expected_model_input_key = \"input_features\"\n",
    "# #             input_data_key = 'input_values' if 'input_values' in batch else 'input_features' # Key from feature extractor output\n",
    "\n",
    "# #             model_inputs = {}\n",
    "# #             if input_data_key in batch:\n",
    "# #                  model_inputs[expected_model_input_key] = batch[input_data_key].to(device)\n",
    "# #             else:\n",
    "# #                  raise KeyError(f\"Neither 'input_values' nor 'input_features' found in batch.\")\n",
    "\n",
    "# #             if 'attention_mask' in batch and batch['attention_mask'] is not None:\n",
    "# #                  model_inputs['attention_mask'] = batch['attention_mask'].to(device)\n",
    "\n",
    "# #             labels = batch['labels'].to(device) # Keep labels separate\n",
    "\n",
    "# #             # --- Automatic Mixed Precision ---\n",
    "# #             with autocast(device_type=device.type): # Runs forward pass and loss in mixed precision\n",
    "# #                 outputs = model(**model_inputs)\n",
    "# #                 logits = outputs.logits\n",
    "# #                 loss = criterion(logits, labels) # BCEWithLogitsLoss expects float labels\n",
    "\n",
    "# #                 # Check for NaN loss immediately after calculation\n",
    "# #                 if torch.isnan(loss):\n",
    "# #                     logging.warning(f\"NaN loss detected at step {step} *inside autocast*. Skipping batch.\")\n",
    "# #                     # Need to zero grad if skipping before optimizer step in accumulation cycle\n",
    "# #                     if (step + 1) % gradient_accumulation_steps != 0:\n",
    "# #                         optimizer.zero_grad() # Zero grad to prevent NaN propagation\n",
    "# #                     continue # Skip backward and optimizer step\n",
    "\n",
    "# #                 # Scale loss for gradient accumulation BEFORE scaler.scale()\n",
    "# #                 scaled_loss = loss / gradient_accumulation_steps\n",
    "# #             # --- End Autocast ---\n",
    "\n",
    "# #             # --- Scaler Scales the loss and Calls backward() ---\n",
    "# #             scaler.scale(scaled_loss).backward()\n",
    "# #             # ---------------------------------------------\n",
    "\n",
    "# #             # Accumulate total loss (use the original non-scaled loss for tracking)\n",
    "# #             batch_size_actual = labels.size(0)\n",
    "# #             total_loss += loss.item() * batch_size_actual\n",
    "# #             num_samples += batch_size_actual\n",
    "\n",
    "# #             # --- Optimizer Step (with Scaler) ---\n",
    "# #             if (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == len(dataloader):\n",
    "# #                 # Optional: Unscale gradients before clipping (if clipping)\n",
    "# #                 # scaler.unscale_(optimizer)\n",
    "# #                 # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "# #                 scaler.step(optimizer) # Unscales gradients, steps optimizer, checks for inf/NaN\n",
    "# #                 scaler.update() # Update scaler for next iteration\n",
    "# #                 optimizer.zero_grad() # Zero gradients *after* stepping or skipping step\n",
    "# #             # -----------------------------------\n",
    "\n",
    "# #             # Update progress bar description with non-scaled loss\n",
    "# #             progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "# #         except Exception as e:\n",
    "# #              logging.error(f\"Error during training step {step}, batch keys: {batch.keys()}. Error: {e}\", exc_info=True)\n",
    "# #              # Ensure gradients are zeroed if an error occurs mid-accumulation cycle\n",
    "# #              optimizer.zero_grad()\n",
    "# #              continue # Skip this batch on error\n",
    "\n",
    "# #     # Final calculation should use accumulated totals\n",
    "# #     avg_loss = total_loss / num_samples if num_samples > 0 else 0\n",
    "# #     print(f\"\\nAverage Training Loss for Epoch: {avg_loss:.4f}\")\n",
    "# #     return avg_loss\n",
    "\n",
    "\n",
    "# def train_epoch(model, dataloader, criterion, optimizer, device, gradient_accumulation_steps, scaler): # Add scaler argument\n",
    "#     model.train() # Set model to training mode\n",
    "#     total_loss = 0\n",
    "#     num_samples = 0\n",
    "#     optimizer.zero_grad() # Zero gradients once before the epoch loop\n",
    "    \n",
    "#     # Add counters for debugging\n",
    "#     successful_batches = 0\n",
    "#     print(f\"Starting training with {len(dataloader)} batches\")\n",
    "\n",
    "#     progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "#     for step, batch in enumerate(progress_bar):\n",
    "#         if batch is None or not batch: \n",
    "#             print(f\"Skipping empty batch at step {step}\")\n",
    "#             continue # Skip potentially None batches if Dataset has errors\n",
    "\n",
    "#         try:\n",
    "#             # Print batch shape information for debugging\n",
    "#             print(f\"Batch {step}: input shape = {batch['input_values'].shape}, label shape = {batch['labels'].shape}\")\n",
    "            \n",
    "#             # Uses 'input_features' as the model's expected key based on previous debugging\n",
    "#             expected_model_input_key = \"input_features\"\n",
    "#             input_data_key = 'input_values' if 'input_values' in batch else 'input_features' # Key from feature extractor output\n",
    "\n",
    "#             model_inputs = {}\n",
    "#             if input_data_key in batch:\n",
    "#                  model_inputs[expected_model_input_key] = batch[input_data_key].to(device)\n",
    "#             else:\n",
    "#                  raise KeyError(f\"Neither 'input_values' nor 'input_features' found in batch.\")\n",
    "\n",
    "#             if 'attention_mask' in batch and batch['attention_mask'] is not None:\n",
    "#                  model_inputs['attention_mask'] = batch['attention_mask'].to(device)\n",
    "\n",
    "#             labels = batch['labels'].to(device) # Keep labels separate\n",
    "\n",
    "#             # --- Automatic Mixed Precision ---\n",
    "#             with torch.autocast(device_type=device.type): # Runs forward pass and loss in mixed precision\n",
    "#                 outputs = model(**model_inputs)\n",
    "#                 logits = outputs.logits\n",
    "#                 loss = criterion(logits, labels) # BCEWithLogitsLoss expects float labels\n",
    "\n",
    "#                 # Check for NaN loss immediately after calculation\n",
    "#                 if torch.isnan(loss):\n",
    "#                     logging.warning(f\"NaN loss detected at step {step} *inside autocast*. Skipping batch.\")\n",
    "#                     print(f\"NaN loss detected at step {step}. Skipping batch.\")\n",
    "#                     # Need to zero grad if skipping before optimizer step in accumulation cycle\n",
    "#                     if (step + 1) % gradient_accumulation_steps != 0:\n",
    "#                         optimizer.zero_grad() # Zero grad to prevent NaN propagation\n",
    "#                     continue # Skip backward and optimizer step\n",
    "\n",
    "#                 # Scale loss for gradient accumulation BEFORE scaler.scale()\n",
    "#                 scaled_loss = loss / gradient_accumulation_steps\n",
    "#             # --- End Autocast ---\n",
    "\n",
    "#             # --- Scaler Scales the loss and Calls backward() ---\n",
    "#             scaler.scale(scaled_loss).backward()\n",
    "#             # ---------------------------------------------\n",
    "\n",
    "#             # Accumulate total loss (use the original non-scaled loss for tracking)\n",
    "#             batch_size_actual = labels.size(0)\n",
    "#             total_loss += loss.item() * batch_size_actual\n",
    "#             num_samples += batch_size_actual\n",
    "\n",
    "#             # --- Optimizer Step (with Scaler) ---\n",
    "#             if (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == len(dataloader):\n",
    "#                 # Optional: Unscale gradients before clipping (if clipping)\n",
    "#                 # scaler.unscale_(optimizer)\n",
    "#                 # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "#                 scaler.step(optimizer) # Unscales gradients, steps optimizer, checks for inf/NaN\n",
    "#                 scaler.update() # Update scaler for next iteration\n",
    "#                 optimizer.zero_grad() # Zero gradients *after* stepping or skipping step\n",
    "#             # -----------------------------------\n",
    "\n",
    "#             # Update progress bar description with non-scaled loss\n",
    "#             progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "#             progress_bar.update(1)  # Explicitly update the progress bar\n",
    "            \n",
    "#             # Count successful batches\n",
    "#             successful_batches += 1\n",
    "\n",
    "#         except Exception as e:\n",
    "#              logging.error(f\"Error during training step {step}, batch keys: {batch.keys()}. Error: {e}\", exc_info=True)\n",
    "#              print(f\"Error during training step {step}: {e}\")\n",
    "#              # Ensure gradients are zeroed if an error occurs mid-accumulation cycle\n",
    "#              optimizer.zero_grad()\n",
    "#              continue # Skip this batch on error\n",
    "\n",
    "#     print(f\"Completed training with {successful_batches}/{len(dataloader)} successful batches\")\n",
    "    \n",
    "#     # Final calculation should use accumulated totals\n",
    "#     avg_loss = total_loss / num_samples if num_samples > 0 else 0\n",
    "#     print(f\"\\nAverage Training Loss for Epoch: {avg_loss:.4f}\")\n",
    "#     return avg_loss\n",
    "\n",
    "\n",
    "# print(\"train_epoch function defined with AMP.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 7: Define Training Function for One Epoch (with AMP)\n",
    "\n",
    "# from torch.cuda.amp import autocast, GradScaler # Import AMP utilities\n",
    "\n",
    "# def train_epoch(model, dataloader, criterion, optimizer, device, gradient_accumulation_steps, scaler): # Add scaler argument\n",
    "#     model.train() # Set model to training mode\n",
    "#     total_loss = 0\n",
    "#     num_samples = 0\n",
    "#     optimizer.zero_grad() # Zero gradients once before the epoch loop\n",
    "    \n",
    "#     # Add counters for debugging\n",
    "#     successful_batches = 0\n",
    "#     print(f\"Starting training with {len(dataloader)} batches\")\n",
    "\n",
    "#     progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "#     for step, batch in enumerate(progress_bar):\n",
    "#         if batch is None or not batch: \n",
    "#             print(f\"Skipping empty batch at step {step}\")\n",
    "#             continue # Skip potentially None batches if Dataset has errors\n",
    "\n",
    "#         try:\n",
    "#             # Print batch shape information for debugging\n",
    "#             print(f\"Batch {step}: input shape = {batch['input_values'].shape}, label shape = {batch['labels'].shape}\")\n",
    "            \n",
    "#             # Uses 'input_features' as the model's expected key based on previous debugging\n",
    "#             expected_model_input_key = \"input_features\"\n",
    "#             input_data_key = 'input_values' if 'input_values' in batch else 'input_features' # Key from feature extractor output\n",
    "\n",
    "#             model_inputs = {}\n",
    "#             if input_data_key in batch:\n",
    "#                  model_inputs[expected_model_input_key] = batch[input_data_key].to(device)\n",
    "#             else:\n",
    "#                  raise KeyError(f\"Neither 'input_values' nor 'input_features' found in batch.\")\n",
    "\n",
    "#             if 'attention_mask' in batch and batch['attention_mask'] is not None:\n",
    "#                  model_inputs['attention_mask'] = batch['attention_mask'].to(device)\n",
    "\n",
    "#             labels = batch['labels'].to(device) # Keep labels separate\n",
    "\n",
    "#             # --- Automatic Mixed Precision ---\n",
    "#             with torch.autocast(device_type=device.type): # Runs forward pass and loss in mixed precision\n",
    "#                 outputs = model(**model_inputs)\n",
    "#                 logits = outputs.logits\n",
    "#                 loss = criterion(logits, labels) # BCEWithLogitsLoss expects float labels\n",
    "\n",
    "#                 # Check for NaN loss immediately after calculation\n",
    "#                 if torch.isnan(loss):\n",
    "#                     logging.warning(f\"NaN loss detected at step {step} *inside autocast*. Skipping batch.\")\n",
    "#                     print(f\"NaN loss detected at step {step}. Skipping batch.\")\n",
    "#                     # Need to zero grad if skipping before optimizer step in accumulation cycle\n",
    "#                     if (step + 1) % gradient_accumulation_steps != 0:\n",
    "#                         optimizer.zero_grad() # Zero grad to prevent NaN propagation\n",
    "#                     continue # Skip backward and optimizer step\n",
    "\n",
    "#                 # Scale loss for gradient accumulation BEFORE scaler.scale()\n",
    "#                 scaled_loss = loss / gradient_accumulation_steps\n",
    "#             # --- End Autocast ---\n",
    "\n",
    "#             # --- Scaler Scales the loss and Calls backward() ---\n",
    "#             scaler.scale(scaled_loss).backward()\n",
    "#             # ---------------------------------------------\n",
    "\n",
    "#             # Accumulate total loss (use the original non-scaled loss for tracking)\n",
    "#             batch_size_actual = labels.size(0)\n",
    "#             total_loss += loss.item() * batch_size_actual\n",
    "#             num_samples += batch_size_actual\n",
    "\n",
    "#             # --- Optimizer Step (with Scaler) ---\n",
    "#             if (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == len(dataloader):\n",
    "#                 # Optional: Unscale gradients before clipping (if clipping)\n",
    "#                 # scaler.unscale_(optimizer)\n",
    "#                 # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "#                 scaler.step(optimizer) # Unscales gradients, steps optimizer, checks for inf/NaN\n",
    "#                 scaler.update() # Update scaler for next iteration\n",
    "#                 optimizer.zero_grad() # Zero gradients *after* stepping or skipping step\n",
    "#             # -----------------------------------\n",
    "\n",
    "#             # Update progress bar description with non-scaled loss\n",
    "#             progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "#             progress_bar.update(1)  # Explicitly update the progress bar\n",
    "            \n",
    "#             # Count successful batches\n",
    "#             successful_batches += 1\n",
    "\n",
    "#         except Exception as e:\n",
    "#              logging.error(f\"Error during training step {step}, batch keys: {batch.keys()}. Error: {e}\", exc_info=True)\n",
    "#              print(f\"Error during training step {step}: {e}\")\n",
    "#              # Ensure gradients are zeroed if an error occurs mid-accumulation cycle\n",
    "#              optimizer.zero_grad()\n",
    "#              continue # Skip this batch on error\n",
    "\n",
    "#     print(f\"Completed training with {successful_batches}/{len(dataloader)} successful batches\")\n",
    "    \n",
    "#     # Final calculation should use accumulated totals\n",
    "#     avg_loss = total_loss / num_samples if num_samples > 0 else 0\n",
    "#     print(f\"\\nAverage Training Loss for Epoch: {avg_loss:.4f}\")\n",
    "#     return avg_loss\n",
    "\n",
    "\n",
    "# print(\"train_epoch function defined with AMP.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_epoch function updated to accept scheduler.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Define Training Function for One Epoch (with AMP and Scheduler)\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler # Or from torch.amp import ...\n",
    "\n",
    "# Ensure compute_metrics, torch, logging, tqdm etc. are imported\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, gradient_accumulation_steps, scaler, scheduler=None): # <<< Added scheduler=None\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_samples = 0\n",
    "    successful_steps = 0 # Counter for successful steps\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    num_batches = len(dataloader) # Get total batches for scheduler check\n",
    "\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        if batch is None or not batch: continue\n",
    "\n",
    "        try:\n",
    "            expected_model_input_key = \"input_features\" # VERIFY THIS KEY NAME\n",
    "            input_data_key = 'input_values' if 'input_values' in batch else 'input_features'\n",
    "            model_inputs = {expected_model_input_key: batch[input_data_key].to(device)}\n",
    "            if 'attention_mask' in batch and batch['attention_mask'] is not None: model_inputs['attention_mask'] = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            with autocast(device_type=device.type, enabled=(device.type=='cuda')): # Correct autocast usage\n",
    "                outputs = model(**model_inputs)\n",
    "                logits = outputs.logits\n",
    "                loss = criterion(logits, labels)\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                logging.warning(f\"NaN loss detected at step {step}. Skipping batch.\")\n",
    "                if (step + 1) % gradient_accumulation_steps != 0: optimizer.zero_grad()\n",
    "                continue\n",
    "\n",
    "            scaled_loss = loss / gradient_accumulation_steps\n",
    "            scaler.scale(scaled_loss).backward()\n",
    "\n",
    "            batch_size_actual = labels.size(0)\n",
    "            total_loss += loss.item() * batch_size_actual\n",
    "            num_samples += batch_size_actual\n",
    "\n",
    "            if (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == num_batches:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                # --- Step the scheduler AFTER the optimizer step ---\n",
    "                if scheduler:\n",
    "                    scheduler.step() # <<<--- ADDED SCHEDULER STEP HERE\n",
    "                # -------------------------------------------------\n",
    "                optimizer.zero_grad()\n",
    "                successful_steps +=1 # Count successful optimizer steps\n",
    "\n",
    "\n",
    "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}', 'lr': f'{optimizer.param_groups[0][\"lr\"]:.2e}'}) # Optionally show LR\n",
    "\n",
    "        except Exception as e:\n",
    "             logging.error(f\"Error during training step {step}: {e}\", exc_info=True)\n",
    "             optimizer.zero_grad() # Zero grad on error too\n",
    "             continue\n",
    "\n",
    "    # Final optimizer step might not be needed if scheduler steps correctly, depends on exact logic.\n",
    "    # Let's remove the extra step outside the loop for now.\n",
    "\n",
    "    avg_loss = total_loss / num_samples if num_samples > 0 else 0\n",
    "    print(f\"\\nCompleted training epoch. Successful optimizer steps: {successful_steps}\")\n",
    "    print(f\"Average Training Loss for Epoch: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "print(\"train_epoch function updated to accept scheduler.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 8: Define Evaluation Function (Corrected Model Input)\n",
    "\n",
    "# def evaluate(model, dataloader, criterion, device):\n",
    "#     model.eval()\n",
    "#     total_loss = 0\n",
    "#     all_logits = []\n",
    "#     all_labels = []\n",
    "#     num_samples = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for step, batch in enumerate(tqdm(dataloader, desc=\"Evaluating\", leave=False)):\n",
    "#             try:\n",
    "#                 # --- CORRECTED INPUT PREPARATION ---\n",
    "#                 expected_model_input_key = \"input_features\" # <<<--- VERIFY THIS KEY NAME\n",
    "\n",
    "#                 if 'input_values' not in batch:\n",
    "#                      raise KeyError(\"Batch dictionary missing 'input_values' from Dataset/Extractor.\")\n",
    "\n",
    "#                 model_inputs = {\n",
    "#                     expected_model_input_key: batch['input_values'].to(device)\n",
    "#                 }\n",
    "#                 if 'attention_mask' in batch and batch['attention_mask'] is not None:\n",
    "#                      model_inputs['attention_mask'] = batch['attention_mask'].to(device)\n",
    "#                 # --- END CORRECTION ---\n",
    "\n",
    "#                 labels = batch['labels'].to(device)\n",
    "\n",
    "#                 # Forward pass\n",
    "#                 outputs = model(**model_inputs) # Pass the correctly named arguments\n",
    "#                 logits = outputs.logits\n",
    "\n",
    "#                 # Calculate loss\n",
    "#                 loss = criterion(logits, labels)\n",
    "#                 total_loss += loss.item() * labels.size(0)\n",
    "#                 num_samples += labels.size(0)\n",
    "\n",
    "#                 all_logits.append(logits.cpu())\n",
    "#                 all_labels.append(labels.cpu())\n",
    "#             except Exception as e:\n",
    "#                  logging.error(f\"Error during evaluation step {step}, batch keys: {batch.keys()}. Error: {e}\", exc_info=True)\n",
    "#                  continue # Skip batch\n",
    "\n",
    "#     if not all_logits or not all_labels or num_samples == 0:\n",
    "#         logging.warning(\"Evaluation yielded no results (all batches failed or empty dataloader?).\")\n",
    "#         return {}\n",
    "\n",
    "#     avg_loss = total_loss / num_samples\n",
    "\n",
    "#     all_logits_cat = torch.cat(all_logits, dim=0)\n",
    "#     all_labels_cat = torch.cat(all_labels, dim=0)\n",
    "\n",
    "#     eval_preds = (all_logits_cat, all_labels_cat)\n",
    "#     metrics = compute_metrics(eval_preds)\n",
    "#     metrics['eval_loss'] = avg_loss\n",
    "\n",
    "#     print(f\"\\nValidation Loss: {avg_loss:.4f}\")\n",
    "#     for name, value in metrics.items():\n",
    "#          if name != 'eval_loss': print(f\"  Validation {name.replace('_', ' ').title()}: {value:.4f}\")\n",
    "\n",
    "#     return metrics\n",
    "\n",
    "# print(\"evaluate function updated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate function defined with AMP (autocast only).\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Define Evaluation Function (with AMP)\n",
    "\n",
    "# Ensure compute_metrics function is defined in a previous cell\n",
    "# Ensure torch, logging, tqdm, np are imported\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    total_loss = 0\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient calculations\n",
    "        for step, batch in enumerate(tqdm(dataloader, desc=\"Evaluating\", leave=False)):\n",
    "            if batch is None or not batch: continue\n",
    "            try:\n",
    "                # Prepare inputs\n",
    "                expected_model_input_key = \"input_features\" # VERIFY THIS KEY NAME\n",
    "                input_data_key = 'input_values' if 'input_values' in batch else 'input_features'\n",
    "\n",
    "                model_inputs = {}\n",
    "                if input_data_key in batch:\n",
    "                    model_inputs[expected_model_input_key] = batch[input_data_key].to(device)\n",
    "                else:\n",
    "                    raise KeyError(f\"Required input key not found in batch during evaluation.\")\n",
    "\n",
    "                if 'attention_mask' in batch and batch['attention_mask'] is not None:\n",
    "                     model_inputs['attention_mask'] = batch['attention_mask'].to(device)\n",
    "\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                # --- Use autocast for forward pass during evaluation ---\n",
    "                # Although not strictly needed for memory unless inputs are huge,\n",
    "                # it ensures consistency with training pass calculations.\n",
    "                with autocast(device_type=device.type):\n",
    "                    outputs = model(**model_inputs)\n",
    "                    logits = outputs.logits\n",
    "                    loss = criterion(logits, labels)\n",
    "                # ----------------------------------------------------\n",
    "\n",
    "                total_loss += loss.item() * labels.size(0)\n",
    "                num_samples += labels.size(0)\n",
    "\n",
    "                all_logits.append(logits.cpu()) # Store logits on CPU\n",
    "                all_labels.append(labels.cpu()) # Store labels on CPU\n",
    "            except Exception as e:\n",
    "                 logging.error(f\"Error during evaluation step {step}: {e}\", exc_info=True)\n",
    "                 continue # Skip batch on error\n",
    "\n",
    "    if not all_logits or not all_labels or num_samples == 0:\n",
    "        logging.warning(\"Evaluation yielded no results.\")\n",
    "        return {}\n",
    "\n",
    "    # Calculate average loss over processed samples\n",
    "    avg_loss = total_loss / num_samples\n",
    "\n",
    "    # Concatenate results from all batches\n",
    "    all_logits_cat = torch.cat(all_logits, dim=0)\n",
    "    all_labels_cat = torch.cat(all_labels, dim=0)\n",
    "\n",
    "    # Calculate metrics using the helper function\n",
    "    eval_preds = (all_logits_cat, all_labels_cat) # Pass tensors directly\n",
    "    metrics = compute_metrics(eval_preds)\n",
    "    metrics['eval_loss'] = avg_loss\n",
    "\n",
    "    # Log metrics\n",
    "    print(f\"\\nValidation Loss: {avg_loss:.4f}\")\n",
    "    for name, value in metrics.items():\n",
    "         if name != 'eval_loss': print(f\"  Validation {name.replace('_', ' ').title()}: {value:.4f}\")\n",
    "\n",
    "    return metrics # Return dictionary of all metrics\n",
    "\n",
    "print(\"evaluate function defined with AMP (autocast only).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 9: Run ONE Epoch for Debugging\n",
    "\n",
    "# from tqdm import tqdm # Ensure tqdm is imported\n",
    "# from torch.cuda.amp import GradScaler\n",
    "\n",
    "\n",
    "\n",
    "# # Ensure model, criterion, optimizer, dataloaders etc. are defined from previous cells\n",
    "# print(f\"\\n--- Starting Debug Training Run for {num_epochs_debug} epoch ---\")\n",
    "# start_time = time.time()\n",
    "\n",
    "\n",
    "# # --- Initialize GradScaler ---\n",
    "# scaler = GradScaler() # <<<--- ADD THIS INITIALIZATION\n",
    "# # ---------------------------\n",
    "\n",
    "\n",
    "\n",
    "# # Make sure model and criterion are on the correct device\n",
    "# model.to(device)\n",
    "# criterion.to(device)\n",
    "\n",
    "# for epoch in range(num_epochs_debug): # num_epochs_debug was set to 1 in Cell 2\n",
    "#     print(f\"\\n--- Debug Epoch {epoch+1}/{num_epochs_debug} ---\")\n",
    "\n",
    "#     # Run training step for one epoch on the debug training data\n",
    "#     train_loss = train_epoch(\n",
    "#         model,\n",
    "#         debug_train_dataloader, # Use the SMALL debug dataloader\n",
    "#         criterion,\n",
    "#         optimizer,\n",
    "#         device,\n",
    "#         gradient_accumulation_steps # Pass grad accum steps\n",
    "#     )\n",
    "\n",
    "#     # Run evaluation step on the debug validation data\n",
    "#     eval_metrics = evaluate(\n",
    "#         model,\n",
    "#         debug_val_dataloader, # Use the SMALL debug dataloader\n",
    "#         criterion,\n",
    "#         device\n",
    "#     )\n",
    "\n",
    "#     print(f\"\\nDebug Epoch {epoch+1} finished.\")\n",
    "#     print(f\"  Avg Train Loss: {train_loss:.4f}\")\n",
    "#     if eval_metrics:\n",
    "#         # Print all collected metrics\n",
    "#         for name, value in eval_metrics.items():\n",
    "#             print(f\"  Validation {name.replace('_', ' ').title()}: {value:.4f}\")\n",
    "#     else:\n",
    "#         print(\"  Validation failed to produce metrics.\")\n",
    "\n",
    "#     # Optional: Save model after this 1 epoch for inspection\n",
    "#     save_path = os.path.join(model_save_dir, f\"wav2vec2bert_debug_epoch_{epoch+1}.pth\") # <<<--- Corrected filename\n",
    "#     try:\n",
    "#          torch.save(model.state_dict(), save_path)\n",
    "#          logging.info(f\"Saved debug model checkpoint to {save_path}\")\n",
    "#     except Exception as e:\n",
    "#          logging.error(f\"Failed to save debug model checkpoint: {e}\", exc_info=True)\n",
    "\n",
    "# end_time = time.time()\n",
    "# print(f\"\\n--- Debug Run Finished in {end_time - start_time:.2f} seconds ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Debug Training Run for 1 epoch (with AMP) ---\n",
      "\n",
      "--- Debug Epoch 1/1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed training epoch. Successful optimizer steps: 2\n",
      "Average Training Loss for Epoch: 0.6867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Loss: 0.6700\n",
      "  Validation Hamming Loss: 0.1705\n",
      "  Validation Jaccard Samples: 0.0833\n",
      "  Validation F1 Micro: 0.1176\n",
      "  Validation F1 Macro: 0.0182\n",
      "\n",
      "Debug Epoch 1 finished.\n",
      "  Avg Train Loss: 0.6867\n",
      "  Validation Hamming Loss: 0.1705\n",
      "  Validation Jaccard Samples: 0.0833\n",
      "  Validation F1 Micro: 0.1176\n",
      "  Validation F1 Macro: 0.0182\n",
      "  Validation Eval Loss: 0.6700\n",
      "2025-05-03 17:34:41,859 - INFO - Saved debug model checkpoint to /workspace/musicClaGen/models/wav2vec2bert_debug_AMP_epoch_1.pth\n",
      "\n",
      "--- Debug Run Finished in 26.96 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Run ONE Epoch for Debugging (with Updated AMP API)\n",
    "\n",
    "# --- Ensure necessary imports are present ---\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Import AMP components from the new location ---\n",
    "from torch.amp import autocast, GradScaler # <<<--- UPDATED IMPORT\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# Ensure model, criterion, optimizer, dataloaders etc. are defined from previous cells\n",
    "print(f\"\\n--- Starting Debug Training Run for {num_epochs_debug} epoch (with AMP) ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "# --- Initialize GradScaler using the NEW API ---\n",
    "# Pass device type, and enable only if device is actually cuda\n",
    "scaler = GradScaler(enabled=(device.type == 'cuda')) # <<<--- UPDATED INITIALIZATION\n",
    "# -------------------------------------------\n",
    "\n",
    "# Make sure model and criterion are on the correct device\n",
    "model.to(device)\n",
    "criterion.to(device) # Ensure criterion is also on device\n",
    "\n",
    "for epoch in range(num_epochs_debug): # num_epochs_debug was set to 1 in Cell 2\n",
    "    print(f\"\\n--- Debug Epoch {epoch+1}/{num_epochs_debug} ---\")\n",
    "\n",
    "    # Run training step (train_epoch function itself doesn't need change here, only how scaler is passed)\n",
    "    train_loss = train_epoch(\n",
    "        model,\n",
    "        debug_train_dataloader,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        device,\n",
    "        gradient_accumulation_steps,\n",
    "        scaler # Pass the scaler object (created with new API)\n",
    "    )\n",
    "\n",
    "    # Run evaluation step (evaluate function itself doesn't need change here for scaler)\n",
    "    eval_metrics = evaluate(\n",
    "        model,\n",
    "        debug_val_dataloader,\n",
    "        criterion,\n",
    "        device\n",
    "    )\n",
    "\n",
    "    print(f\"\\nDebug Epoch {epoch+1} finished.\")\n",
    "    print(f\"  Avg Train Loss: {train_loss:.4f}\")\n",
    "    if eval_metrics:\n",
    "        for name, value in eval_metrics.items():\n",
    "            print(f\"  Validation {name.replace('_', ' ').title()}: {value:.4f}\")\n",
    "    else:\n",
    "        print(\"  Validation failed to produce metrics.\")\n",
    "\n",
    "    # Optional: Save model after this 1 epoch for inspection\n",
    "    save_path = os.path.join(model_save_dir, f\"wav2vec2bert_debug_AMP_epoch_{epoch+1}.pth\")\n",
    "    try:\n",
    "         torch.save(model.state_dict(), save_path)\n",
    "         logging.info(f\"Saved debug model checkpoint to {save_path}\")\n",
    "    except Exception as e:\n",
    "         logging.error(f\"Failed to save debug model checkpoint: {e}\", exc_info=True)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\n--- Debug Run Finished in {end_time - start_time:.2f} seconds ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Trial debug training run worked! Now let's try the full training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-03 17:34:41,918 - INFO - Loading feature extractor for: facebook/w2v-bert-2.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-03 17:34:42,015 - INFO - Feature extractor loaded successfully.\n",
      "Feature extractor expects sample rate: 16000\n",
      "2025-05-03 17:34:42,017 - INFO - Initializing FMARawAudioDataset from: /workspace/musicClaGen/data/processed/small_subset_multihot.csv\n",
      "2025-05-03 17:34:42,018 - INFO - Target sampling rate set from feature extractor: 16000 Hz\n",
      "2025-05-03 17:34:42,020 - INFO - Loading manifest from: /workspace/musicClaGen/data/processed/small_subset_multihot.csv\n",
      "2025-05-03 17:34:42,054 - INFO - Attempting to parse 'multi_hot_label' column using ast.literal_eval...\n",
      "2025-05-03 17:34:42,375 - INFO - Example parsed label verified (type <class 'list'>, length 22): [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "2025-05-03 17:34:42,376 - INFO - Loaded and parsed manifest with 8000 entries.\n",
      "2025-05-03 17:34:42,377 - INFO - Creating DataLoaders with FULL splits and custom collator...\n",
      "DataCollatorAudio instance created.\n",
      "2025-05-03 17:34:42,384 - INFO - Batch size: 2, Grad Accum Steps: 4, Effective BS: 8\n",
      "2025-05-03 17:34:42,386 - INFO - FULL Dataset sizes: Train=6400, Val=800, Test=800\n",
      "2025-05-03 17:34:42,387 - INFO - FULL DataLoaders with custom collator created.\n",
      "2025-05-03 17:34:42,389 - INFO - LR Scheduler created. Total optimization steps: 800\n",
      "\n",
      "Setup for full training run complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Setup DataLoaders for FULL Splits & LR Scheduler\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup # Import scheduler\n",
    "\n",
    "# --- Ensure Feature Extractor is Loaded ---\n",
    "# (Code from previous Cell 4 - necessary if kernel restarted)\n",
    "logging.info(f\"Loading feature extractor for: {model_checkpoint}\")\n",
    "try:\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint)\n",
    "    logging.info(\"Feature extractor loaded successfully.\")\n",
    "    target_sr = feature_extractor.sampling_rate\n",
    "    print(f\"Feature extractor expects sample rate: {target_sr}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Could not load feature extractor. Error: {e}\", exc_info=True)\n",
    "    raise SystemExit\n",
    "\n",
    "# --- Create Full Dataset instance ---\n",
    "try:\n",
    "    full_dataset = FMARawAudioDataset(manifest_path, feature_extractor=feature_extractor)\n",
    "    manifest_df = full_dataset.manifest\n",
    "except Exception as e:\n",
    "     logging.error(\"Failed to instantiate FMARawAudioDataset.\", exc_info=True)\n",
    "     raise SystemExit\n",
    "\n",
    "# --- Create FULL Datasets for Train/Val/Test ---\n",
    "logging.info(\"Creating DataLoaders with FULL splits and custom collator...\")\n",
    "try:\n",
    "    # Get indices for the splits from the manifest\n",
    "    train_indices = manifest_df[manifest_df['split'] == 'training'].index.tolist()\n",
    "    val_indices = manifest_df[manifest_df['split'] == 'validation'].index.tolist()\n",
    "    test_indices = manifest_df[manifest_df['split'] == 'test'].index.tolist() # Get test indices too\n",
    "\n",
    "    # Create Subset instances using the FULL index lists\n",
    "    train_dataset = Subset(full_dataset, train_indices)\n",
    "    val_dataset = Subset(full_dataset, val_indices)\n",
    "    test_dataset = Subset(full_dataset, test_indices) # Create test dataset\n",
    "\n",
    "    # --- Create Data Collator Instance ---\n",
    "    data_collator = DataCollatorAudio()\n",
    "    print(\"DataCollatorAudio instance created.\")\n",
    "\n",
    "    # --- Create DataLoaders ---\n",
    "    # Use actual batch_size from config\n",
    "    effective_batch_size = config.MODEL_PARAMS[\"batch_size\"] * config.MODEL_PARAMS[\"gradient_accumulation_steps\"]\n",
    "    logging.info(f\"Batch size: {config.MODEL_PARAMS['batch_size']}, Grad Accum Steps: {config.MODEL_PARAMS['gradient_accumulation_steps']}, Effective BS: {effective_batch_size}\")\n",
    "\n",
    "    # Use num_workers for faster loading (adjust based on instance cores)\n",
    "    num_workers = 4 if os.name == 'posix' else 0\n",
    "    pin_memory = True if device.type == 'cuda' else False\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, batch_size=config.MODEL_PARAMS[\"batch_size\"], shuffle=True,\n",
    "        collate_fn=data_collator, num_workers=num_workers, pin_memory=pin_memory, persistent_workers=(num_workers>0)\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset, batch_size=config.MODEL_PARAMS[\"batch_size\"], shuffle=False,\n",
    "        collate_fn=data_collator, num_workers=num_workers, pin_memory=pin_memory, persistent_workers=(num_workers>0)\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset, batch_size=config.MODEL_PARAMS[\"batch_size\"], shuffle=False,\n",
    "        collate_fn=data_collator, num_workers=num_workers, pin_memory=pin_memory, persistent_workers=(num_workers>0)\n",
    "    )\n",
    "    logging.info(f\"FULL Dataset sizes: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n",
    "    logging.info(\"FULL DataLoaders with custom collator created.\")\n",
    "\n",
    "    # --- Setup LR Scheduler ---\n",
    "    num_epochs = config.MODEL_PARAMS[\"epochs\"]\n",
    "    num_training_steps = (len(train_dataloader) // config.MODEL_PARAMS[\"gradient_accumulation_steps\"]) * num_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "         optimizer, # Optimizer defined in Cell 6\n",
    "         num_warmup_steps=0, # You can add warmup steps if desired (e.g., 10% of total steps)\n",
    "         num_training_steps=num_training_steps\n",
    "    )\n",
    "    logging.info(f\"LR Scheduler created. Total optimization steps: {num_training_steps}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to create datasets/dataloaders: {e}\", exc_info=True)\n",
    "    raise SystemExit\n",
    "\n",
    "print(\"\\nSetup for full training run complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-03 17:34:42,411 - INFO - --- Starting FULL Training for 1 epochs ---\n",
      "2025-05-03 17:34:42,424 - INFO - \n",
      "--- Epoch 1/1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██        | 662/3200 [09:13<36:35,  1.16it/s, loss=0.2289, lr=3.97e-05]/tmp/ipykernel_163240/4040614510.py:148: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  waveform, loaded_sr = librosa.load(\n",
      "/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-03 17:43:55,596 - ERROR - Error loading/processing track 133297: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 209, in __soundfile_load\n",
      "    context = sf.SoundFile(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 690, in __init__\n",
      "    self._file = self._open(file, mode_int, closefd)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1265, in _open\n",
      "    raise LibsndfileError(err, prefix=\"Error opening {0!r}: \".format(self.name))\n",
      "soundfile.LibsndfileError: Error opening '/workspace/musicClaGen/data/raw/fma_audio/fma_small/133/133297.mp3': File does not exist or is not a regular file (possibly a pipe?).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_163240/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/parse.c:do_readahead():1083] warning: Cannot read next header, a one-frame stream? Duh...\n",
      "Training:  27%|██▋       | 855/3200 [11:53<32:41,  1.20it/s, loss=0.2395, lr=3.67e-05]/tmp/ipykernel_163240/4040614510.py:148: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  waveform, loaded_sr = librosa.load(\n",
      "/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-03 17:46:35,939 - ERROR - Error loading/processing track 99134: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 209, in __soundfile_load\n",
      "    context = sf.SoundFile(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 690, in __init__\n",
      "    self._file = self._open(file, mode_int, closefd)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1265, in _open\n",
      "    raise LibsndfileError(err, prefix=\"Error opening {0!r}: \".format(self.name))\n",
      "soundfile.LibsndfileError: Error opening '/workspace/musicClaGen/data/raw/fma_audio/fma_small/099/099134.mp3': File does not exist or is not a regular file (possibly a pipe?).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_163240/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/parse.c:do_readahead():1083] warning: Cannot read next header, a one-frame stream? Duh...\n",
      "Training:  46%|████▋     | 1487/3200 [20:34<22:12,  1.29it/s, loss=0.2392, lr=2.68e-05][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (3360) too large for available bit count (3240)\n",
      "Training:  49%|████▉     | 1560/3200 [21:34<22:22,  1.22it/s, loss=0.2461, lr=2.56e-05]/tmp/ipykernel_163240/4040614510.py:148: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  waveform, loaded_sr = librosa.load(\n",
      "/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-03 17:56:17,289 - ERROR - Error loading/processing track 98569: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 222, in __soundfile_load\n",
      "    y = sf_desc.read(frames=frame_duration, dtype=dtype, always_2d=False).T\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 942, in read\n",
      "    frames = self._array_io('read', out, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1394, in _array_io\n",
      "    return self._cdata_io(action, cdata, ctype, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1404, in _cdata_io\n",
      "    _error_check(self._errorcode)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1480, in _error_check\n",
      "    raise LibsndfileError(err, prefix=prefix)\n",
      "soundfile.LibsndfileError: Unspecified internal error.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_163240/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
      "Note: Illegal Audio-MPEG-Header 0x00000000 at offset 63168.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      "Training:  63%|██████▎   | 2005/3200 [27:40<14:46,  1.35it/s, loss=0.1509, lr=1.87e-05]Note: Illegal Audio-MPEG-Header 0x00000000 at offset 22401.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      "/tmp/ipykernel_163240/4040614510.py:148: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  waveform, loaded_sr = librosa.load(\n",
      "/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-03 18:02:23,401 - ERROR - Error loading/processing track 98567: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 222, in __soundfile_load\n",
      "    y = sf_desc.read(frames=frame_duration, dtype=dtype, always_2d=False).T\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 942, in read\n",
      "    frames = self._array_io('read', out, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1394, in _array_io\n",
      "    return self._cdata_io(action, cdata, ctype, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1404, in _cdata_io\n",
      "    _error_check(self._errorcode)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1480, in _error_check\n",
      "    raise LibsndfileError(err, prefix=prefix)\n",
      "soundfile.LibsndfileError: Unspecified internal error.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_163240/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████▍   | 2055/3200 [28:20<14:27,  1.32it/s, loss=0.1530, lr=1.79e-05][src/libmpg123/layer3.c:INT123_do_layer3():1844] error: dequantization failed!\n",
      "Training:  73%|███████▎  | 2331/3200 [32:05<11:37,  1.25it/s, loss=0.1596, lr=1.36e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-03 18:06:48,495 - ERROR - Error loading/processing track 98565: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 222, in __soundfile_load\n",
      "    y = sf_desc.read(frames=frame_duration, dtype=dtype, always_2d=False).T\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 942, in read\n",
      "    frames = self._array_io('read', out, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1394, in _array_io\n",
      "    return self._cdata_io(action, cdata, ctype, frames)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1404, in _cdata_io\n",
      "    _error_check(self._errorcode)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1480, in _error_check\n",
      "    raise LibsndfileError(err, prefix=prefix)\n",
      "soundfile.LibsndfileError: Unspecified internal error.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_163240/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Illegal Audio-MPEG-Header 0x00000000 at offset 33361.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      "Training:  78%|███████▊  | 2482/3200 [34:09<09:45,  1.23it/s, loss=0.2416, lr=1.13e-05][src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (3328) too large for available bit count (3240)\n",
      "Training:  84%|████████▎ | 2678/3200 [36:48<07:03,  1.23it/s, loss=0.3141, lr=8.19e-06][src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
      "Training:  85%|████████▌ | 2732/3200 [37:32<06:32,  1.19it/s, loss=0.3897, lr=7.31e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-03 18:12:15,083 - ERROR - Error loading/processing track 108925: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 176, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 209, in __soundfile_load\n",
      "    context = sf.SoundFile(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 690, in __init__\n",
      "    self._file = self._open(file, mode_int, closefd)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/soundfile.py\", line 1265, in _open\n",
      "    raise LibsndfileError(err, prefix=\"Error opening {0!r}: \".format(self.name))\n",
      "soundfile.LibsndfileError: Error opening '/workspace/musicClaGen/data/raw/fma_audio/fma_small/108/108925.mp3': File does not exist or is not a regular file (possibly a pipe?).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_163240/4040614510.py\", line 148, in __getitem__\n",
      "    waveform, loaded_sr = librosa.load(\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 184, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/decorator.py\", line 235, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/util/decorators.py\", line 63, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/librosa/core/audio.py\", line 240, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"/root/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/audioread/__init__.py\", line 132, in audio_open\n",
      "    raise NoBackendError()\n",
      "audioread.exceptions.NoBackendError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/parse.c:do_readahead():1083] warning: Cannot read next header, a one-frame stream? Duh...\n",
      "Training:  92%|█████████▏| 2950/3200 [40:31<03:19,  1.25it/s, loss=0.2443, lr=3.94e-06][src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n",
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed training epoch. Successful optimizer steps: 800\n",
      "Average Training Loss for Epoch: 0.2418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Loss: 0.2318\n",
      "  Validation Hamming Loss: 0.0691\n",
      "  Validation Jaccard Samples: 0.0000\n",
      "  Validation F1 Micro: 0.0000\n",
      "  Validation F1 Macro: 0.0000\n",
      "\n",
      "Epoch 1 finished.\n",
      "  Avg Train Loss: 0.2418\n",
      "  Validation Hamming Loss: 0.0691\n",
      "  Validation Jaccard Samples: 0.0000\n",
      "  Validation F1 Micro: 0.0000\n",
      "  Validation F1 Macro: 0.0000\n",
      "  Validation Eval Loss: 0.2318\n",
      "2025-05-03 18:22:08,397 - INFO - Validation metric improved (hamming_loss=0.0691). Saved best model to /workspace/musicClaGen/models/facebook_w2v-bert-2.0_finetuned_best.pth\n",
      "2025-05-03 18:22:08,399 - INFO - Epoch 1 finished in 47.43 minutes.\n",
      "2025-05-03 18:22:08,401 - INFO - --- Training Finished in 47.43 minutes ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Run Full Training Loop\n",
    "\n",
    "# Make sure model, criterion, optimizer, scheduler, dataloaders defined from previous cells\n",
    "num_epochs = config.MODEL_PARAMS[\"epochs\"] # Get actual epochs from config\n",
    "gradient_accumulation_steps = config.MODEL_PARAMS[\"gradient_accumulation_steps\"]\n",
    "metric_to_monitor = 'hamming_loss' # Metric to decide best model (lower is better)\n",
    "best_val_metric = float('inf')\n",
    "\n",
    "# --- Initialize GradScaler for AMP ---\n",
    "scaler = GradScaler(enabled=(device.type == 'cuda'))\n",
    "# ------------------------------------\n",
    "\n",
    "logging.info(f\"--- Starting FULL Training for {num_epochs} epochs ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Make sure model and criterion are on the correct device\n",
    "model.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    logging.info(f\"\\n--- Epoch {epoch+1}/{num_epochs} ---\")\n",
    "\n",
    "    # Run training for one epoch\n",
    "    train_loss = train_epoch(\n",
    "        model, train_dataloader, criterion, optimizer, device,\n",
    "        gradient_accumulation_steps, scaler, scheduler # Pass scaler and scheduler\n",
    "    )\n",
    "\n",
    "    # Run evaluation on validation set\n",
    "    eval_metrics = evaluate(model, val_dataloader, criterion, device)\n",
    "\n",
    "    print(f\"\\nEpoch {epoch+1} finished.\")\n",
    "    print(f\"  Avg Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "    if not eval_metrics:\n",
    "        logging.warning(f\"Epoch {epoch+1}: Evaluation failed, skipping checkpoint.\")\n",
    "        continue\n",
    "\n",
    "    # Log all validation metrics\n",
    "    for name, value in eval_metrics.items():\n",
    "        print(f\"  Validation {name.replace('_', ' ').title()}: {value:.4f}\")\n",
    "\n",
    "    # Save model checkpoint if validation metric improved\n",
    "    current_val_metric = eval_metrics.get(metric_to_monitor, float('inf'))\n",
    "    if current_val_metric < best_val_metric:\n",
    "        best_val_metric = current_val_metric\n",
    "        # Use a consistent name for the best model checkpoint\n",
    "        save_path = os.path.join(model_save_dir, f\"{model_checkpoint.replace('/', '_')}_finetuned_best.pth\")\n",
    "        try:\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            logging.info(f\"Validation metric improved ({metric_to_monitor}={current_val_metric:.4f}). Saved best model to {save_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to save model checkpoint: {e}\", exc_info=True)\n",
    "    else:\n",
    "         logging.info(f\"Validation metric did not improve ({metric_to_monitor}={current_val_metric:.4f}). Best: {best_val_metric:.4f}\")\n",
    "\n",
    "    epoch_duration = time.time() - epoch_start_time\n",
    "    logging.info(f\"Epoch {epoch+1} finished in {epoch_duration / 60:.2f} minutes.\")\n",
    "\n",
    "total_training_time = time.time() - start_time\n",
    "logging.info(f\"--- Training Finished in {total_training_time / 60:.2f} minutes ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the converted markdown that will render correctly in Google Colab. I've kept all content exactly the same but adjusted the code-style formatting to use backticks:\n",
    "\n",
    "• Time: Took ~47 minutes for 1 epoch on the full `fma_small` training set (~6400 samples). This is a realistic time given the model size, 30s inputs, data loading, and AMP.\n",
    "\n",
    "• Errors During Training: The log shows several errors during the training loop:\n",
    "\n",
    "  • `ERROR - Error loading/processing track ...`\n",
    "  \n",
    "  `audioread.exceptions.NoBackendError`: This error occurred multiple times (tracks 133297, 99134, 98569, 98567, 98565, 108925). It indicates `librosa.load` failed. It first tries `soundfile` (which fails often with MP3s, sometimes due to file existence/permissions or internal errors), then falls back to `audioread`, which then fails because no suitable backend (like `ffmpeg`) was found or successfully used by `audioread`. This is despite installing `ffmpeg` earlier. It suggests `librosa`'s fallback mechanism isn't working reliably in this environment.\n",
    "\n",
    "  • `[src/libmpg123/...]: warning: Cannot read next header...`, `error: dequantization failed!`, `error: part2_3_length ... too large...`, `error: Giving up resync...`: These are lower-level MP3 decoding errors from the `mpg123` library, likely called by `audioread` or another backend. They indicate corrupted or non-standard MP3 files.\n",
    "\n",
    "Note: In this case, there weren't any mathematical equations to convert, but I've maintained the code-style formatting using backticks for technical terms and error messages. The text will render properly in Google Colab with this formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-03 18:57:08,617 - INFO - \n",
      "--- Evaluating on Test Set using Best Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2BertForSequenceClassification were not initialized from the model checkpoint at facebook/w2v-bert-2.0 and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-03 18:57:41,207 - INFO - Loaded best model checkpoint from /workspace/musicClaGen/models/facebook_w2v-bert-2.0_finetuned_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded best model checkpoint from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Run evaluation on the test set\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m test_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_reloaded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Use test_dataloader\u001b[39;00m\n\u001b[1;32m     30\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Final Test Set Results ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m test_metrics:\n",
      "Cell \u001b[0;32mIn[23], line 14\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, dataloader, criterion, device)\u001b[0m\n\u001b[1;32m     11\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(): \u001b[38;5;66;03m# Disable gradient calculations\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)):\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batch: \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m             \u001b[38;5;66;03m# Prepare inputs\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/torch/utils/data/dataloader.py:490\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_iterator()\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 490\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1263\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._reset\u001b[0;34m(self, loader, first_iter)\u001b[0m\n\u001b[1;32m   1261\u001b[0m resume_iteration_cnt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_workers\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m resume_iteration_cnt \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1263\u001b[0m     return_idx, return_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1264\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_idx, _utils\u001b[38;5;241m.\u001b[39mworker\u001b[38;5;241m.\u001b[39m_ResumeIteration):\n\u001b[1;32m   1265\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m return_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1443\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1443\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1444\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1445\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda/envs/musicClaGen_env22/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1284\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1272\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1273\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1282\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1284\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1285\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1286\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1287\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1288\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1289\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/musicClaGen_env22/lib/python3.10/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m~/miniconda/envs/musicClaGen_env22/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Cell 12: Evaluate Best Model on Test Set\n",
    "\n",
    "logging.info(\"\\n--- Evaluating on Test Set using Best Model ---\")\n",
    "\n",
    "# Construct path to the best saved model\n",
    "best_model_path = os.path.join(model_save_dir, f\"{model_checkpoint.replace('/', '_')}_finetuned_best.pth\")\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    try:\n",
    "        # Load best model state\n",
    "        # Re-initialize model structure first (important!)\n",
    "        model_reloaded = AutoModelForAudioClassification.from_pretrained(\n",
    "            model_checkpoint, # Need original checkpoint to know architecture\n",
    "            num_labels=num_labels,\n",
    "            ignore_mismatched_sizes=True # Load base weights, ignore head\n",
    "        )\n",
    "        # Explicitly replace head again to be sure\n",
    "        classifier_attr = 'classifier' # VERIFY\n",
    "        if hasattr(model_reloaded, classifier_attr) and isinstance(getattr(model_reloaded, classifier_attr), nn.Linear):\n",
    "             in_features = getattr(model_reloaded, classifier_attr).in_features\n",
    "             setattr(model_reloaded, classifier_attr, nn.Linear(in_features, num_labels))\n",
    "        # Load the saved state dict\n",
    "        model_reloaded.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "        model_reloaded.to(device) # Move reloaded model to device\n",
    "        logging.info(f\"Loaded best model checkpoint from {best_model_path}\")\n",
    "\n",
    "        # Run evaluation on the test set\n",
    "        test_metrics = evaluate(model_reloaded, test_dataloader, criterion, device) # Use test_dataloader\n",
    "\n",
    "        logging.info(f\"\\n--- Final Test Set Results ---\")\n",
    "        if test_metrics:\n",
    "             for metric_name, metric_value in test_metrics.items():\n",
    "                  logging.info(f\"Test {metric_name.replace('_', ' ').title()}: {metric_value:.4f}\")\n",
    "        else:\n",
    "             logging.info(\"Test evaluation failed to produce metrics.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load best model or evaluate on test set: {e}\", exc_info=True)\n",
    "else:\n",
    "    logging.warning(f\"Best model checkpoint not found at {best_model_path}. Skipping final test evaluation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicClaGen_env22",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
