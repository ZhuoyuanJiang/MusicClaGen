{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zhuoyuan/CSprojects/musicClaGen\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "PROJECT_ROOT detected as: /home/zhuoyuan/CSprojects/musicClaGen\n",
      "Imports and basic setup complete.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from transformers import AutoModelForAudioClassification, AutoProcessor, AdamW, get_linear_schedule_with_warmup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import ast # For parsing string representations of lists/arrays\n",
    "import logging\n",
    "import time\n",
    "from sklearn.metrics import hamming_loss, jaccard_score, f1_score, roc_auc_score\n",
    "from tqdm.notebook import tqdm # Use notebook version of tqdm\n",
    "\n",
    "# --- Project Setup ---\n",
    "# Detect if running in notebook or script to adjust path\n",
    "\n",
    "cwd = os.getcwd()\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(cwd, '../../')) # NOTE: remember to change if change the directory structure\n",
    "\n",
    "\n",
    "\n",
    "print(f\"PROJECT_ROOT detected as: {PROJECT_ROOT}\")\n",
    "# Add project root to Python's module search path if necessary\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    print(f\"Adding {PROJECT_ROOT} to sys.path\")\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "import config # Import your configuration file\n",
    "\n",
    "# --- Setup Logging ---\n",
    "# Clear previous handlers\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "# Basic logging setup\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    handlers=[logging.StreamHandler(sys.stdout)])\n",
    "\n",
    "print(\"Imports and basic setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-02 12:19:38,392 - INFO - Loaded 22 unified genres from /home/zhuoyuan/CSprojects/musicClaGen/data/processed/unified_genres.txt\n",
      "2025-05-02 12:19:38,395 - INFO - Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# --- Load Config ---\n",
    "manifest_path = config.PATHS['MANIFEST_PATH']\n",
    "genre_list_path = config.PATHS['GENRE_LIST_PATH']\n",
    "model_save_dir = config.PATHS['MODELS_DIR']\n",
    "features_dir = config.PATHS['FMA_FEATURES_DIR'] # Needed if dataset uses it relative\n",
    "\n",
    "model_checkpoint = config.MODEL_PARAMS['model_checkpoint']\n",
    "learning_rate = config.MODEL_PARAMS['learning_rate']\n",
    "batch_size = config.MODEL_PARAMS['batch_size'] # Use the small BS for notebook test\n",
    "num_epochs = 1 # <<<--- RUN ONLY 1 EPOCH FOR DEBUGGING ---<<<\n",
    "weight_decay = config.MODEL_PARAMS['weight_decay']\n",
    "gradient_accumulation_steps = config.MODEL_PARAMS['gradient_accumulation_steps']\n",
    "\n",
    "# --- Load unified genre list to get num_labels ---\n",
    "try:\n",
    "    with open(genre_list_path, 'r') as f:\n",
    "        unified_genres = [line.strip() for line in f if line.strip()]\n",
    "    num_labels = len(unified_genres)\n",
    "    logging.info(f\"Loaded {num_labels} unified genres from {genre_list_path}\")\n",
    "    # print(\"Unified Genres:\", unified_genres) # Uncomment to verify list\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to load or process unified genre list: {e}\", exc_info=True)\n",
    "    raise SystemExit(\"Cannot proceed without genre list.\")\n",
    "\n",
    "# --- Setup Device ---\n",
    "device = torch.device(config.DEVICE if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(f\"Using device: {device}\")\n",
    "if not torch.cuda.is_available() and config.DEVICE==\"cuda\":\n",
    "     logging.warning(\"CUDA selected but not available, falling back to CPU.\")\n",
    "\n",
    "# --- Create Save Directory ---\n",
    "os.makedirs(model_save_dir, exist_ok=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dataset Class ---\n",
    "# (Copied from previous response - Ensure this class is defined here or imported)\n",
    "# Make sure it uses config.PROJECT_ROOT correctly for paths if needed\n",
    "\n",
    "class FMAFeatureDataset(Dataset):\n",
    "    \"\"\"Loads pre-computed Mel-spectrograms and labels from manifest.\"\"\"\n",
    "    def __init__(self, manifest_path, processor=None):\n",
    "        logging.info(f\"Loading manifest from: {manifest_path}\")\n",
    "        try:\n",
    "            self.manifest = pd.read_csv(manifest_path)\n",
    "            # Parse the label vector string back into a list\n",
    "            # Use ast.literal_eval if saved as pure list string '[0.0, 1.0]'\n",
    "            # Use your custom parse_numpy_array_string if saved as '[np.float32(0.0)...]'\n",
    "            # Adjust parser function based on how small_subset_multihot.csv looks\n",
    "            label_parser = ast.literal_eval # Or your custom function\n",
    "            self.manifest['label_vector'] = self.manifest['label_vector'].apply(label_parser)\n",
    "            logging.info(f\"Loaded manifest with {len(self.manifest)} entries.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading or parsing manifest {manifest_path}: {e}\", exc_info=True)\n",
    "            raise\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.manifest)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx): idx = idx.tolist()\n",
    "        row = self.manifest.iloc[idx]\n",
    "        track_id = row['track_id']\n",
    "        # Construct absolute path from project root + relative path in manifest\n",
    "        feature_file_path = os.path.join(config.PROJECT_ROOT, row['feature_path'])\n",
    "\n",
    "        try:\n",
    "            spectrogram = np.load(feature_file_path).astype(np.float32) # Load .npy\n",
    "\n",
    "            # Apply Processor (if applicable) - CHECK DOCUMENTATION!\n",
    "            if self.processor:\n",
    "                processed = self.processor(\n",
    "                    spectrogram, # Or maybe requires list? check docs\n",
    "                    sampling_rate=config.PREPROCESSING_PARAMS[\"sample_rate\"],\n",
    "                    return_tensors=\"pt\"\n",
    "                    # Add other relevant processor args\n",
    "                )\n",
    "                # Adjust key based on actual processor output\n",
    "                feature_tensor = processed.get('input_features', processed.get('input_values')).squeeze(0)\n",
    "                attention_mask = processed.get('attention_mask', None)\n",
    "                if attention_mask is not None: attention_mask = attention_mask.squeeze(0)\n",
    "            else:\n",
    "                # Basic tensor conversion if no processor needed/used\n",
    "                feature_tensor = torch.tensor(spectrogram)\n",
    "                attention_mask = None # No mask generated\n",
    "\n",
    "            label_vector = row['label_vector']\n",
    "            label_tensor = torch.tensor(label_vector, dtype=torch.float32)\n",
    "\n",
    "            # Return dictionary matching model's expected input names\n",
    "            inputs = {\"input_values\": feature_tensor, \"labels\": label_tensor}\n",
    "            if attention_mask is not None: inputs['attention_mask'] = attention_mask\n",
    "\n",
    "            return inputs\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading/processing track {track_id} at {feature_file_path}: {e}\", exc_info=True)\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "facebook/w2v-bert-2.0\n"
     ]
    }
   ],
   "source": [
    "print(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-02 12:21:17,469 - WARNING - Could not load processor. Proceeding without. Error: 'NoneType' object has no attribute 'from_pretrained'\n",
      "2025-05-02 12:21:17,469 - INFO - Loading manifest from: /home/zhuoyuan/CSprojects/musicClaGen/data/processed/final_feature_manifest.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhuoyuan/miniconda3/envs/musicClaGen_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-02 12:21:17,612 - INFO - Loaded manifest with 7994 entries.\n",
      "2025-05-02 12:21:17,613 - INFO - Creating DEBUG DataLoaders with small subsets...\n",
      "2025-05-02 12:21:17,618 - INFO - DEBUG Dataset sizes: Train=16, Val=8\n",
      "2025-05-02 12:21:17,618 - INFO - DEBUG DataLoaders created.\n"
     ]
    }
   ],
   "source": [
    "# --- Load Processor (if needed) ---\n",
    "try:\n",
    "    processor = AutoProcessor.from_pretrained(model_checkpoint)\n",
    "    logging.info(f\"Loaded processor: {model_checkpoint}\")\n",
    "except Exception as e:\n",
    "    logging.warning(f\"Could not load processor. Proceeding without. Error: {e}\")\n",
    "    processor = None\n",
    "\n",
    "# --- Create Full Dataset ---\n",
    "try:\n",
    "    full_dataset = FMAFeatureDataset(manifest_path, processor)\n",
    "    manifest_df = full_dataset.manifest\n",
    "except Exception as e:\n",
    "     logging.error(\"Failed to instantiate FMAFeatureDataset.\", exc_info=True)\n",
    "     raise SystemExit\n",
    "\n",
    "# --- Create SMALLER DEBUG Datasets ---\n",
    "logging.info(\"Creating DEBUG DataLoaders with small subsets...\")\n",
    "try:\n",
    "    # Get small number of samples from each split\n",
    "    train_indices = manifest_df[manifest_df['split'] == 'training'].index[:16] # e.g., 16 train samples\n",
    "    val_indices = manifest_df[manifest_df['split'] == 'validation'].index[:8]  # e.g., 8 val samples\n",
    "    test_indices = manifest_df[manifest_df['split'] == 'test'].index[:8]   # e.g., 8 test samples\n",
    "\n",
    "    debug_train_dataset = Subset(full_dataset, train_indices)\n",
    "    debug_val_dataset = Subset(full_dataset, val_indices)\n",
    "    # debug_test_dataset = Subset(full_dataset, test_indices) # Can create if needed\n",
    "\n",
    "    # Use configured batch size, even if small\n",
    "    debug_train_dataloader = DataLoader(debug_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    debug_val_dataloader = DataLoader(debug_val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    logging.info(f\"DEBUG Dataset sizes: Train={len(debug_train_dataset)}, Val={len(debug_val_dataset)}\")\n",
    "    logging.info(\"DEBUG DataLoaders created.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to create DEBUG datasets/dataloaders: {e}\", exc_info=True)\n",
    "    raise SystemExit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **!!!! OH NO!!!! While the documentation says that the input is mel spectrogram, the model actually takes raw audio as input. So we need to convert the mel spectrogram back to raw audio!!!!  Need to re-clean the data...**\n",
    "\n",
    "# **Let's pass Wav2Vec2-BERT model because of the 24 hour the time constraint. Switch to AST! Come back when we have time.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicClaGen_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
